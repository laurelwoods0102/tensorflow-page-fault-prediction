{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599269436263",
   "display_name": "Python 3.7.6 64-bit ('ProgramData': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'20200905-103143'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "'''\n",
    "log_dir = \"logs/fit/\" + timestamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "'''\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(version_dir)\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., ..., 1., 3., 1.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dataset = np.genfromtxt(\"data/{}_train_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.float32) #np.int64\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "14882"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "word_index = np.genfromtxt(\"data/word_index.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"BATCH_SIZE\"] = 128\n",
    "param_list[\"EPOCHS\"] = 100\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"VOCAB_SIZE\"] = vocab_size\n",
    "param_list[\"LEARNING_RATE\"] = 0.01\n",
    "param_list[\"NUM_1_NEURONS\"] = 177\n",
    "param_list[\"NUM_2_NEURONS\"] = 177\n",
    "param_list[\"DROPOUT_1\"] = 0.1\n",
    "param_list[\"DROPOUT_2\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((14858, 16, 1), (14858, 8, 1))"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "x_train, y_train = generate_timeseries(dataset, 0, None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [933.],\n        [  0.],\n        [  0.],\n        [  0.],\n        [  0.]], dtype=float32),\n array([[ 0.],\n        [ 0.],\n        [48.],\n        [ 0.],\n        [ 0.],\n        [ 0.],\n        [ 0.],\n        [ 0.]], dtype=float32))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x_train[10], y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(param_list[\"NUM_1_NEURONS\"])))\n",
    "model.add(keras.layers.Dropout(param_list[\"DROPOUT_1\"]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.RepeatVector(param_list[\"FUTURE_TARGET\"]))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(param_list[\"NUM_2_NEURONS\"], return_sequences=True)))\n",
    "model.add(keras.layers.Dropout(param_list[\"DROPOUT_2\"]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(param_list[\"NUM_2_NEURONS\"], return_sequences=True)))\n",
    "model.add(keras.layers.Dropout(param_list[\"DROPOUT_2\"]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(param_list[\"VOCAB_SIZE\"], activation='softmax')))\n",
    "model.compile(optimizer=keras.optimizers.Adam(param_list[\"LEARNING_RATE\"]), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0s - loss: 0.4909 - accuracy: 0.8787/93 [===========================>..] - ETA: 0s - loss: 0.4902 - accuracy: 0.8789/93 [===========================>..] - ETA: 0s - loss: 0.4917 - accuracy: 0.8791/93 [============================>.] - ETA: 0s - loss: 0.4899 - accuracy: 0.8793/93 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.8793/93 [==============================] - 4s 43ms/step - loss: 0.4912 - accuracy: 0.8776 - val_loss: 0.5033 - val_accuracy: 0.8273\nEpoch 96/100\n 1/93 [..............................] - ETA: 0s - loss: 0.4941 - accuracy: 0.88 3/93 [..............................] - ETA: 2s - loss: 0.4746 - accuracy: 0.88 5/93 [>.............................] - ETA: 2s - loss: 0.4556 - accuracy: 0.88 7/93 [=>............................] - ETA: 2s - loss: 0.4791 - accuracy: 0.88 9/93 [=>............................] - ETA: 2s - loss: 0.4593 - accuracy: 0.8811/93 [==>...........................] - ETA: 2s - loss: 0.4521 - accuracy: 0.8813/93 [===>..........................] - ETA: 2s - loss: 0.4477 - accuracy: 0.8915/93 [===>..........................] - ETA: 2s - loss: 0.4445 - accuracy: 0.8917/93 [====>.........................] - ETA: 2s - loss: 0.4379 - accuracy: 0.8919/93 [=====>........................] - ETA: 2s - loss: 0.4441 - accuracy: 0.8921/93 [=====>........................] - ETA: 2s - loss: 0.4484 - accuracy: 0.8823/93 [======>.......................] - ETA: 2s - loss: 0.4590 - accuracy: 0.8825/93 [=======>......................] - ETA: 2s - loss: 0.4680 - accuracy: 0.8827/93 [=======>......................] - ETA: 2s - loss: 0.4634 - accuracy: 0.8829/93 [========>.....................] - ETA: 2s - loss: 0.4638 - accuracy: 0.8831/93 [=========>....................] - ETA: 2s - loss: 0.4641 - accuracy: 0.8833/93 [=========>....................] - ETA: 2s - loss: 0.4666 - accuracy: 0.8835/93 [==========>...................] - ETA: 2s - loss: 0.4627 - accuracy: 0.8837/93 [==========>...................] - ETA: 2s - loss: 0.4672 - accuracy: 0.8839/93 [===========>..................] - ETA: 1s - loss: 0.4722 - accuracy: 0.8841/93 [============>.................] - ETA: 1s - loss: 0.4727 - accuracy: 0.8843/93 [============>.................] - ETA: 1s - loss: 0.4782 - accuracy: 0.8845/93 [=============>................] - ETA: 1s - loss: 0.4810 - accuracy: 0.8847/93 [==============>...............] - ETA: 1s - loss: 0.4848 - accuracy: 0.8849/93 [==============>...............] - ETA: 1s - loss: 0.4866 - accuracy: 0.8851/93 [===============>..............] - ETA: 1s - loss: 0.4854 - accuracy: 0.8853/93 [================>.............] - ETA: 1s - loss: 0.4799 - accuracy: 0.8855/93 [================>.............] - ETA: 1s - loss: 0.4798 - accuracy: 0.8857/93 [=================>............] - ETA: 1s - loss: 0.4797 - accuracy: 0.8859/93 [==================>...........] - ETA: 1s - loss: 0.4786 - accuracy: 0.8861/93 [==================>...........] - ETA: 1s - loss: 0.4780 - accuracy: 0.8863/93 [===================>..........] - ETA: 1s - loss: 0.4777 - accuracy: 0.8865/93 [===================>..........] - ETA: 1s - loss: 0.4737 - accuracy: 0.8867/93 [====================>.........] - ETA: 0s - loss: 0.4721 - accuracy: 0.8869/93 [=====================>........] - ETA: 0s - loss: 0.4722 - accuracy: 0.8871/93 [=====================>........] - ETA: 0s - loss: 0.4723 - accuracy: 0.8873/93 [======================>.......] - ETA: 0s - loss: 0.4710 - accuracy: 0.8875/93 [=======================>......] - ETA: 0s - loss: 0.4685 - accuracy: 0.8877/93 [=======================>......] - ETA: 0s - loss: 0.4691 - accuracy: 0.8879/93 [========================>.....] - ETA: 0s - loss: 0.4651 - accuracy: 0.8881/93 [=========================>....] - ETA: 0s - loss: 0.4678 - accuracy: 0.8883/93 [=========================>....] - ETA: 0s - loss: 0.4698 - accuracy: 0.8885/93 [==========================>...] - ETA: 0s - loss: 0.4678 - accuracy: 0.8887/93 [===========================>..] - ETA: 0s - loss: 0.4665 - accuracy: 0.8889/93 [===========================>..] - ETA: 0s - loss: 0.4664 - accuracy: 0.8891/93 [============================>.] - ETA: 0s - loss: 0.4657 - accuracy: 0.8893/93 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.8893/93 [==============================] - 4s 43ms/step - loss: 0.4673 - accuracy: 0.8851 - val_loss: 0.5003 - val_accuracy: 0.8238\nEpoch 97/100\n 1/93 [..............................] - ETA: 0s - loss: 0.4117 - accuracy: 0.90 3/93 [..............................] - ETA: 2s - loss: 0.4659 - accuracy: 0.89 5/93 [>.............................] - ETA: 2s - loss: 0.4540 - accuracy: 0.89 7/93 [=>............................] - ETA: 2s - loss: 0.4606 - accuracy: 0.89 9/93 [=>............................] - ETA: 2s - loss: 0.4722 - accuracy: 0.8811/93 [==>...........................] - ETA: 2s - loss: 0.4790 - accuracy: 0.8813/93 [===>..........................] - ETA: 2s - loss: 0.4822 - accuracy: 0.8815/93 [===>..........................] - ETA: 2s - loss: 0.4683 - accuracy: 0.8817/93 [====>.........................] - ETA: 2s - loss: 0.4660 - accuracy: 0.8819/93 [=====>........................] - ETA: 2s - loss: 0.4589 - accuracy: 0.8821/93 [=====>........................] - ETA: 2s - loss: 0.4561 - accuracy: 0.8823/93 [======>.......................] - ETA: 2s - loss: 0.4662 - accuracy: 0.8825/93 [=======>......................] - ETA: 2s - loss: 0.4644 - accuracy: 0.8827/93 [=======>......................] - ETA: 2s - loss: 0.4565 - accuracy: 0.8829/93 [========>.....................] - ETA: 2s - loss: 0.4608 - accuracy: 0.8831/93 [=========>....................] - ETA: 2s - loss: 0.4574 - accuracy: 0.8833/93 [=========>....................] - ETA: 2s - loss: 0.4540 - accuracy: 0.8835/93 [==========>...................] - ETA: 2s - loss: 0.4549 - accuracy: 0.8837/93 [==========>...................] - ETA: 2s - loss: 0.4510 - accuracy: 0.8839/93 [===========>..................] - ETA: 1s - loss: 0.4517 - accuracy: 0.8841/93 [============>.................] - ETA: 1s - loss: 0.4553 - accuracy: 0.8843/93 [============>.................] - ETA: 1s - loss: 0.4584 - accuracy: 0.8845/93 [=============>................] - ETA: 1s - loss: 0.4596 - accuracy: 0.8847/93 [==============>...............] - ETA: 1s - loss: 0.4589 - accuracy: 0.8849/93 [==============>...............] - ETA: 1s - loss: 0.4584 - accuracy: 0.8851/93 [===============>..............] - ETA: 1s - loss: 0.4597 - accuracy: 0.8853/93 [================>.............] - ETA: 1s - loss: 0.4577 - accuracy: 0.8855/93 [================>.............] - ETA: 1s - loss: 0.4624 - accuracy: 0.8857/93 [=================>............] - ETA: 1s - loss: 0.4646 - accuracy: 0.8859/93 [==================>...........] - ETA: 1s - loss: 0.4665 - accuracy: 0.8861/93 [==================>...........] - ETA: 1s - loss: 0.4644 - accuracy: 0.8863/93 [===================>..........] - ETA: 1s - loss: 0.4652 - accuracy: 0.8865/93 [===================>..........] - ETA: 1s - loss: 0.4647 - accuracy: 0.8867/93 [====================>.........] - ETA: 0s - loss: 0.4606 - accuracy: 0.8869/93 [=====================>........] - ETA: 0s - loss: 0.4616 - accuracy: 0.8871/93 [=====================>........] - ETA: 0s - loss: 0.4590 - accuracy: 0.8873/93 [======================>.......] - ETA: 0s - loss: 0.4606 - accuracy: 0.8875/93 [=======================>......] - ETA: 0s - loss: 0.4623 - accuracy: 0.8877/93 [=======================>......] - ETA: 0s - loss: 0.4616 - accuracy: 0.8879/93 [========================>.....] - ETA: 0s - loss: 0.4611 - accuracy: 0.8881/93 [=========================>....] - ETA: 0s - loss: 0.4641 - accuracy: 0.8883/93 [=========================>....] - ETA: 0s - loss: 0.4627 - accuracy: 0.8885/93 [==========================>...] - ETA: 0s - loss: 0.4641 - accuracy: 0.8887/93 [===========================>..] - ETA: 0s - loss: 0.4629 - accuracy: 0.8889/93 [===========================>..] - ETA: 0s - loss: 0.4644 - accuracy: 0.8891/93 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.8893/93 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.8893/93 [==============================] - 4s 42ms/step - loss: 0.4638 - accuracy: 0.8860 - val_loss: 0.4702 - val_accuracy: 0.8340\nEpoch 98/100\n 1/93 [..............................] - ETA: 0s - loss: 0.4137 - accuracy: 0.91 3/93 [..............................] - ETA: 2s - loss: 0.4693 - accuracy: 0.89 5/93 [>.............................] - ETA: 2s - loss: 0.4129 - accuracy: 0.90 7/93 [=>............................] - ETA: 2s - loss: 0.4294 - accuracy: 0.89 9/93 [=>............................] - ETA: 2s - loss: 0.4483 - accuracy: 0.8911/93 [==>...........................] - ETA: 2s - loss: 0.4485 - accuracy: 0.8913/93 [===>..........................] - ETA: 2s - loss: 0.4555 - accuracy: 0.8815/93 [===>..........................] - ETA: 2s - loss: 0.4489 - accuracy: 0.8917/93 [====>.........................] - ETA: 2s - loss: 0.4416 - accuracy: 0.8919/93 [=====>........................] - ETA: 2s - loss: 0.4452 - accuracy: 0.8921/93 [=====>........................] - ETA: 2s - loss: 0.4438 - accuracy: 0.8923/93 [======>.......................] - ETA: 2s - loss: 0.4358 - accuracy: 0.8925/93 [=======>......................] - ETA: 2s - loss: 0.4416 - accuracy: 0.8927/93 [=======>......................] - ETA: 2s - loss: 0.4456 - accuracy: 0.8929/93 [========>.....................] - ETA: 2s - loss: 0.4417 - accuracy: 0.8931/93 [=========>....................] - ETA: 2s - loss: 0.4395 - accuracy: 0.8933/93 [=========>....................] - ETA: 2s - loss: 0.4452 - accuracy: 0.8835/93 [==========>...................] - ETA: 2s - loss: 0.4475 - accuracy: 0.8837/93 [==========>...................] - ETA: 2s - loss: 0.4513 - accuracy: 0.8839/93 [===========>..................] - ETA: 1s - loss: 0.4491 - accuracy: 0.8841/93 [============>.................] - ETA: 1s - loss: 0.4506 - accuracy: 0.8843/93 [============>.................] - ETA: 1s - loss: 0.4493 - accuracy: 0.8845/93 [=============>................] - ETA: 1s - loss: 0.4504 - accuracy: 0.8847/93 [==============>...............] - ETA: 1s - loss: 0.4530 - accuracy: 0.8849/93 [==============>...............] - ETA: 1s - loss: 0.4502 - accuracy: 0.8851/93 [===============>..............] - ETA: 1s - loss: 0.4505 - accuracy: 0.8853/93 [================>.............] - ETA: 1s - loss: 0.4513 - accuracy: 0.8855/93 [================>.............] - ETA: 1s - loss: 0.4520 - accuracy: 0.8857/93 [=================>............] - ETA: 1s - loss: 0.4520 - accuracy: 0.8859/93 [==================>...........] - ETA: 1s - loss: 0.4529 - accuracy: 0.8861/93 [==================>...........] - ETA: 1s - loss: 0.4541 - accuracy: 0.8863/93 [===================>..........] - ETA: 1s - loss: 0.4543 - accuracy: 0.8865/93 [===================>..........] - ETA: 1s - loss: 0.4537 - accuracy: 0.8867/93 [====================>.........] - ETA: 0s - loss: 0.4543 - accuracy: 0.8869/93 [=====================>........] - ETA: 0s - loss: 0.4589 - accuracy: 0.8871/93 [=====================>........] - ETA: 0s - loss: 0.4617 - accuracy: 0.8873/93 [======================>.......] - ETA: 0s - loss: 0.4621 - accuracy: 0.8875/93 [=======================>......] - ETA: 0s - loss: 0.4655 - accuracy: 0.8877/93 [=======================>......] - ETA: 0s - loss: 0.4653 - accuracy: 0.8879/93 [========================>.....] - ETA: 0s - loss: 0.4665 - accuracy: 0.8881/93 [=========================>....] - ETA: 0s - loss: 0.4679 - accuracy: 0.8883/93 [=========================>....] - ETA: 0s - loss: 0.4682 - accuracy: 0.8885/93 [==========================>...] - ETA: 0s - loss: 0.4663 - accuracy: 0.8887/93 [===========================>..] - ETA: 0s - loss: 0.4683 - accuracy: 0.8889/93 [===========================>..] - ETA: 0s - loss: 0.4689 - accuracy: 0.8891/93 [============================>.] - ETA: 0s - loss: 0.4711 - accuracy: 0.8893/93 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.8893/93 [==============================] - 4s 42ms/step - loss: 0.4723 - accuracy: 0.8826 - val_loss: 0.5325 - val_accuracy: 0.7924\nEpoch 99/100\n 1/93 [..............................] - ETA: 0s - loss: 0.4532 - accuracy: 0.86 3/93 [..............................] - ETA: 2s - loss: 0.4598 - accuracy: 0.87 5/93 [>.............................] - ETA: 2s - loss: 0.4759 - accuracy: 0.87 7/93 [=>............................] - ETA: 2s - loss: 0.4508 - accuracy: 0.87 9/93 [=>............................] - ETA: 2s - loss: 0.4748 - accuracy: 0.8711/93 [==>...........................] - ETA: 2s - loss: 0.4700 - accuracy: 0.8713/93 [===>..........................] - ETA: 2s - loss: 0.4607 - accuracy: 0.8715/93 [===>..........................] - ETA: 2s - loss: 0.4676 - accuracy: 0.8717/93 [====>.........................] - ETA: 2s - loss: 0.4731 - accuracy: 0.8719/93 [=====>........................] - ETA: 2s - loss: 0.4745 - accuracy: 0.8721/93 [=====>........................] - ETA: 2s - loss: 0.4769 - accuracy: 0.8723/93 [======>.......................] - ETA: 2s - loss: 0.4804 - accuracy: 0.8725/93 [=======>......................] - ETA: 2s - loss: 0.4807 - accuracy: 0.8727/93 [=======>......................] - ETA: 2s - loss: 0.4784 - accuracy: 0.8729/93 [========>.....................] - ETA: 2s - loss: 0.4759 - accuracy: 0.8731/93 [=========>....................] - ETA: 2s - loss: 0.4806 - accuracy: 0.8733/93 [=========>....................] - ETA: 2s - loss: 0.4793 - accuracy: 0.8735/93 [==========>...................] - ETA: 2s - loss: 0.4782 - accuracy: 0.8737/93 [==========>...................] - ETA: 2s - loss: 0.4844 - accuracy: 0.8739/93 [===========>..................] - ETA: 1s - loss: 0.4834 - accuracy: 0.8741/93 [============>.................] - ETA: 1s - loss: 0.4837 - accuracy: 0.8743/93 [============>.................] - ETA: 1s - loss: 0.4860 - accuracy: 0.8745/93 [=============>................] - ETA: 1s - loss: 0.4866 - accuracy: 0.8747/93 [==============>...............] - ETA: 1s - loss: 0.4860 - accuracy: 0.8749/93 [==============>...............] - ETA: 1s - loss: 0.4871 - accuracy: 0.8751/93 [===============>..............] - ETA: 1s - loss: 0.4881 - accuracy: 0.8753/93 [================>.............] - ETA: 1s - loss: 0.4872 - accuracy: 0.8755/93 [================>.............] - ETA: 1s - loss: 0.4911 - accuracy: 0.8757/93 [=================>............] - ETA: 1s - loss: 0.4933 - accuracy: 0.8759/93 [==================>...........] - ETA: 1s - loss: 0.4900 - accuracy: 0.8861/93 [==================>...........] - ETA: 1s - loss: 0.4885 - accuracy: 0.8863/93 [===================>..........] - ETA: 1s - loss: 0.4893 - accuracy: 0.8865/93 [===================>..........] - ETA: 1s - loss: 0.4923 - accuracy: 0.8867/93 [====================>.........] - ETA: 0s - loss: 0.4919 - accuracy: 0.8869/93 [=====================>........] - ETA: 0s - loss: 0.4917 - accuracy: 0.8871/93 [=====================>........] - ETA: 0s - loss: 0.4937 - accuracy: 0.8873/93 [======================>.......] - ETA: 0s - loss: 0.4950 - accuracy: 0.8775/93 [=======================>......] - ETA: 0s - loss: 0.4956 - accuracy: 0.8777/93 [=======================>......] - ETA: 0s - loss: 0.4967 - accuracy: 0.8779/93 [========================>.....] - ETA: 0s - loss: 0.4977 - accuracy: 0.8781/93 [=========================>....] - ETA: 0s - loss: 0.4957 - accuracy: 0.8783/93 [=========================>....] - ETA: 0s - loss: 0.4929 - accuracy: 0.8785/93 [==========================>...] - ETA: 0s - loss: 0.4949 - accuracy: 0.8787/93 [===========================>..] - ETA: 0s - loss: 0.4942 - accuracy: 0.8789/93 [===========================>..] - ETA: 0s - loss: 0.4938 - accuracy: 0.8791/93 [============================>.] - ETA: 0s - loss: 0.4953 - accuracy: 0.8793/93 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8793/93 [==============================] - 4s 42ms/step - loss: 0.4952 - accuracy: 0.8795 - val_loss: 0.4811 - val_accuracy: 0.8323\nEpoch 100/100\n 1/93 [..............................] - ETA: 0s - loss: 0.4372 - accuracy: 0.87 3/93 [..............................] - ETA: 2s - loss: 0.4906 - accuracy: 0.88 5/93 [>.............................] - ETA: 2s - loss: 0.4765 - accuracy: 0.88 7/93 [=>............................] - ETA: 2s - loss: 0.4819 - accuracy: 0.88 9/93 [=>............................] - ETA: 2s - loss: 0.4876 - accuracy: 0.8811/93 [==>...........................] - ETA: 2s - loss: 0.4810 - accuracy: 0.8813/93 [===>..........................] - ETA: 2s - loss: 0.4689 - accuracy: 0.8815/93 [===>..........................] - ETA: 2s - loss: 0.4659 - accuracy: 0.8817/93 [====>.........................] - ETA: 2s - loss: 0.4678 - accuracy: 0.8819/93 [=====>........................] - ETA: 2s - loss: 0.4718 - accuracy: 0.8821/93 [=====>........................] - ETA: 2s - loss: 0.4790 - accuracy: 0.8823/93 [======>.......................] - ETA: 2s - loss: 0.4771 - accuracy: 0.8825/93 [=======>......................] - ETA: 2s - loss: 0.4761 - accuracy: 0.8827/93 [=======>......................] - ETA: 2s - loss: 0.4854 - accuracy: 0.8729/93 [========>.....................] - ETA: 2s - loss: 0.4828 - accuracy: 0.8731/93 [=========>....................] - ETA: 2s - loss: 0.4890 - accuracy: 0.8733/93 [=========>....................] - ETA: 2s - loss: 0.4866 - accuracy: 0.8735/93 [==========>...................] - ETA: 2s - loss: 0.4867 - accuracy: 0.8737/93 [==========>...................] - ETA: 2s - loss: 0.4824 - accuracy: 0.8739/93 [===========>..................] - ETA: 1s - loss: 0.4812 - accuracy: 0.8741/93 [============>.................] - ETA: 1s - loss: 0.4845 - accuracy: 0.8743/93 [============>.................] - ETA: 1s - loss: 0.4830 - accuracy: 0.8745/93 [=============>................] - ETA: 1s - loss: 0.4818 - accuracy: 0.8747/93 [==============>...............] - ETA: 1s - loss: 0.4818 - accuracy: 0.8749/93 [==============>...............] - ETA: 1s - loss: 0.4841 - accuracy: 0.8751/93 [===============>..............] - ETA: 1s - loss: 0.4833 - accuracy: 0.8753/93 [================>.............] - ETA: 1s - loss: 0.4880 - accuracy: 0.8755/93 [================>.............] - ETA: 1s - loss: 0.4908 - accuracy: 0.8757/93 [=================>............] - ETA: 1s - loss: 0.4912 - accuracy: 0.8759/93 [==================>...........] - ETA: 1s - loss: 0.4927 - accuracy: 0.8661/93 [==================>...........] - ETA: 1s - loss: 0.4955 - accuracy: 0.8663/93 [===================>..........] - ETA: 1s - loss: 0.4993 - accuracy: 0.8665/93 [===================>..........] - ETA: 1s - loss: 0.5019 - accuracy: 0.8667/93 [====================>.........] - ETA: 0s - loss: 0.5006 - accuracy: 0.8669/93 [=====================>........] - ETA: 0s - loss: 0.4980 - accuracy: 0.8671/93 [=====================>........] - ETA: 0s - loss: 0.4960 - accuracy: 0.8673/93 [======================>.......] - ETA: 0s - loss: 0.4919 - accuracy: 0.8775/93 [=======================>......] - ETA: 0s - loss: 0.4938 - accuracy: 0.8777/93 [=======================>......] - ETA: 0s - loss: 0.4946 - accuracy: 0.8779/93 [========================>.....] - ETA: 0s - loss: 0.4984 - accuracy: 0.8681/93 [=========================>....] - ETA: 0s - loss: 0.4972 - accuracy: 0.8783/93 [=========================>....] - ETA: 0s - loss: 0.4997 - accuracy: 0.8685/93 [==========================>...] - ETA: 0s - loss: 0.4996 - accuracy: 0.8687/93 [===========================>..] - ETA: 0s - loss: 0.5000 - accuracy: 0.8689/93 [===========================>..] - ETA: 0s - loss: 0.4987 - accuracy: 0.8791/93 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.8693/93 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.8793/93 [==============================] - 4s 42ms/step - loss: 0.4991 - accuracy: 0.8700 - val_loss: 0.4947 - val_accuracy: 0.8280\n"
    }
   ],
   "source": [
    "model_history = model.fit(x_train, y_train, batch_size=param_list[\"BATCH_SIZE\"], validation_split=0.2, epochs=param_list[\"EPOCHS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"version/{}/model.h5\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nbidirectional (Bidirectional multiple                  253464\n_________________________________________________________________\ndropout (Dropout)            multiple                  0\n_________________________________________________________________\nbatch_normalization (BatchNo multiple                  1416\n_________________________________________________________________\nrepeat_vector (RepeatVector) multiple                  0\n_________________________________________________________________\nbidirectional_1 (Bidirection multiple                  753312\n_________________________________________________________________\ndropout_1 (Dropout)          multiple                  0\n_________________________________________________________________\nbatch_normalization_1 (Batch multiple                  1416\n_________________________________________________________________\nbidirectional_2 (Bidirection multiple                  753312\n_________________________________________________________________\ndropout_2 (Dropout)          multiple                  0\n_________________________________________________________________\nbatch_normalization_2 (Batch multiple                  1416\n_________________________________________________________________\ntime_distributed (TimeDistri multiple                  5283110\n=================================================================\nTotal params: 7,047,446\nTrainable params: 7,045,322\nNon-trainable params: 2,124\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}