{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595891893089",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + timestamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "os.makedirs(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"TRAIN_SPLIT\"] = 100000\n",
    "param_list[\"BATCH_SIZE\"] = 256\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"EVALUATION_INTERVAL\"] = 300\n",
    "param_list[\"VAL_STEPS\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-1.58765354e+09,  6.17292586e+09, -6.18790972e+09, ...,\n        4.75252240e+07, -7.52773980e+07,  3.22916640e+07])"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "pattern_length = 4\n",
    "data = np.genfromtxt(\"data/SEG_delta_pattern_length_{}.csv\".format(pattern_length), delimiter=\"\\n\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[-1.58765354e+09],\n        [ 6.17292586e+09],\n        [-6.18790972e+09],\n        ...,\n        [ 6.55360000e+04],\n        [-1.66730240e+07],\n        [-5.88800000e+03]]),\n array([[   878848.],\n        [-11182192.],\n        [-11128760.],\n        ...,\n        [ 47525224.],\n        [-75277398.],\n        [ 32291664.]]))"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.4, shuffle=False, random_state=42)\n",
    "train_set = train_set.reshape(-1, 1)\n",
    "test_set = test_set.reshape(-1, 1)\n",
    "train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.39820164],\n       [0.89577508],\n       [0.10325391],\n       ...,\n       [0.49999905],\n       [0.49892585],\n       [0.49999447]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_set_scaled = minmax_scaler.fit_transform(train_set)\n",
    "joblib.dump(minmax_scaler, \"{}/scaler.pkl\".format(timestamp))\n",
    "train_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.499996    1380\n0.499995     574\n0.499996     527\n0.499995     387\n0.499996     302\n            ... \n0.494874       1\n0.489484       1\n0.486970       1\n0.588957       1\n0.000000       1\nLength: 63732, dtype: int64"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "pd.Series(data=train_set_scaled.reshape(1, -1)[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_timeseries(train_set_scaled, 0, param_list[\"TRAIN_SPLIT\"], param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "x_val, y_val = generate_timeseries(train_set_scaled, param_list[\"TRAIN_SPLIT\"], None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat()\n",
    "train_data = train_data.cache().shuffle(param_list[\"BUFFER_SIZE\"]).batch(param_list[\"BATCH_SIZE\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).repeat()\n",
    "val_data = val_data.cache().batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'MinMaxScaler' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-970af8cf7983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_set_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminmax_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PAST_HISTORY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FUTURE_TARGET\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"BATCH_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d07c175711d9>\u001b[0m in \u001b[0;36mgenerate_timeseries\u001b[1;34m(dataset, start_index, end_index, history_size, target_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhistory_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mend_index\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mend_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'MinMaxScaler' has no len()"
     ]
    }
   ],
   "source": [
    "test_set_scaled = minmax_scaler.fit(test_set)\n",
    "\n",
    "x_test, y_test = generate_timeseries(test_set_scaled, 0, None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).repeat()\n",
    "test_data = test_data.cache().batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_NUM_LSTM_1_UNITS = hp.HParam(\"num_LSTM_1_units\", hp.Discrete([32, 64, 128]))\n",
    "HP_NUM_LSTM_2_UNITS = hp.HParam(\"num_LSTM_2_units\", hp.Discrete([32, 64, 128]))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.1, 0.5))\n",
    "\n",
    "METRIC_ACCURACY = 'mae'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_LSTM_1_UNITS, HP_NUM_LSTM_2_UNITS, HP_DROPOUT],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='mae')]\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(hparams[HP_NUM_LSTM_1_UNITS], return_sequences=True, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.LSTM(hparams[HP_NUM_LSTM_2_UNITS]),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(FUTURE_TARGET),\n",
    "        tf.keras.layers.Activation(\"relu\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(train_data, epochs=1, steps_per_epoch=EVALUATION_INTERVAL, validation_data=val_data, validation_steps=VAL_STEPS, callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),  # log metrics\n",
    "        hp.KerasCallback(log_dir, hparams),  # log hparams\n",
    "    ],)\n",
    "    _, mae = model.evaluate(x_test, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}