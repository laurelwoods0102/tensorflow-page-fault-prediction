{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596428835291",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + timestamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"TRAIN_SPLIT\"] = 100000\n",
    "param_list[\"BATCH_SIZE\"] = 256\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"EVALUATION_INTERVAL\"] = 300\n",
    "param_list[\"VAL_STEPS\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        delta            t          t+1\n0 -1587653544  95444983808  93857330264\n1  6172925864  87674650304  93847576168\n2 -6187909720  93857313352  87669403632\n3 -6187918552  93857325056  87669406504\n4 -6198466584  93857330264  87658863680",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>delta</th>\n      <th>t</th>\n      <th>t+1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1587653544</td>\n      <td>95444983808</td>\n      <td>93857330264</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6172925864</td>\n      <td>87674650304</td>\n      <td>93847576168</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-6187909720</td>\n      <td>93857313352</td>\n      <td>87669403632</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-6187918552</td>\n      <td>93857325056</td>\n      <td>87669406504</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-6198466584</td>\n      <td>93857330264</td>\n      <td>87658863680</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "pattern_length = 4\n",
    "\n",
    "dataset = pd.read_csv(\"data/SEG_dataset_{}.csv\".format(pattern_length))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(             delta            t          t+1\n 0      -1587653544  95444983808  93857330264\n 1       6172925864  87674650304  93847576168\n 2      -6187909720  93857313352  87669403632\n 3      -6187918552  93857325056  87669406504\n 4      -6198466584  93857330264  87658863680\n ...            ...          ...          ...\n 122816       63120  93730480256  93730543376\n 122817       65536  93730495504  93730561040\n 122818       65536  93730497920  93730563456\n 122819   -16673024  93747165384  93730492360\n 122820       -5888  93730543376  93730537488\n \n [122821 rows x 3 columns],\n            delta            t          t+1\n 122821    878848  93730561040  93731439888\n 122822 -11182192  93730563456  93719381264\n 122823 -11128760  93730492360  93719363600\n 122824  53784064  93730537488  93784321552\n 122825  52884080  93731439888  93784323968\n ...          ...          ...          ...\n 204697  29537720  92593157808  92622695528\n 204698  11427864  92611267656  92622695520\n 204699  47525224  92611267648  92658792872\n 204700 -75277398  92734070262  92658792864\n 204701  32291664  92622695528  92654987192\n \n [81881 rows x 3 columns])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.4, shuffle=False, random_state=42)\n",
    "train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.39820164],\n       [0.89577508],\n       [0.10325391],\n       ...,\n       [0.49999905],\n       [0.49892585],\n       [0.49999447]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_set_scaled = minmax_scaler.fit_transform(train_set[\"delta\"].values.reshape(-1, 1))\n",
    "joblib.dump(minmax_scaler, \"version/{}/scaler.pkl\".format(timestamp))\n",
    "train_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_timeseries(train_set_scaled, 0, param_list[\"TRAIN_SPLIT\"], param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "x_val, y_val = generate_timeseries(train_set_scaled, param_list[\"TRAIN_SPLIT\"], None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat()\n",
    "train_data = train_data.cache().shuffle(param_list[\"BUFFER_SIZE\"]).batch(param_list[\"BATCH_SIZE\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).repeat()\n",
    "val_data = val_data.cache().batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'MinMaxScaler' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-970af8cf7983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_set_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminmax_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PAST_HISTORY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FUTURE_TARGET\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"BATCH_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d07c175711d9>\u001b[0m in \u001b[0;36mgenerate_timeseries\u001b[1;34m(dataset, start_index, end_index, history_size, target_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhistory_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mend_index\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mend_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'MinMaxScaler' has no len()"
     ]
    }
   ],
   "source": [
    "test_set_scaled = minmax_scaler.fit(test_set)\n",
    "\n",
    "x_test, y_test = generate_timeseries(test_set_scaled, 0, None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).repeat()\n",
    "test_data = test_data.cache().batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_NUM_LSTM_1_UNITS = hp.HParam(\"num_LSTM_1_units\", hp.Discrete([32, 64, 128]))\n",
    "HP_NUM_LSTM_2_UNITS = hp.HParam(\"num_LSTM_2_units\", hp.Discrete([32, 64, 128]))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.1, 0.5))\n",
    "\n",
    "METRIC_ACCURACY = 'mae'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_LSTM_1_UNITS, HP_NUM_LSTM_2_UNITS, HP_DROPOUT],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='mae')]\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(hparams[HP_NUM_LSTM_1_UNITS], return_sequences=True, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.LSTM(hparams[HP_NUM_LSTM_2_UNITS]),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(FUTURE_TARGET),\n",
    "        tf.keras.layers.Activation(\"relu\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(train_data, epochs=1, steps_per_epoch=EVALUATION_INTERVAL, validation_data=val_data, validation_steps=VAL_STEPS, callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),  # log metrics\n",
    "        hp.KerasCallback(log_dir, hparams),  # log hparams\n",
    "    ],)\n",
    "    _, mae = model.evaluate(x_test, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}