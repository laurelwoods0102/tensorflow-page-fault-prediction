{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Script for AWS, combining training and testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"GEMM_STREAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'20210412-034716'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(\"version\"):\n",
    "    os.makedirs(\"version\")\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "timestamp"
   ]
  },
  {
   "source": [
    "vocabulary_gpa = np.genfromtxt(\"static/vocabulary_gpa.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "gpa_vocab_size = vocabulary_gpa.shape[0]\n",
    "gpa_vocab_size"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "vocabulary_rip = np.genfromtxt(\"static/vocabulary_rip.csv\", delimiter=\"\\n\", dtype=np.uint64)\n",
    "rip_vocab_size = vocabulary_rip.shape[0]\n",
    "rip_vocab_size"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"BATCH_SIZE\"] = 64       # For AWS ml.p3.2xlarge instances\n",
    "param_list[\"DILATIONS\"] = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "param_list[\"FILTER_WIDTH\"] = 2                          # == kernel_size\n",
    "param_list[\"SCALAR_INPUT\"] = False\n",
    "param_list[\"INITIAL_FILTER_WIDTH\"] = 32     # Scalar Input\n",
    "param_list[\"DILATION_CHANNELS\"] = 32\n",
    "param_list[\"RESIDUAL_CHANNELS\"] = 24\n",
    "param_list[\"SKIP_CHANNELS\"] = 128\n",
    "param_list[\"OUT_CHANNELS\"] = gpa_vocab_size\n",
    "param_list[\"USE_BIASES\"] = False\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"SHUFFLE_SEED\"] = 102\n",
    "\n",
    "if param_list[\"SCALAR_INPUT\"]:\n",
    "    param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"INITIAL_FILTER_WIDTH\"]\n",
    "else:\n",
    "    param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"FILTER_WIDTH\"]\n",
    "\n",
    "param_list[\"MULTI_INPUT\"] = True\n",
    "param_list[\"INPUT_1_CHANNELS\"] = param_list[\"OUT_CHANNELS\"]\n",
    "param_list[\"INPUT_2_CHANNELS\"] = rip_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "param_list[\"RECEPTIVE_FIELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/{}_train_set.csv\".format(dataset_name), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_set[:-1].values).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_train = x_train.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"])) \n",
    "x_train = x_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.data.Dataset.from_tensor_slices(train_set[param_list[\"RECEPTIVE_FIELD\"]:]['gpa'].values).window(1, 1, 1, True)\n",
    "y_train = y_train.flat_map(lambda y: y.batch(1))\n",
    "y_train = y_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip((x_train, y_train)).shuffle(param_list[\"BUFFER_SIZE\"], param_list[\"SHUFFLE_SEED\"], reshuffle_each_iteration=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = pd.read_csv(\"data/{}_val_set.csv\".format(dataset_name), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tf.data.Dataset.from_tensor_slices(val_set[:-1].values).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_val = x_val.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"]))\n",
    "x_val = x_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = tf.data.Dataset.from_tensor_slices(val_set[param_list[\"RECEPTIVE_FIELD\"]:]['gpa'].values).window(1, 1, 1, True)\n",
    "y_val = y_val.flat_map(lambda y: y.batch(1))\n",
    "y_val = y_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.data.Dataset.zip((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels        # out_channels == quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, dilation_channels, residual_channels, skip_channels, out_channels, scalar_input=False, initial_filter_width=None, use_biases=False, multi_input=False, input_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width       # Scalar Input\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        # quantization_channels == out_channels\n",
    "        self.out_channels = out_channels             # Same as vocab_size in encoder-decoder\n",
    "        self.use_biases = use_biases\n",
    "        self.multi_input = multi_input\n",
    "\n",
    "        if self.scalar_input:\n",
    "            self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "        else:\n",
    "            self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.filter_width\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def build(self, input_shape):  # Unable to retrieve input_shape when using tf.data.Dataset  \n",
    "        #self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model        \n",
    "        self.output_width = 1\n",
    "\n",
    "        if self.scalar_input:\n",
    "            self.preprocessing_layer = keras.layers.Conv1D(\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=self.initial_filter_width,\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"preprocessing/conv\")\n",
    "        else:\n",
    "            self.preprocessing_layer = keras.layers.Conv1D(\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=self.filter_width,\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for i, dilation in enumerate(self.dilations):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    layer_index=i,\n",
    "                    dilation=dilation, \n",
    "                    filter_width=self.filter_width, \n",
    "                    dilation_channels=self.dilation_channels, \n",
    "                    residual_channels=self.residual_channels, \n",
    "                    skip_channels=self.skip_channels, \n",
    "                    use_biases=self.use_biases, \n",
    "                    output_width=self.output_width)\n",
    "                )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        if not self.scalar_input and not self.multi_input:\n",
    "            inputs = tf.one_hot(inputs, self.out_channels, axis=-1)\n",
    "\n",
    "        if self.multi_input:\n",
    "            split = tf.split(inputs, num_or_size_splits=len(self.input_channels), axis=-1)\n",
    "            tmp = []\n",
    "            for i, s in enumerate(split):\n",
    "                tmp.append(tf.one_hot(s, self.input_channels[i], axis=-1))\n",
    "            inputs = tf.squeeze(tf.concat(tmp, axis=-1), axis=2)\n",
    "        \n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "        \n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "            \n",
    "        skip_sum = tf.math.add_n(skip_outputs)\n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "        \n",
    "        #out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "        #output = sample_from_discretized_mix_logistic(out)             # Generative\n",
    "\n",
    "        #if not training:\n",
    "        #    output = tf.nn.softmax(tf.cast(output, tf.float64))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            #reduced_loss = tf.math.reduce_mean(loss)\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(\n",
    "    batch_size=param_list[\"BATCH_SIZE\"], \n",
    "    dilations=param_list[\"DILATIONS\"], \n",
    "    filter_width=param_list[\"FILTER_WIDTH\"], \n",
    "    dilation_channels=param_list[\"DILATION_CHANNELS\"], \n",
    "    residual_channels=param_list[\"RESIDUAL_CHANNELS\"], \n",
    "    skip_channels=param_list[\"SKIP_CHANNELS\"], \n",
    "    out_channels=param_list[\"OUT_CHANNELS\"],\n",
    "    scalar_input=param_list[\"SCALAR_INPUT\"],\n",
    "    initial_filter_width=param_list[\"INITIAL_FILTER_WIDTH\"],\n",
    "    multi_input=param_list[\"MULTI_INPUT\"], \n",
    "    input_channels=[param_list[\"INPUT_1_CHANNELS\"], param_list[\"INPUT_2_CHANNELS\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "source": [
    "history = wavenet.fit(train_data, epochs=10000, validation_data=val_data, \n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)])"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10000\n"
     ]
    }
   ]
  },
  {
   "source": [
    "wavenet.save(version_dir)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: version/20201209-094522\\assets\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
       "0  3.943306              0.433426  6.563071                   0.13706"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>categorical_accuracy</th>\n      <th>val_loss</th>\n      <th>val_categorical_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.943306</td>\n      <td>0.433426</td>\n      <td>6.563071</td>\n      <td>0.13706</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_history = pd.DataFrame.from_dict(history.history)\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.to_csv(\"version/{}/train_history.csv\".format(timestamp), index=False)"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"data/{}_test_set.csv\".format(dataset_name), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.data.Dataset.from_tensor_slices(test_set[:-1].values).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_test = x_test.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"])) \n",
    "x_test = x_test.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.data.Dataset.from_tensor_slices(test_set[param_list[\"RECEPTIVE_FIELD\"]:]['gpa'].values).window(1, 1, 1, True)\n",
    "y_test_slices = y_test.flat_map(lambda y: y.batch(1))\n",
    "y_test = y_test_slices.map(lambda y: tf.one_hot(y, param_list[\"OUT_CHANNELS\"], axis=-1))\n",
    "y_test = y_test.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for x in x_test:\n",
    "    y_pred.extend(tf.argmax(model.predict(x), axis=-1).numpy())\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([yt for yt in y_test_slices.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f = [], [], []\n",
    "average_method = [\"micro\", \"macro\", \"weighted\"]\n",
    "\n",
    "for method in average_method:\n",
    "    precision = precision_score(np.ravel(y_true), np.ravel(y_pred), average=method)\n",
    "    recall = recall_score(np.ravel(y_true), np.ravel(y_pred), average=method)\n",
    "    f1 = f1_score(np.ravel(y_true), np.ravel(y_pred), average=method)\n",
    "     \n",
    "    p.append(precision)\n",
    "    r.append(recall)\n",
    "    f.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(data=[p, r, f], columns=average_method, index=[\"precision\", \"recall\", \"f1\"])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = pd.DataFrame(data=[[loss, acc]], columns=[\"loss\", \"accuracy\"])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(\"version/{}/report.csv\".format(timestamp))\n",
    "accuracy.to_csv(\"version/{}/accuracy.csv\".format(timestamp), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}