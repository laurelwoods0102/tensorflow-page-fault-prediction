{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill\n",
    "import tqdm\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hier_softmax:\n",
    "    def __init__(self, tree, contex_size, model):\n",
    "        self._tree_tools = TreeTools()\n",
    "        self.str2weight = {}\n",
    "        #create a weight matrix and bias vector for each node in the tree\n",
    "        for i, subtree in enumerate(self._tree_tools._get_subtrees(tree)):\n",
    "            self.str2weight[\"softmax_node_\"+str(i)+\"_w\"] = model.add_parameters((len(subtree), contex_size))\n",
    "            self.str2weight[\"softmax_node_\" + str(i) + \"_b\"] = model.add_parameters(len(subtree))\n",
    "        \n",
    "        #create a dictionary from each value to its path\n",
    "        value_to_path_and_nodes_dict = {}\n",
    "        for path, value in self._tree_tools._get_leaves_paths(tree):\n",
    "            nodes = self._tree_tools._get_nodes(tree, path)\n",
    "            value_to_path_and_nodes_dict[data.char2int[value]] = path, nodes\n",
    "        self.value_to_path_and_nodes_dict = value_to_path_and_nodes_dict\n",
    "        self.model = model\n",
    "        self.tree = tree\n",
    "    \n",
    "    #get the loss on a given value (for training)\n",
    "    def get_loss(self, context, value):\n",
    "        loss = []\n",
    "        path, nodes = self.value_to_path_and_nodes_dict[value]\n",
    "        for p, n in zip(path, nodes):\n",
    "            w = dy.parameter(self.str2weight[\"softmax_node_\"+str(n)+\"_w\"])\n",
    "            b = dy.parameter(self.str2weight[\"softmax_node_\" + str(n) + \"_b\"])\n",
    "            probs = tf.nn.softmax(w*context+b)\n",
    "            #loss.append(-tf.math.log(dy.pick(probs, p)))\n",
    "            print(probs)\n",
    "            print(p)\n",
    "        #return dy.esum(loss)\n",
    "\n",
    "    #get the most likely\n",
    "    def generate(self, context):\n",
    "        best_value = None\n",
    "        best_loss = float(100000)\n",
    "        for value in self.value_to_path_and_nodes_dict:\n",
    "            loss = self.get_loss(context, value)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_value = value\n",
    "        return best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/{}_train_set.csv\".format(\"SEG_Wavenet\"), delimiter=\"\\n\", dtype=np.int64)\n",
    "val_set = np.genfromtxt(\"data/{}_val_set.csv\".format(\"SEG_Wavenet\"), delimiter=\"\\n\", dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 897, 242, 961], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "dataset = np.r_[train_set, val_set]\n",
    "dataset"
   ]
  },
  {
   "source": [
    "## NOTE\n",
    "Intermediate Nodes are indexed by **Preorder Traversal**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16293\n",
    "num_of_nodes = vocab_size - 1   # num of intermediate nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{2192: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]],\n",
       " 2396: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]],\n",
       " 2647: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14]],\n",
       " 1579: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14]],\n",
       " 2247: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 16]]}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "with open(\"outputs/tree_mapping.json\", \"r\") as j:\n",
    "    tree_mapping = json.load(j)     # category : [path, nodes in path]\n",
    "tree_mapping = {int(i):j for i, j in tree_mapping.items()}      # As JSON converts key values to string\n",
    "dict(list(tree_mapping.items())[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[0, 1], [0, 1]], [[1, 0, 1], [0, 5436, 5437]])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "tree_mapping[0], tree_mapping[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_matrix = np.full([vocab_size, num_of_nodes], -1, dtype=np.int32)\n",
    "nodes_matrix = np.full([vocab_size, num_of_nodes], -1, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, (path, nodes) in tree_mapping.items():\n",
    "    path_matrix[category][:len(path)] = path\n",
    "    nodes_matrix[category][:len(path)] = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((16293, 16292), (16293, 16292))"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "path_matrix.shape, nodes_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierachicalSoftmax(keras.Model):\n",
    "    def __init__(self, tree_mapping, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_nodes = self.vocab_size - 1\n",
    "        # decision_function = {1:1, 0:-1}\n",
    "        self.decision_function = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(tf.constant([0, 1], dtype=tf.int32), tf.constant([-1, 1], dtype=tf.float32)), -1\n",
    "        )      \n",
    "\n",
    "        path_matrix = np.full([vocab_size, vocab_size-1], -1, dtype=np.int32)\n",
    "        nodes_matrix = np.full([vocab_size, vocab_size-1], -1, dtype=np.int32)\n",
    "        for category, (path, nodes) in tree_mapping.items():\n",
    "            path_matrix[category][:len(path)] = path\n",
    "            nodes_matrix[category][:len(path)] = nodes\n",
    "\n",
    "        self.path_matrix = tf.constant(path_matrix)\n",
    "        self.nodes_matrix = tf.constant(nodes_matrix)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.initializer = keras.initializers.GlorotNormal()\n",
    "        self.softmax_weights = tf.Variable(self.initializer(shape=(self.num_nodes, input_shape[-1])))\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        total_loss = tf.TensorArray(dtype=tf.float32, size=vocab_size)\n",
    "        for i in tf.range(self.vocab_size):\n",
    "            loss = self.get_loss(inputs, i)\n",
    "            total_loss = total_loss.write(i, tf.reduce_prod(loss, axis=-1))\n",
    "            tf.print(i)\n",
    "        return total_loss.stack()\n",
    "        '''\n",
    "        y = tf.constant(0, dtype=tf.int32)\n",
    "        return self.get_loss(inputs, y)\n",
    "\n",
    "    @tf.function\n",
    "    def get_loss(self, x, category):\n",
    "        path = tf.gather(self.path_matrix, category)\n",
    "        nodes = tf.gather(self.nodes_matrix, category)\n",
    "\n",
    "        loss = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(self.num_nodes):\n",
    "            n = tf.gather(nodes, i)\n",
    "            if n == tf.constant(-1):\n",
    "                break\n",
    "            w = tf.gather(self.softmax_weights, n)\n",
    "            d = self.decision_function.lookup(tf.gather(path, i))\n",
    "            #sigma = tf.nn.softmax(w*x)\n",
    "            sigma = tf.math.sigmoid(tf.tensordot(w*x, d, axes=0))\n",
    "            tf.print(sigma) ###\n",
    "            #loss = loss.write(i, tf.tensordot(sigma, d, axes=0))\n",
    "            loss = loss.write(i, sigma)\n",
    "        return tf.reduce_prod(loss.stack(), axis=-1)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        loss = self.get_loss(x, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsm = HierachicalSoftmax(tree_mapping, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "x = tf.constant([0, 1], dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "y = tf.constant([0], dtype=tf.int32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.5 0.497892112]\n",
      "[0.5 0.49898687]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.24894606, 0.24949344], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "hsm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[3, 4],\n",
       "       [6, 8]])>"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "a = tf.constant([1, 2], shape=[2, 1])\n",
    "b = tf.constant([3, 4])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}