{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.6 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"STREAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"BATCH_SIZE\"] = 2    #16\n",
    "param_list[\"DILATIONS\"] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "param_list[\"FILTER_WIDTH\"] = 2                          # == kernel_size\n",
    "param_list[\"SCALAR_INPUT\"] = False\n",
    "param_list[\"INITIAL_FILTER_WIDTH\"] = 32     # Scalar Input\n",
    "param_list[\"DILATION_CHANNELS\"] = 32\n",
    "param_list[\"RESIDUAL_CHANNELS\"] = 24\n",
    "param_list[\"SKIP_CHANNELS\"] = 128\n",
    "# param_list[\"OUT_CHANNELS\"] = 33476                      # == vocab_size\n",
    "param_list[\"USE_BIASES\"] = False\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"SHUFFLE_SEED\"] = 102\n",
    "\n",
    "if param_list[\"SCALAR_INPUT\"]:\n",
    "    param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"INITIAL_FILTER_WIDTH\"]\n",
    "else:\n",
    "    param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"FILTER_WIDTH\"]\n",
    "\n",
    "param_list[\"MULTI_INPUT\"] = True\n",
    "# param_list[\"INPUT_1_CHANNELS\"] = param_list[\"OUT_CHANNELS\"]\n",
    "# param_list[\"INPUT_2_CHANNELS\"] = 1883 #rip_vocab_size"
   ]
  },
  {
   "source": [
    "## Dataset Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpa_threshold_range = (1, 9)\n",
    "gpa_threshold_range = (1, 2)\n",
    "gpa_threshold_interval = 5\n",
    "\n",
    "# rip_threshold_range = (1, 6)\n",
    "rip_threshold_range = (1, 2)\n",
    "rip_threshold_interval = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(gpa_threshold, rip_threshold):\n",
    "    vocabulary_gpa = np.genfromtxt(f\"experiment/static/gpa/gpa_threshold={gpa_threshold}/vocabulary_gpa.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "    gpa_vocab_size = vocabulary_gpa.shape[0]\n",
    "\n",
    "    vocabulary_rip = np.genfromtxt(f\"experiment/static/rip/rip_threshold={rip_threshold}/vocabulary_rip.csv\", delimiter=\"\\n\", dtype=np.uint64)\n",
    "    rip_vocab_size = vocabulary_rip.shape[0]\n",
    "\n",
    "    return gpa_vocab_size, rip_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(gpa_threshold, rip_threshold):\n",
    "    train_gpa = pd.read_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{dataset_name}_train_gpa.csv\", dtype=np.int32)\n",
    "    train_rip = pd.read_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{dataset_name}_train_rip.csv\", dtype=np.int32)\n",
    "    train_set = pd.concat([train_gpa, train_rip], axis=1)\n",
    "\n",
    "    x_train = tf.data.Dataset.from_tensor_slices(train_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "    x_train = x_train.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"])) \n",
    "    x_train = x_train.batch(param_list[\"BATCH_SIZE\"])\n",
    "\n",
    "    y_train = tf.data.Dataset.from_tensor_slices(train_set[param_list[\"RECEPTIVE_FIELD\"]:]['gpa']).window(1, 1, 1, True)\n",
    "    y_train = y_train.flat_map(lambda y: y.batch(1))\n",
    "    y_train = y_train.batch(param_list[\"BATCH_SIZE\"])\n",
    "\n",
    "    train_data = tf.data.Dataset.zip((x_train, y_train)).shuffle(param_list[\"BUFFER_SIZE\"], param_list[\"SHUFFLE_SEED\"], reshuffle_each_iteration=True).prefetch(param_list[\"BUFFER_SIZE\"])\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_dataset(gpa_threshold, rip_threshold):\n",
    "    val_gpa = pd.read_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{dataset_name}_val_gpa.csv\", dtype=np.int32)\n",
    "    val_rip = pd.read_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{dataset_name}_val_rip.csv\", dtype=np.int32)\n",
    "    val_set = pd.concat([val_gpa, val_rip], axis=1)\n",
    "\n",
    "    x_val = tf.data.Dataset.from_tensor_slices(val_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "    x_val = x_val.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"]))\n",
    "    x_val = x_val.batch(param_list[\"BATCH_SIZE\"])\n",
    "\n",
    "    y_val = tf.data.Dataset.from_tensor_slices(val_set[param_list[\"RECEPTIVE_FIELD\"]:]['gpa']).window(1, 1, 1, True)\n",
    "    y_val = y_val.flat_map(lambda y: y.batch(1))\n",
    "    y_val = y_val.batch(param_list[\"BATCH_SIZE\"])\n",
    "\n",
    "    val_data = tf.data.Dataset.zip((x_val, y_val))\n",
    "    return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels        # out_channels == quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, dilation_channels, residual_channels, skip_channels, out_channels, scalar_input=False, initial_filter_width=None, use_biases=False, multi_input=False, input_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width       # Scalar Input\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        # quantization_channels == out_channels\n",
    "        self.out_channels = out_channels             # Same as vocab_size in encoder-decoder\n",
    "        self.use_biases = use_biases\n",
    "        self.multi_input = multi_input\n",
    "\n",
    "        if self.scalar_input:\n",
    "            self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "        else:\n",
    "            self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.filter_width\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def build(self, input_shape):  # Unable to retrieve input_shape when using tf.data.Dataset  \n",
    "        #self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model        \n",
    "        self.output_width = 1\n",
    "\n",
    "        if self.scalar_input:\n",
    "            self.preprocessing_layer = keras.layers.Conv1D(\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=self.initial_filter_width,\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"preprocessing/conv\")\n",
    "        else:\n",
    "            self.preprocessing_layer = keras.layers.Conv1D(\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=self.filter_width,\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for i, dilation in enumerate(self.dilations):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    layer_index=i,\n",
    "                    dilation=dilation, \n",
    "                    filter_width=self.filter_width, \n",
    "                    dilation_channels=self.dilation_channels, \n",
    "                    residual_channels=self.residual_channels, \n",
    "                    skip_channels=self.skip_channels, \n",
    "                    use_biases=self.use_biases, \n",
    "                    output_width=self.output_width)\n",
    "                )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        if not self.scalar_input and not self.multi_input:\n",
    "            inputs = tf.one_hot(inputs, self.out_channels, axis=-1)\n",
    "\n",
    "        if self.multi_input:\n",
    "            split = tf.split(inputs, num_or_size_splits=len(self.input_channels), axis=-1)\n",
    "            tmp = []\n",
    "            for i, s in enumerate(split):\n",
    "                tmp.append(tf.one_hot(s, self.input_channels[i], axis=-1))\n",
    "            inputs = tf.squeeze(tf.concat(tmp, axis=-1), axis=2)\n",
    "        \n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "        \n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "            \n",
    "        skip_sum = tf.math.add_n(skip_outputs)\n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "        \n",
    "        #out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "        #output = sample_from_discretized_mix_logistic(out)             # Generative\n",
    "\n",
    "        #if not training:\n",
    "        #    output = tf.nn.softmax(tf.cast(output, tf.float64))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            #reduced_loss = tf.math.reduce_mean(loss)\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(gpa_threshold, rip_threshold, experiment_epochs=5):    \n",
    "    version_dir = f\"experiment/version/gpa_threshold={gpa_threshold}-rip_threshold={rip_threshold}\"\n",
    "    os.makedirs(version_dir)\n",
    "\n",
    "    gpa_vocab_size, rip_vocab_size = get_vocab_size(gpa_threshold, rip_threshold)\n",
    "\n",
    "    train_data = create_train_dataset(gpa_threshold, rip_threshold)\n",
    "    val_data = create_val_dataset(gpa_threshold, rip_threshold)    \n",
    "\n",
    "    wavenet = WaveNet(\n",
    "        batch_size=param_list[\"BATCH_SIZE\"], \n",
    "        dilations=param_list[\"DILATIONS\"], \n",
    "        filter_width=param_list[\"FILTER_WIDTH\"], \n",
    "        dilation_channels=param_list[\"DILATION_CHANNELS\"], \n",
    "        residual_channels=param_list[\"RESIDUAL_CHANNELS\"], \n",
    "        skip_channels=param_list[\"SKIP_CHANNELS\"], \n",
    "        # out_channels=param_list[\"OUT_CHANNELS\"],\n",
    "        out_channels=gpa_vocab_size,\n",
    "        scalar_input=param_list[\"SCALAR_INPUT\"],\n",
    "        initial_filter_width=param_list[\"INITIAL_FILTER_WIDTH\"],\n",
    "        multi_input=param_list[\"MULTI_INPUT\"], \n",
    "        # input_channels=[param_list[\"INPUT_1_CHANNELS\"], param_list[\"INPUT_2_CHANNELS\"]]\n",
    "        input_channels=[gpa_vocab_size, rip_vocab_size]\n",
    "    )\n",
    "\n",
    "    wavenet.compile(keras.optimizers.Nadam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "    history = wavenet.fit(train_data, epochs=experiment_epochs, validation_data=val_data)\n",
    "    wavenet.save(version_dir)\n",
    "\n",
    "    train_history = pd.DataFrame.from_dict(history.history)\n",
    "    train_history.to_csv(version_dir + \"train_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "for gpa_threshold_i in range(*gpa_threshold_range):\n",
    "    gpa_threshold = gpa_threshold_i * gpa_threshold_interval\n",
    "\n",
    "    for rip_threshold_i in range(*rip_threshold_range):\n",
    "        rip_threshold = rip_threshold_i * rip_threshold_interval\n",
    "\n",
    "        experiment(gpa_threshold, rip_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}