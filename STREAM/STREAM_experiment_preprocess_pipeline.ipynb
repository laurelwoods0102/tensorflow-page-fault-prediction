{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.6 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dill         # 0.3.2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "## Set Global/Environment Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"STREAM\""
   ]
  },
  {
   "source": [
    "## Load Dataset/Static Param List"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   extra             time address_type         gpa  \\\n",
       "0       [336633.698810]   04:01:00:854206           PF   251797631   \n",
       "1       [336633.739463]   04:01:00:894857           PF  1007005696   \n",
       "2       [336633.867033]   04:01:01:022428           PF   906555392   \n",
       "3       [336633.901503]   04:01:01:056898           PF  1914261504   \n",
       "4       [336633.904886]   04:01:01:060282           PF   259330048   \n",
       "...                  ...              ...          ...         ...   \n",
       "180309   [ 1806.225725]   07:00:33:381360           PF   602140104   \n",
       "180310   [ 1806.225827]   07:00:33:381465           PF   597469140   \n",
       "180311   [ 1806.240176]   07:00:33:395810           PF   596315344   \n",
       "180312   [ 1806.240198]   07:00:33:395836           PF   597544452   \n",
       "180313   [ 1806.401068]   07:00:33:556702           PF   603197568   \n",
       "\n",
       "                         rip  vmid  \n",
       "0       18446744072449302655  4034  \n",
       "1       18446744072452043863  4034  \n",
       "2       18446744072452043863  4034  \n",
       "3       18446744072452043863  4034  \n",
       "4       18446744072452043863  4034  \n",
       "...                      ...   ...  \n",
       "180309       139790387006578  4034  \n",
       "180310  18446744072443263295  4034  \n",
       "180311  18446744072442219334  4034  \n",
       "180312  18446744072441579461  4034  \n",
       "180313       139675253067982  4034  \n",
       "\n",
       "[4157042 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>extra</th>\n      <th>time</th>\n      <th>address_type</th>\n      <th>gpa</th>\n      <th>rip</th>\n      <th>vmid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[336633.698810]</td>\n      <td>04:01:00:854206</td>\n      <td>PF</td>\n      <td>251797631</td>\n      <td>18446744072449302655</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[336633.739463]</td>\n      <td>04:01:00:894857</td>\n      <td>PF</td>\n      <td>1007005696</td>\n      <td>18446744072452043863</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[336633.867033]</td>\n      <td>04:01:01:022428</td>\n      <td>PF</td>\n      <td>906555392</td>\n      <td>18446744072452043863</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[336633.901503]</td>\n      <td>04:01:01:056898</td>\n      <td>PF</td>\n      <td>1914261504</td>\n      <td>18446744072452043863</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[336633.904886]</td>\n      <td>04:01:01:060282</td>\n      <td>PF</td>\n      <td>259330048</td>\n      <td>18446744072452043863</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>180309</th>\n      <td>[ 1806.225725]</td>\n      <td>07:00:33:381360</td>\n      <td>PF</td>\n      <td>602140104</td>\n      <td>139790387006578</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>180310</th>\n      <td>[ 1806.225827]</td>\n      <td>07:00:33:381465</td>\n      <td>PF</td>\n      <td>597469140</td>\n      <td>18446744072443263295</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>180311</th>\n      <td>[ 1806.240176]</td>\n      <td>07:00:33:395810</td>\n      <td>PF</td>\n      <td>596315344</td>\n      <td>18446744072442219334</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>180312</th>\n      <td>[ 1806.240198]</td>\n      <td>07:00:33:395836</td>\n      <td>PF</td>\n      <td>597544452</td>\n      <td>18446744072441579461</td>\n      <td>4034</td>\n    </tr>\n    <tr>\n      <th>180313</th>\n      <td>[ 1806.401068]</td>\n      <td>07:00:33:556702</td>\n      <td>PF</td>\n      <td>603197568</td>\n      <td>139675253067982</td>\n      <td>4034</td>\n    </tr>\n  </tbody>\n</table>\n<p>4157042 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "original_dataset = pd.concat([pd.read_csv(\"../로그 데이터/STREAM/STREAM/stream_4034_generic2_{}.csv\".format(i), dtype=np.object) for i in reversed(range(1, 11))], axis=0)\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               gpa                   rip\n",
       "0        251797631  18446744072449302655\n",
       "1       1007005696  18446744072452043863\n",
       "2        906555392  18446744072452043863\n",
       "3       1914261504  18446744072452043863\n",
       "4        259330048  18446744072452043863\n",
       "...            ...                   ...\n",
       "180309   602140104       139790387006578\n",
       "180310   597469140  18446744072443263295\n",
       "180311   596315344  18446744072442219334\n",
       "180312   597544452  18446744072441579461\n",
       "180313   603197568       139675253067982\n",
       "\n",
       "[4156988 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gpa</th>\n      <th>rip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>251797631</td>\n      <td>18446744072449302655</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1007005696</td>\n      <td>18446744072452043863</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>906555392</td>\n      <td>18446744072452043863</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1914261504</td>\n      <td>18446744072452043863</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>259330048</td>\n      <td>18446744072452043863</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>180309</th>\n      <td>602140104</td>\n      <td>139790387006578</td>\n    </tr>\n    <tr>\n      <th>180310</th>\n      <td>597469140</td>\n      <td>18446744072443263295</td>\n    </tr>\n    <tr>\n      <th>180311</th>\n      <td>596315344</td>\n      <td>18446744072442219334</td>\n    </tr>\n    <tr>\n      <th>180312</th>\n      <td>597544452</td>\n      <td>18446744072441579461</td>\n    </tr>\n    <tr>\n      <th>180313</th>\n      <td>603197568</td>\n      <td>139675253067982</td>\n    </tr>\n  </tbody>\n</table>\n<p>4156988 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "original_dataset = original_dataset[[\"gpa\", \"rip\"]].dropna()       # rip for PCs\n",
    "original_dataset"
   ]
  },
  {
   "source": [
    "## Dataset Processing Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateDelta(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X[:-1] - X[1:]\n",
    "        # In case of unsigned types, change its type to string type\n",
    "        if X_transformed.dtype in [np.uint8, np.uint16, np.uint32, np.uint64]:\n",
    "            X_transformed = X_transformed.astype(np.string_)\n",
    "        return X_transformed\n",
    "\n",
    "    def inverse_transform(self, X, y=None):     # Just for test_pipeline.inverse_transform()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTokenizer(TransformerMixin):\n",
    "    def __init__(self, minimum_category_occurence=2, oov_token=-1):        \n",
    "        self.minimum_category_occurence = minimum_category_occurence\n",
    "        self.oov_token = oov_token\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if X.dtype == np.string_:\n",
    "            self.oov_token = str(self.oov_token)\n",
    "\n",
    "        mask = (pd.Series(X).value_counts() <= self.minimum_category_occurence)\n",
    "        noise_index = np.where(np.isin(X, mask.index[mask == True]))[0]\n",
    "    \n",
    "        X[noise_index] = self.oov_token\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, oov_token=-1):\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_counts = pd.Series(X).value_counts()\n",
    "        self.vocab_size = len(X_counts)\n",
    "        self.word_index = X_counts.index\n",
    "        \n",
    "        if X.dtype in [np.dtype(\"S\" + str(i)) for i in range(24)]:  # X.dtype == |S{0~24}\n",
    "            # As np.string_ type is byte type, not str(), need to be decoded.\n",
    "            self.vocabulary = {X_counts.index[i].decode():i for i in range(self.vocab_size)}\n",
    "        else:\n",
    "            self.vocabulary = {X_counts.index[i]:i for i in range(self.vocab_size)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        if X.dtype in [np.dtype(\"S\" + str(i)) for i in range(24)]:\n",
    "            self.oov_token = str(self.oov_token)\n",
    "            for i in range(len(X)):\n",
    "                if X[i] in self.word_index:\n",
    "                    X_transformed.append(self.vocabulary[X[i].decode()])\n",
    "                else:\n",
    "                    X_transformed.append(self.vocabulary[self.oov_token])\n",
    "        else:\n",
    "            for i in range(len(X)):\n",
    "                if X[i] in self.word_index:\n",
    "                    X_transformed.append(self.vocabulary[X[i]])\n",
    "                else:\n",
    "                    X_transformed.append(self.vocabulary[self.oov_token])\n",
    "\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        return np.array([self.word_index[X[i]] for i in range(len(X))])"
   ]
  },
  {
   "source": [
    "## Process Train/Validation Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val / Test Ratio : 70% / 15% / 15%\n",
    "train_val_set, _ = train_test_split(original_dataset, test_size=0.15, shuffle=False)\n",
    "\n",
    "train_val_gpa = train_val_set[\"gpa\"].values.astype(np.int64)\n",
    "train_val_rip = train_val_set[\"rip\"].values.astype(np.uint64)      # As uint64 not exists in pandas"
   ]
  },
  {
   "source": [
    "## Process Dataset per given Dataset Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_threshold_range = (1, 9)\n",
    "gpa_threshold_interval = 5\n",
    "\n",
    "rip_threshold_range = (1, 6)\n",
    "rip_threshold_interval = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpa_threshold_i in range(*gpa_threshold_range):\n",
    "    gpa_threshold = gpa_threshold_interval * gpa_threshold_i\n",
    "\n",
    "    # Make dirs\n",
    "    os.makedirs(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}\")\n",
    "    os.makedirs(f\"experiment/static/gpa_threshold={gpa_threshold}\")\n",
    "\n",
    "    gpa_train_pipeline = Pipeline([\n",
    "        ('calculate_delta', CalculateDelta()),\n",
    "        ('noise_tokenizer', NoiseTokenizer(minimum_category_occurence=gpa_threshold)),\n",
    "        ('sparse_category_encoder', SparseCategoryEncoder())\n",
    "    ])\n",
    "\n",
    "    # Process\n",
    "    processed_train_val_gpa = gpa_train_pipeline.fit_transform(train_val_gpa.copy())\n",
    "\n",
    "    # train/val split\n",
    "    processed_train_gpa, processed_val_gpa = train_test_split(processed_train_val_gpa, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # train/val original split\n",
    "    train_gpa = train_val_gpa[:processed_train_gpa.shape[0]+1]\n",
    "    val_gpa = train_val_gpa[processed_train_gpa.shape[0]:]\n",
    "\n",
    "    # to dataframe\n",
    "    processed_train_gpa = pd.DataFrame(processed_train_gpa, columns=[\"gpa\"], index=None)\n",
    "    processed_val_gpa = pd.DataFrame(processed_val_gpa, columns=[\"gpa\"], index=None)\n",
    "\n",
    "    train_gpa = pd.DataFrame(train_gpa, columns=[\"gpa\"], index=None)\n",
    "    val_gpa = pd.DataFrame(val_gpa, columns=[\"gpa\"], index=None)\n",
    "\n",
    "    # Save Dataset\n",
    "    processed_train_gpa.to_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{model_name}_train_gpa.csv\", index=None)\n",
    "    processed_val_gpa.to_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{model_name}_val_gpa.csv\", index=None)\n",
    "\n",
    "    train_gpa.to_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{model_name}_train_gpa_original.csv\", index=None)\n",
    "    val_gpa.to_csv(f\"experiment/data/gpa/gpa_threshold={gpa_threshold}/{model_name}_val_gpa_original.csv\", index=None)\n",
    "\n",
    "    # Save pipeline\n",
    "    with open(f\"experiment/static/gpa/gpa_threshold={gpa_threshold}/pipeline_gpa.pkl\", 'wb') as f:\n",
    "        dill.dump(gpa_train_pipeline, f)\n",
    "    np.savetxt(f\"experiment/static/gpa/gpa_threshold={gpa_threshold}/vocabulary_gpa.csv\", np.array(list(gpa_train_pipeline[\"sparse_category_encoder\"].vocabulary.keys())), fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rip_threshold_i in range(*rip_threshold_range):\n",
    "    rip_threshold = rip_threshold_interval * rip_threshold_i\n",
    "\n",
    "    # Make dirs\n",
    "    os.makedirs(f\"experiment/data/rip/rip_threshold={rip_threshold}\")\n",
    "    os.makedirs(f\"experiment/static/rip_threshold={rip_threshold}\")\n",
    "\n",
    "    rip_train_pipeline = Pipeline([\n",
    "        ('calculate_delta', CalculateDelta()),\n",
    "        ('noise_tokenizer', NoiseTokenizer(minimum_category_occurence=rip_threshold)),\n",
    "        ('sparse_category_encoder', SparseCategoryEncoder())\n",
    "    ])\n",
    "\n",
    "    # Process\n",
    "    processed_train_val_rip = rip_train_pipeline.fit_transform(train_val_rip.copy())\n",
    "\n",
    "    # train/val split\n",
    "    processed_train_rip, processed_val_rip = train_test_split(processed_train_val_rip, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # train/val original split\n",
    "    train_rip = train_val_rip[:processed_train_rip.shape[0]+1]\n",
    "    val_rip = train_val_rip[processed_train_rip.shape[0]:]\n",
    "\n",
    "    # to dataframe\n",
    "    processed_train_rip = pd.DataFrame(processed_train_rip, columns=[\"rip\"], index=None)\n",
    "    processed_val_rip = pd.DataFrame(processed_val_rip, columns=[\"rip\"], index=None)\n",
    "\n",
    "    train_rip = pd.DataFrame(train_rip, columns=[\"rip\"], index=None)\n",
    "    val_rip = pd.DataFrame(val_rip, columns=[\"rip\"], index=None)\n",
    "\n",
    "    # Save Dataset\n",
    "    processed_train_rip.to_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{model_name}_train_rip.csv\", index=None)\n",
    "    processed_val_rip.to_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{model_name}_val_rip.csv\", index=None)\n",
    "\n",
    "    train_rip.to_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{model_name}_train_rip_original.csv\", index=None)\n",
    "    val_rip.to_csv(f\"experiment/data/rip/rip_threshold={rip_threshold}/{model_name}_val_rip_original.csv\", index=None)\n",
    "\n",
    "    # Save pipeline\n",
    "    with open(f\"experiment/static/rip/rip_threshold={rip_threshold}/pipeline_rip.pkl\", 'wb') as f:\n",
    "        dill.dump(rip_train_pipeline, f)\n",
    "    np.savetxt(f\"experiment/static/rip/rip_threshold={rip_threshold}/vocabulary_rip.csv\", np.array(list(rip_train_pipeline[\"sparse_category_encoder\"].vocabulary.keys())), fmt=\"%s\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}