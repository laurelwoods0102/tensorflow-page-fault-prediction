{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600433816059",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dill\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global/Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NU_AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'data/'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-7e17cbf1c2df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"static/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'data/'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"data/\")\n",
    "os.mkdir(\"static/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset/Static Param List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([          -1, 105950216192, 105943924736, ..., 103563653120,\n       103565225984, 103560867840], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "original_dataset = np.genfromtxt(\"../로그 데이터/NU-MineBench.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "original_dataset"
   ]
  },
  {
   "source": [
    "## Segregate Initial/Main/Terminal Stages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_stage_index = 793\n",
    "terminal_stage_index = 10502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([104287174656, 104289271808, 104282980352, ..., 103984320512,\n       103984324608, 103984328704], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "dataset = original_dataset[initial_stage_index:terminal_stage_index]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateDelta(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([X[i+1] - X[i] for i in range(int(len(X))-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTokenizer(TransformerMixin):\n",
    "    def __init__(self, minimum_category_occurence=2, oov_token=-1):\n",
    "        self.minimum_category_occurence = minimum_category_occurence\n",
    "        self.oov_token = oov_token\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        mask = (pd.Series(X).value_counts() <= self.minimum_category_occurence)\n",
    "        noise_index = np.where(np.isin(X, mask.index[mask == True]))[0]\n",
    "        X[noise_index] = self.oov_token\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, oov_token=-1):\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_counts = pd.Series(X).value_counts()\n",
    "        self.vocab_size = len(X_counts)\n",
    "        self.word_index = X_counts.index\n",
    "        self.vocabulary = {X_counts.index[i]:i for i in range(self.vocab_size)}\n",
    "\n",
    "        if -1 not in self.word_index:\n",
    "            self.vocabulary[-1] = self.vocab_size\n",
    "            self.vocab_size += 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] in self.word_index:\n",
    "                X_transformed.append(self.vocabulary[X[i]])\n",
    "            else:\n",
    "                X_transformed.append(self.vocabulary[self.oov_token])\n",
    "\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        return np.array([self.word_index[X[i]] for i in range(len(X))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Train/Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val / Test Ratio : 70% / 15% / 15%\n",
    "train_val_set, test_set = train_test_split(dataset, test_size=0.15, shuffle=False)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = Pipeline([\n",
    "    ('calculate_delta', CalculateDelta()),\n",
    "    ('noise_tokenizer', NoiseTokenizer()),\n",
    "    ('sparse_category_encoder', SparseCategoryEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 1, 0, ..., 0, 0, 0])"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "processed_train_val_set = train_pipeline.fit_transform(train_val_set.copy())\n",
    "processed_train_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set, processed_val_set = train_test_split(processed_train_val_set, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Datasets/Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/{}_train_set_original.csv\".format(model_name), train_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "np.savetxt(\"data/{}_test_set_original.csv\".format(model_name), test_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "\n",
    "np.savetxt(\"data/{}_train_set.csv\".format(model_name), processed_train_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "np.savetxt(\"data/{}_val_set.csv\".format(model_name), processed_val_set, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = np.r_[train_set[-1], val_set]  # As one data point is lost during CalculateDelta process\n",
    "np.savetxt(\"data/{}_val_set_original.csv\".format(model_name), validation_set, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline([\n",
    "    ('calculate_delta', CalculateDelta()),\n",
    "    ('sparse_category_encoder', train_pipeline[\"sparse_category_encoder\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"static/pipeline.pkl\", 'wb') as f:\n",
    "    dill.dump(test_pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{4096: 0, -6291456: 1, 2097152: 2, 0: 3, -8384512: 4, -1: 5}"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "dict(list(test_pipeline[\"sparse_category_encoder\"].vocabulary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "test_pipeline[\"sparse_category_encoder\"].vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}