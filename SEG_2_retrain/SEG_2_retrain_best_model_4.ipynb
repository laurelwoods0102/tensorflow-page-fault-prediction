{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_2_retrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'20201203-180335'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(\"version\"):\n",
    "    os.makedirs(\"version\")\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "timestamp"
   ]
  },
  {
   "source": [
    "vocabulary = np.genfromtxt(\"static/vocabulary.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "vocab_size = vocabulary.shape[0]\n",
    "vocab_size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tuned Hyper-parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import dill\n",
    "\n",
    "with open(\"best_hps/ht_4/best_hp_0.pkl\", \"rb\") as f:\n",
    "    best_hp_0 = dill.load(f)\n",
    "best_hp_0.values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"BATCH_SIZE\"] = 128\n",
    "param_list[\"EPOCHS\"] = 1000         # Early Stopping\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"VOCAB_SIZE\"] = 16293    #vocab_size\n",
    "\n",
    "# Tuned HPs\n",
    "param_list['ENCODER_1_NEURONS'] = 192\n",
    "param_list['ENCODER_1_DROPOUT'] = 0.1\n",
    "param_list['ENCODER_2_NEURONS'] = 32\n",
    "param_list['ENCODER_2_DROPOUT'] = 0.1\n",
    "param_list['DECODER_1_NEURONS'] = 48\n",
    "param_list['DECODER_DROPOUT_1'] = 0.2\n",
    "param_list['DECODER_2_NEURONS'] = 224\n",
    "param_list['DECODER_DROPOUT_2'] = 0.4\n",
    "#param_list['LEARNING_RATE'] = 0.001        # Nadam recommends to use default lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/{}_train_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_set[:-param_list[\"FUTURE_TARGET\"]]).window(param_list[\"PAST_HISTORY\"], 1, 1, True)\n",
    "# As dataset.window() returns \"dataset\", not \"tensor\", need to flat_map() it with sequence length\n",
    "x_train = x_train.flat_map(lambda x: x.batch(param_list[\"PAST_HISTORY\"])) \n",
    "x_train = x_train.map(lambda x: tf.one_hot(x, param_list[\"VOCAB_SIZE\"], axis=-1))\n",
    "x_train = x_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.data.Dataset.from_tensor_slices(train_set[param_list[\"PAST_HISTORY\"]:]).window(param_list[\"FUTURE_TARGET\"], 1, 1, True)\n",
    "y_train = y_train.flat_map(lambda y: y.batch(param_list[\"FUTURE_TARGET\"]))\n",
    "y_train = y_train.map(lambda y: tf.one_hot(y, param_list[\"VOCAB_SIZE\"], axis=-1))\n",
    "y_train = y_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip((x_train, y_train))#.shuffle(param_list[\"BUFFER_SIZE\"], seed=param_list[\"SHUFFLE_SEED\"])   # shuffle not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/{}_val_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tf.data.Dataset.from_tensor_slices(val_set[:-param_list[\"FUTURE_TARGET\"]]).window(param_list[\"PAST_HISTORY\"], 1, 1, True)\n",
    "x_val = x_val.flat_map(lambda x: x.batch(param_list[\"PAST_HISTORY\"]))\n",
    "x_val = x_val.map(lambda x: tf.one_hot(x, param_list[\"VOCAB_SIZE\"], axis=-1))\n",
    "x_val = x_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = tf.data.Dataset.from_tensor_slices(val_set[param_list[\"PAST_HISTORY\"]:]).window(param_list[\"FUTURE_TARGET\"], 1, 1, True)\n",
    "y_val = y_val.flat_map(lambda y: y.batch(param_list[\"FUTURE_TARGET\"]))\n",
    "y_val = y_val.map(lambda y: tf.one_hot(y, param_list[\"VOCAB_SIZE\"], axis=-1))\n",
    "y_val = y_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.data.Dataset.zip((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(param_list[\"ENCODER_1_NEURONS\"], return_sequences=True)))\n",
    "model.add(keras.layers.Dropout(param_list[\"ENCODER_1_DROPOUT\"]))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(param_list[\"ENCODER_2_NEURONS\"])))\n",
    "model.add(keras.layers.Dropout(param_list[\"ENCODER_2_DROPOUT\"]))\n",
    "model.add(keras.layers.RepeatVector(param_list[\"FUTURE_TARGET\"]))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(param_list[\"DECODER_1_NEURONS\"], return_sequences=True)))\n",
    "model.add(keras.layers.Dropout(param_list[\"DECODER_DROPOUT_1\"]))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(param_list[\"DECODER_2_NEURONS\"], return_sequences=True)))\n",
    "model.add(keras.layers.Dropout(param_list[\"DECODER_DROPOUT_2\"]))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(param_list[\"VOCAB_SIZE\"], activation='softmax')))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1260/1260 [==============================] - 180s 143ms/step - loss: 6.0492 - accuracy: 0.3660 - val_loss: 7.8757 - val_accuracy: 0.2121\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(train_data, epochs=param_list[\"EPOCHS\"], validation_data=val_data, callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5), ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: version/20201203-180335\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version Issue\n",
    "model.save(version_dir + \"/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}