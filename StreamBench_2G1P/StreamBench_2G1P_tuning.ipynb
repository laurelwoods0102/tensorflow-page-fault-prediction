{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597666763391",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'20200817-215220'"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + timestamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(version_dir)\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"StreamBench_2G1P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t           t+1    delta  tokenized_data\n0  106749231104  106749231104        0               0\n1  106749231104  106751328256  2097152         2097152\n2  106751328256  106751328256        0               0\n3  106751328256  106745036800 -6291456        -6291456\n4  106745036800  106745036800        0               0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t</th>\n      <th>t+1</th>\n      <th>delta</th>\n      <th>tokenized_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106749231104</td>\n      <td>106749231104</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>106749231104</td>\n      <td>106751328256</td>\n      <td>2097152</td>\n      <td>2097152</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106751328256</td>\n      <td>106751328256</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>106751328256</td>\n      <td>106745036800</td>\n      <td>-6291456</td>\n      <td>-6291456</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106745036800</td>\n      <td>106745036800</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/{}_train_set.csv\".format(dataset_name))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"BATCH_SIZE\"] = 1024\n",
    "param_list[\"EPOCHS\"] = 500\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"NUM_1_NEURONS\"] = 64\n",
    "param_list[\"NUM_2_NEURONS\"] = 64\n",
    "\n",
    "with open(\"version/{}/params.json\".format(timestamp), \"w\") as p:\n",
    "    json.dump(param_list, p, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size, n_features):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, n_features)))\n",
    "        labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<1x5 sparse matrix of type '<class 'numpy.float32'>'\n \twith 1 stored elements in Compressed Sparse Row format>,\n [array([-6291456,       -1,        0,     4096,  2097152], dtype=int64)])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(dtype=np.float32)\n",
    "encoded_data = encoder.fit_transform(dataset[\"tokenized_data\"].values.reshape(-1, 1))\n",
    "encoded_data[0], encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((112878, 16, 5), (112878, 8, 5))"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "x_train, y_train = generate_timeseries(encoded_data.toarray(), 0, None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"], len(encoder.categories_[0]))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(param_list[\"NUM_1_NEURONS\"])))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.RepeatVector(param_list[\"FUTURE_TARGET\"]))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(param_list[\"NUM_2_NEURONS\"], return_sequences=True)))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(keras.layers.TimeDistributed(tf.keras.layers.Dense(len(encoder.categories_[0]), activation=\"softmax\")))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/89 [==============================] - 1s 10ms/step - loss: 0.0153 - accuracy: 0.9974 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 363/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9975 - val_loss: 0.0570 - val_accuracy: 0.9898\nEpoch 364/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0566 - val_accuracy: 0.9898\nEpoch 365/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0553 - val_accuracy: 0.9898\nEpoch 366/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.0567 - val_accuracy: 0.9898\nEpoch 367/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0579 - val_accuracy: 0.9898\nEpoch 368/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0554 - val_accuracy: 0.9898\nEpoch 369/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0568 - val_accuracy: 0.9898\nEpoch 370/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0585 - val_accuracy: 0.9898\nEpoch 371/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 372/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0587 - val_accuracy: 0.9898\nEpoch 373/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0588 - val_accuracy: 0.9898\nEpoch 374/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 375/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0563 - val_accuracy: 0.9898\nEpoch 376/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0571 - val_accuracy: 0.9898\nEpoch 377/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9975 - val_loss: 0.0568 - val_accuracy: 0.9898\nEpoch 378/500\n89/89 [==============================] - 1s 11ms/step - loss: 0.0154 - accuracy: 0.9974 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 379/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0575 - val_accuracy: 0.9898\nEpoch 380/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0576 - val_accuracy: 0.9898\nEpoch 381/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0583 - val_accuracy: 0.9898\nEpoch 382/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0586 - val_accuracy: 0.9898\nEpoch 383/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0568 - val_accuracy: 0.9898\nEpoch 384/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.0593 - val_accuracy: 0.9898\nEpoch 385/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.0575 - val_accuracy: 0.9898\nEpoch 386/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9977 - val_loss: 0.0568 - val_accuracy: 0.9898\nEpoch 387/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 388/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0564 - val_accuracy: 0.9898\nEpoch 389/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0561 - val_accuracy: 0.9898\nEpoch 390/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 391/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0589 - val_accuracy: 0.9898\nEpoch 392/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0562 - val_accuracy: 0.9898\nEpoch 393/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0600 - val_accuracy: 0.9898\nEpoch 394/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0577 - val_accuracy: 0.9898\nEpoch 395/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0575 - val_accuracy: 0.9898\nEpoch 396/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0596 - val_accuracy: 0.9898\nEpoch 397/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0587 - val_accuracy: 0.9898\nEpoch 398/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0577 - val_accuracy: 0.9898\nEpoch 399/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 400/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0581 - val_accuracy: 0.9898\nEpoch 401/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0587 - val_accuracy: 0.9898\nEpoch 402/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0566 - val_accuracy: 0.9898\nEpoch 403/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0596 - val_accuracy: 0.9898\nEpoch 404/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0584 - val_accuracy: 0.9898\nEpoch 405/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0570 - val_accuracy: 0.9898\nEpoch 406/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0604 - val_accuracy: 0.9898\nEpoch 407/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 408/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0569 - val_accuracy: 0.9898\nEpoch 409/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 410/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0580 - val_accuracy: 0.9898\nEpoch 411/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0558 - val_accuracy: 0.9898\nEpoch 412/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0599 - val_accuracy: 0.9898\nEpoch 413/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 414/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0586 - val_accuracy: 0.9898\nEpoch 415/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0576 - val_accuracy: 0.9898\nEpoch 416/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0577 - val_accuracy: 0.9898\nEpoch 417/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0557 - val_accuracy: 0.9898\nEpoch 418/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0574 - val_accuracy: 0.9898\nEpoch 419/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0564 - val_accuracy: 0.9898\nEpoch 420/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 421/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0589 - val_accuracy: 0.9898\nEpoch 422/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0564 - val_accuracy: 0.9898\nEpoch 423/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0571 - val_accuracy: 0.9898\nEpoch 424/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0557 - val_accuracy: 0.9898\nEpoch 425/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9975 - val_loss: 0.0566 - val_accuracy: 0.9898\nEpoch 426/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0150 - accuracy: 0.9975 - val_loss: 0.0576 - val_accuracy: 0.9898\nEpoch 427/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 428/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0583 - val_accuracy: 0.9898\nEpoch 429/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0540 - val_accuracy: 0.9898\nEpoch 430/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0575 - val_accuracy: 0.9898\nEpoch 431/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0559 - val_accuracy: 0.9898\nEpoch 432/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0582 - val_accuracy: 0.9898\nEpoch 433/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 434/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0586 - val_accuracy: 0.9898\nEpoch 435/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0584 - val_accuracy: 0.9898\nEpoch 436/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 437/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 438/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0579 - val_accuracy: 0.9898\nEpoch 439/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 440/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0577 - val_accuracy: 0.9898\nEpoch 441/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 442/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0579 - val_accuracy: 0.9898\nEpoch 443/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0589 - val_accuracy: 0.9898\nEpoch 444/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 445/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0580 - val_accuracy: 0.9898\nEpoch 446/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0589 - val_accuracy: 0.9898\nEpoch 447/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0590 - val_accuracy: 0.9898\nEpoch 448/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0556 - val_accuracy: 0.9898\nEpoch 449/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0580 - val_accuracy: 0.9898\nEpoch 450/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 451/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0579 - val_accuracy: 0.9898\nEpoch 452/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0547 - val_accuracy: 0.9898\nEpoch 453/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0588 - val_accuracy: 0.9898\nEpoch 454/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 455/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 456/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0585 - val_accuracy: 0.9898\nEpoch 457/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 458/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0562 - val_accuracy: 0.9898\nEpoch 459/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 460/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0586 - val_accuracy: 0.9898\nEpoch 461/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0565 - val_accuracy: 0.9898\nEpoch 462/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0581 - val_accuracy: 0.9898\nEpoch 463/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0576 - val_accuracy: 0.9898\nEpoch 464/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0585 - val_accuracy: 0.9898\nEpoch 465/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0582 - val_accuracy: 0.9898\nEpoch 466/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0565 - val_accuracy: 0.9898\nEpoch 467/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0569 - val_accuracy: 0.9898\nEpoch 468/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0594 - val_accuracy: 0.9898\nEpoch 469/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 470/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0566 - val_accuracy: 0.9898\nEpoch 471/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0596 - val_accuracy: 0.9898\nEpoch 472/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0596 - val_accuracy: 0.9898\nEpoch 473/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0595 - val_accuracy: 0.9898\nEpoch 474/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0576 - val_accuracy: 0.9898\nEpoch 475/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0590 - val_accuracy: 0.9898\nEpoch 476/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 477/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0567 - val_accuracy: 0.9898\nEpoch 478/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0596 - val_accuracy: 0.9898\nEpoch 479/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0582 - val_accuracy: 0.9898\nEpoch 480/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0574 - val_accuracy: 0.9898\nEpoch 481/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0581 - val_accuracy: 0.9898\nEpoch 482/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0581 - val_accuracy: 0.9898\nEpoch 483/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 484/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0568 - val_accuracy: 0.9898\nEpoch 485/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0573 - val_accuracy: 0.9898\nEpoch 486/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0591 - val_accuracy: 0.9898\nEpoch 487/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0585 - val_accuracy: 0.9898\nEpoch 488/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0572 - val_accuracy: 0.9898\nEpoch 489/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9977 - val_loss: 0.0583 - val_accuracy: 0.9898\nEpoch 490/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0584 - val_accuracy: 0.9898\nEpoch 491/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.0582 - val_accuracy: 0.9898\nEpoch 492/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0589 - val_accuracy: 0.9898\nEpoch 493/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0577 - val_accuracy: 0.9898\nEpoch 494/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0574 - val_accuracy: 0.9898\nEpoch 495/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0150 - accuracy: 0.9975 - val_loss: 0.0578 - val_accuracy: 0.9898\nEpoch 496/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0585 - val_accuracy: 0.9898\nEpoch 497/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.0569 - val_accuracy: 0.9898\nEpoch 498/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9975 - val_loss: 0.0581 - val_accuracy: 0.9898\nEpoch 499/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0588 - val_accuracy: 0.9898\nEpoch 500/500\n89/89 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0581 - val_accuracy: 0.9898\n"
    }
   ],
   "source": [
    "model_history = model.fit(x_train, y_train, batch_size=param_list[\"BATCH_SIZE\"], validation_split=0.2, epochs=param_list[\"EPOCHS\"])\n",
    "model.save(\"version/{}/model.h5\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t           t+1  delta  tokenized_data\n0  102762393600  102762393600      0               0\n1  102762393600  102762397696   4096            4096\n2  102762397696  102762397696      0               0\n3  102762397696  102762401792   4096            4096\n4  102762401792  102762401792      0               0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t</th>\n      <th>t+1</th>\n      <th>delta</th>\n      <th>tokenized_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102762393600</td>\n      <td>102762393600</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102762393600</td>\n      <td>102762397696</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102762397696</td>\n      <td>102762397696</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102762397696</td>\n      <td>102762401792</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>102762401792</td>\n      <td>102762401792</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/{}_test_set.csv\".format(dataset_name))\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<1x5 sparse matrix of type '<class 'numpy.float32'>'\n \twith 1 stored elements in Compressed Sparse Row format>,\n [array([-6291456,       -1,        0,     4096,  2097152], dtype=int64)])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "test_encoder = OneHotEncoder(dtype=np.float32)\n",
    "encoded_test_data = test_encoder.fit_transform(test_dataset[\"tokenized_data\"].values.reshape(-1, 1))\n",
    "encoded_test_data[0], test_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_timeseries(encoded_test_data.toarray(), 0, None, 16, 8, len(test_encoder.categories_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_true.extend(np.argmax(y_test[i], axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    y_pred.extend(np.argmax(model.predict(x_test[i].reshape(1, 16, 5))[0], axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[1.0693275e-09, 2.1834114e-07, 9.9999499e-01, 4.7102244e-06,\n         3.3979104e-11],\n        [2.3365993e-04, 7.2977581e-04, 1.9987993e-04, 9.9882382e-01,\n         1.2825866e-05],\n        [1.6047409e-09, 1.0699457e-07, 9.9985671e-01, 1.4315771e-04,\n         2.0312559e-11],\n        [2.5425880e-04, 7.4768625e-04, 3.5318063e-04, 9.9835891e-01,\n         2.8583620e-04],\n        [9.2985941e-10, 5.9050532e-08, 9.9969339e-01, 3.0654480e-04,\n         1.0667801e-10],\n        [5.1631802e-04, 7.2454673e-04, 4.7993491e-04, 9.9799734e-01,\n         2.8180063e-04],\n        [4.3889949e-09, 3.1290847e-07, 9.9958223e-01, 4.1735955e-04,\n         1.2785907e-10],\n        [4.7631745e-04, 6.7538983e-04, 5.6909717e-04, 9.9774998e-01,\n         5.2916544e-04]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "model.predict(x_test[0].reshape(1, 16, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 3, 2, 3, 2, 3, 2, 3], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.argmax(model.predict(x_test[0].reshape(1, 16, 5))[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9481288221217761"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "with open(\"version/{}/accuracy.txt\".format(timestamp), \"w\") as t:\n",
    "    t.write(str(accuracy.tolist()))\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score        support\n0              0.918061  0.910908  0.914470    2952.000000\n1              0.180929  0.113497  0.139491     652.000000\n2              0.994090  0.915866  0.953376  163264.000000\n3              0.903605  0.994154  0.946719  131036.000000\n4              0.916836  0.911290  0.914055    2976.000000\naccuracy       0.948129  0.948129  0.948129       0.948129\nmacro avg      0.782704  0.769143  0.773622  300880.000000\nweighted avg   0.951411  0.948129  0.947943  300880.000000\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).transpose()\n",
    "report.to_csv(\"version/{}/report.csv\".format(timestamp))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}