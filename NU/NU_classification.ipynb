{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597330989374",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'20200814-030220'"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + timestamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(version_dir)\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t           t+1    delta  tokenized_data\n0  105950216192  105943924736 -6291456        -6291456\n1  105943924736  105946021888  2097152         2097152\n2  105946021888  105939873792 -6148096              -1\n3  105939873792  105941845296  1971504              -1\n4  105941845296  105935536128 -6309168              -1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t</th>\n      <th>t+1</th>\n      <th>delta</th>\n      <th>tokenized_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>105950216192</td>\n      <td>105943924736</td>\n      <td>-6291456</td>\n      <td>-6291456</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>105943924736</td>\n      <td>105946021888</td>\n      <td>2097152</td>\n      <td>2097152</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>105946021888</td>\n      <td>105939873792</td>\n      <td>-6148096</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>105939873792</td>\n      <td>105941845296</td>\n      <td>1971504</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>105941845296</td>\n      <td>105935536128</td>\n      <td>-6309168</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/NU_train_set.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"PAST_HISTORY\"] = 16\n",
    "param_list[\"FUTURE_TARGET\"] = 8\n",
    "param_list[\"BATCH_SIZE\"] = 1024\n",
    "param_list[\"EPOCHS\"] = 250\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "\n",
    "with open(\"version/{}/params.json\".format(timestamp), \"w\") as p:\n",
    "    json.dump(param_list, p, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 5)))\n",
    "        labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<1x5 sparse matrix of type '<class 'numpy.float64'>'\n \twith 1 stored elements in Compressed Sparse Row format>,\n [array([-6291456,       -1,        0,     4096,  2097152], dtype=int64)])"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()   #dtype=np.int64\n",
    "encoded_data = encoder.fit_transform(dataset[\"tokenized_data\"].values.reshape(-1, 1))\n",
    "encoded_data[0], encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(encoder, \"data/encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_timeseries(encoded_data.toarray(), 0, None, param_list[\"PAST_HISTORY\"], param_list[\"FUTURE_TARGET\"])\n",
    "\n",
    "#train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "#train_data = train_data.cache().shuffle(param_list[\"BUFFER_SIZE\"]).batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(47958, 16, 5)"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(47958, 8, 5)"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(5))  #x_train.shape[-2:] , input_shape=[16, 5] , return_sequences=True\n",
    "#model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.RepeatVector(8))\n",
    "model.add(keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation=\"softmax\")))\n",
    "#model.add(tf.keras.layers.Flatten())\n",
    "#model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5)))  #x_train.shape[-2:] , input_shape=[16, 5] , return_sequences=True\n",
    "model.add(tf.keras.layers.RepeatVector(8))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5, return_sequences=True)))\n",
    "model.add(keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation=\"softmax\")))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.9809 - val_loss: 0.0202 - val_accuracy: 0.9965\nEpoch 112/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.9821 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 113/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0505 - accuracy: 0.9816 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 114/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.9822 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 115/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.9818 - val_loss: 0.0201 - val_accuracy: 0.9967\nEpoch 116/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 117/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 118/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0204 - val_accuracy: 0.9966\nEpoch 119/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0502 - accuracy: 0.9820 - val_loss: 0.0202 - val_accuracy: 0.9967\nEpoch 120/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0502 - accuracy: 0.9821 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 121/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 122/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 123/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0501 - accuracy: 0.9822 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 124/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0501 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 125/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 0.0202 - val_accuracy: 0.9967\nEpoch 126/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9812 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 127/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9816 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 128/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9813 - val_loss: 0.0203 - val_accuracy: 0.9966\nEpoch 129/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9815 - val_loss: 0.0203 - val_accuracy: 0.9966\nEpoch 130/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9818 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 131/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.0201 - val_accuracy: 0.9967\nEpoch 132/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0499 - accuracy: 0.9826 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 133/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0499 - accuracy: 0.9824 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 134/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9831 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 135/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 136/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9829 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 137/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.0201 - val_accuracy: 0.9967\nEpoch 138/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9826 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 139/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9823 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 140/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9818 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 141/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 142/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9814 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 143/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 144/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9814 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 145/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 146/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 147/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 148/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 149/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 150/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 151/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 152/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9813 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 153/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9819 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 154/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 155/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9818 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 156/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9817 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 157/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9820 - val_loss: 0.0205 - val_accuracy: 0.9965\nEpoch 158/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 159/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9817 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 160/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9815 - val_loss: 0.0204 - val_accuracy: 0.9966\nEpoch 161/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9812 - val_loss: 0.0200 - val_accuracy: 0.9967\nEpoch 162/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 163/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9819 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 164/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9965\nEpoch 165/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 166/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9817 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 167/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9818 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 168/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 169/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9817 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 170/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 171/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 172/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 173/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 174/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9814 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 175/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 176/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 177/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9815 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 178/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 179/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 180/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 181/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 182/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9818 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 183/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 184/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9820 - val_loss: 0.0202 - val_accuracy: 0.9966\nEpoch 185/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9821 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 186/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9816 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 187/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9820 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 188/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9818 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 189/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 190/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9822 - val_loss: 0.0202 - val_accuracy: 0.9965\nEpoch 191/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 192/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9819 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 193/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 194/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9817 - val_loss: 0.0201 - val_accuracy: 0.9965\nEpoch 195/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 196/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9819 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 197/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9819 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 198/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9817 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 199/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9817 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 200/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9815 - val_loss: 0.0196 - val_accuracy: 0.9966\nEpoch 201/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9820 - val_loss: 0.0202 - val_accuracy: 0.9965\nEpoch 202/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 203/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9814 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 204/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 205/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 206/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 207/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9967\nEpoch 208/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 209/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9819 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 210/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9817 - val_loss: 0.0196 - val_accuracy: 0.9967\nEpoch 211/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9818 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 212/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9820 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 213/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9817 - val_loss: 0.0200 - val_accuracy: 0.9965\nEpoch 214/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 215/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 216/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9821 - val_loss: 0.0200 - val_accuracy: 0.9965\nEpoch 217/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 218/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 219/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.9820 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 220/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9822 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 221/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9819 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 222/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9821 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 223/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9818 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 224/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.9815 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 225/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 226/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9818 - val_loss: 0.0196 - val_accuracy: 0.9966\nEpoch 227/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9821 - val_loss: 0.0201 - val_accuracy: 0.9965\nEpoch 228/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9824 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 229/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 230/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9817 - val_loss: 0.0200 - val_accuracy: 0.9965\nEpoch 231/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9819 - val_loss: 0.0196 - val_accuracy: 0.9967\nEpoch 232/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9818 - val_loss: 0.0196 - val_accuracy: 0.9967\nEpoch 233/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0485 - accuracy: 0.9822 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 234/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9824 - val_loss: 0.0199 - val_accuracy: 0.9965\nEpoch 235/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9816 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 236/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9819 - val_loss: 0.0195 - val_accuracy: 0.9967\nEpoch 237/250\n38/38 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 0.0200 - val_accuracy: 0.9966\nEpoch 238/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9824 - val_loss: 0.0196 - val_accuracy: 0.9967\nEpoch 239/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 240/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9821 - val_loss: 0.0199 - val_accuracy: 0.9965\nEpoch 241/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9824 - val_loss: 0.0198 - val_accuracy: 0.9967\nEpoch 242/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9825 - val_loss: 0.0201 - val_accuracy: 0.9966\nEpoch 243/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0483 - accuracy: 0.9824 - val_loss: 0.0200 - val_accuracy: 0.9965\nEpoch 244/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0483 - accuracy: 0.9827 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 245/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0482 - accuracy: 0.9831 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 246/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 247/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.9826 - val_loss: 0.0199 - val_accuracy: 0.9966\nEpoch 248/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0482 - accuracy: 0.9826 - val_loss: 0.0198 - val_accuracy: 0.9966\nEpoch 249/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.9830 - val_loss: 0.0197 - val_accuracy: 0.9967\nEpoch 250/250\n38/38 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.9830 - val_loss: 0.0199 - val_accuracy: 0.9966\n"
    }
   ],
   "source": [
    "model_history = model.fit(x_train, y_train, batch_size=param_list[\"BATCH_SIZE\"], validation_split=0.2, epochs=param_list[\"EPOCHS\"])\n",
    "model.save(\"version/{}/model.h5\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}