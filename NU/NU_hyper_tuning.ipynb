{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598127001482",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\"\n",
    "version_dir = \"version/\"\n",
    "static_dir = \"static/\"\n",
    "\n",
    "os.makedirs(log_dir)\n",
    "os.makedirs(version_dir)\n",
    "os.makedirs(static_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              t           t+1    delta  tokenized_data\n0  104289271808  104282980352 -6291456        -6291456\n1  104282980352  104282984448     4096            4096\n2  104282984448  104282988544     4096            4096\n3  104282988544  104282992640     4096            4096\n4  104282992640  104282996736     4096            4096",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t</th>\n      <th>t+1</th>\n      <th>delta</th>\n      <th>tokenized_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>104289271808</td>\n      <td>104282980352</td>\n      <td>-6291456</td>\n      <td>-6291456</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>104282980352</td>\n      <td>104282984448</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104282984448</td>\n      <td>104282988544</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>104282988544</td>\n      <td>104282992640</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104282992640</td>\n      <td>104282996736</td>\n      <td>4096</td>\n      <td>4096</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/{}_train_set.csv\".format(dataset_name))\n",
    "test_dataset = pd.read_csv(\"data/{}_test_set.csv\".format(dataset_name))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Parameters \n",
    "static_params = dict()\n",
    "static_params[\"PAST_HISTORY\"] = 16\n",
    "static_params[\"FUTURE_TARGET\"] = 8\n",
    "static_params[\"BUFFER_SIZE\"] = 200000\n",
    "static_params[\"ACTIVATION\"] = 'softmax'\n",
    "static_params[\"LOSS_FUNCTION\"] = 'categorical_crossentropy'\n",
    "static_params[\"VAL_SPLIT\"] = 0.2\n",
    "static_params[\"METRIC_ACCURACY\"] = 'accuracy'\n",
    "static_params[\"OPTIMIZER\"] = 'adam'\n",
    "\n",
    "# Hyper Parameters\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([256, 512, 1024]))\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([250, 500, 750, 1000]))\n",
    "HP_LAYER_1_UNITS = hp.HParam('layer_1_units', hp.Discrete([16, 32, 64, 128]))\n",
    "HP_LAYER_2_UNITS = hp.HParam('layer_2_units', hp.Discrete([16, 32, 64, 128]))\n",
    "HP_LAYER_1_DROPOUT = hp.HParam('layer_1_dropout', hp.RealInterval(0.1, 0.3))\n",
    "HP_LAYER_2_DROPOUT = hp.HParam('layer_2_dropout', hp.RealInterval(0.1, 0.3))\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_BATCH_SIZE, HP_EPOCHS, HP_LAYER_1_UNITS, HP_LAYER_2_UNITS, HP_LAYER_1_DROPOUT, HP_LAYER_2_DROPOUT],\n",
    "    metrics=[hp.Metric(static_params[\"METRIC_ACCURACY\"], display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "with open(\"static/static_params.json\", \"w\") as j :\n",
    "  json.dump(static_params, j, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size, n_features):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, n_features)))\n",
    "        labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<1x5 sparse matrix of type '<class 'numpy.float32'>'\n \twith 1 stored elements in Compressed Sparse Row format>,\n <1x5 sparse matrix of type '<class 'numpy.float32'>'\n \twith 1 stored elements in Compressed Sparse Row format>,\n [array([-6291456,       -1,        0,     4096,  2097152], dtype=int64)])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "encoder = OneHotEncoder(dtype=np.float32)\n",
    "joblib.dump(encoder, \"{}/encoder.pkl\".format(static_dir))\n",
    "\n",
    "encoded_data = encoder.fit_transform(dataset[\"tokenized_data\"].values.reshape(-1, 1))\n",
    "encoded_test_data = encoder.transform(test_dataset[\"tokenized_data\"].values.reshape(-1, 1))\n",
    "encoded_data[0], encoded_test_data[0], encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((47165, 16, 5), (47165, 8, 5), (10478, 16, 5), (10478, 8, 5))"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "x_train, y_train = generate_timeseries(encoded_data.toarray(), 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"], len(encoder.categories_[0]))\n",
    "x_test, y_test = generate_timeseries(encoded_test_data.toarray(), 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"], len(encoder.categories_[0]))\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_callback(log_dir, hist_freq=1):\n",
    "    return keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=hist_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams, timestamp):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(hparams[HP_LAYER_1_UNITS])),\n",
    "        keras.layers.Dropout(hparams[HP_LAYER_1_DROPOUT]),\n",
    "        keras.layers.RepeatVector(static_params[\"FUTURE_TARGET\"]),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(hparams[HP_LAYER_2_UNITS], return_sequences=True)),\n",
    "        keras.layers.Dropout(hparams[HP_LAYER_2_DROPOUT]),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(len(encoder.categories_[0]), activation=static_params[\"ACTIVATION\"]))\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=static_params[\"OPTIMIZER\"],\n",
    "        loss=static_params[\"LOSS_FUNCTION\"],\n",
    "        metrics=static_params[\"METRIC_ACCURACY\"]\n",
    "    )\n",
    "    history = model.fit(x_train, y_train, batch_size=hparams[HP_BATCH_SIZE], validation_split=static_params[\"VAL_SPLIT\"], epochs=hparams[HP_EPOCHS], callbacks=[tensorboard_callback(log_dir + timestamp)])\n",
    "    loss, accuracy = model.evaluate(x_test, y_test)\n",
    "    return history, accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, timestamp, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        history, accuracy, loss = create_model(hparams, timestamp)\n",
    "        tf.summary.scalar(static_params[\"METRIC_ACCURACY\"], accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3456\n"
    }
   ],
   "source": [
    "for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "    for epochs in HP_EPOCHS.domain.values:\n",
    "        for layer_1_units in HP_LAYER_1_UNITS.domain.values:\n",
    "            for layer_1_dropout in tf.linspace(HP_LAYER_1_DROPOUT.domain.min_value,HP_LAYER_1_DROPOUT.domain.max_value,3):\n",
    "                for layer_2_units in HP_LAYER_2_UNITS.domain.values:\n",
    "                    for layer_2_dropout in tf.linspace(HP_LAYER_2_DROPOUT.domain.min_value,HP_LAYER_2_DROPOUT.domain.max_value,3):\n",
    "                        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                            hparams = {\n",
    "                                HP_LAYER_1_UNITS: layer_1_units,\n",
    "                                HP_LAYER_1_DROPOUT: layer_1_dropout,\n",
    "                                HP_LAYER_2_UNITS: layer_2_units,\n",
    "                                HP_LAYER_2_DROPOUT: layer_2_dropout,\n",
    "                                HP_BATCH_SIZE: batch_size,\n",
    "                                HP_EPOCHS: epochs\n",
    "                            }\n",
    "\n",
    "                            timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                            run_name = \"Session : {}\".format(timestamp)\n",
    "                            print('--- Starting trial: {}'.format(run_name))\n",
    "                            print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "                            run('logs/hparam_tuning/', timestamp, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}