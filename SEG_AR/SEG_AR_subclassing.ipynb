{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600324721341",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TimeDistributed, \n",
    "    Dense, \n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Bidirectional, \n",
    "    LSTM, \n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"ex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = {\n",
    "    'PAST_HISTORY': 16,\n",
    "    'FUTURE_TARGET': 8,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'BUFFER_SIZE': 200000,\n",
    "    'EPOCHS': 500,\n",
    "    'VOCAB_SIZE': 16293\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"HP_LSTM_1_UNITS\" : 128,\n",
    "    \"HP_LSTM_1_DROPOUT\" : 0.0,\n",
    "    \"HP_LEARNING_RATE\" : 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/SEG_train_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_train, y_train = generate_timeseries(train_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().batch(static_params[\"BATCH_SIZE\"]).shuffle(static_params[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/SEG_val_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_val, y_val = generate_timeseries(val_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.cache().batch(static_params[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGAutoRegressive(keras.Model):\n",
    "    def __init__(self, units, dropout, output_steps, output_size):\n",
    "        super().__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = keras.layers.LSTMCell(units, dropout=dropout)\n",
    "\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = Dense(output_size, activation=\"softmax\")\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def warmup(self, inputs):\n",
    "        onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(onehot_inputs)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "\n",
    "        return prediction, state\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        #predictions = []\n",
    "        predictions = tf.TensorArray(tf.float32, size=self.output_steps, clear_after_read=False)\n",
    "        # Initialize the lstm state\n",
    "        prediction, state = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction\n",
    "        #predictions.append(prediction)\n",
    "        predictions = predictions.write(0, prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps\n",
    "        for i in tf.range(1, self.output_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "            \n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "\n",
    "            # Add the prediction to the output\n",
    "            #predictions.append(prediction)\n",
    "            predictions = predictions.write(i, prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        #predictions = tf.stack(predictions)\n",
    "        predictions = predictions.stack()\n",
    "\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEGAutoRegressive(\n",
    "    units=hparams[\"HP_LSTM_1_UNITS\"], dropout=hparams[\"HP_LSTM_1_DROPOUT\"], \n",
    "    output_steps=static_params[\"FUTURE_TARGET\"], output_size=static_params[\"VOCAB_SIZE\"])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(hparams[\"HP_LEARNING_RATE\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tensor(\"lstm_cell_1/mul_2:0\", shape=(None, 128), dtype=float32)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[6.1445746e-05, 6.1353123e-05, 6.1505591e-05, ...,\n         6.1475395e-05, 6.1428320e-05, 6.1368424e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        ...,\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1445746e-05, 6.1353123e-05, 6.1505591e-05, ...,\n         6.1475395e-05, 6.1428320e-05, 6.1368424e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        ...,\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.predict(x_train[:2].reshape(2, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n         6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n        [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n         6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n        [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n         6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n        ...,\n        [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n         6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n        [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n         6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n         6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n        [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n         6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n        [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n         6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n        ...,\n        [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n         6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n        [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n         6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 281
    }
   ],
   "source": [
    "model.predict(x_train[:2].reshape(2, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[6.1394290e-05, 6.1312676e-05, 6.1343890e-05, ...,\n         6.1373983e-05, 6.1312516e-05, 6.1396429e-05],\n        [6.1401341e-05, 6.1324237e-05, 6.1342740e-05, ...,\n         6.1372775e-05, 6.1332474e-05, 6.1393475e-05],\n        [6.1404193e-05, 6.1334067e-05, 6.1344050e-05, ...,\n         6.1372601e-05, 6.1346393e-05, 6.1390361e-05],\n        ...,\n        [6.1400817e-05, 6.1354905e-05, 6.1353589e-05, ...,\n         6.1373510e-05, 6.1367158e-05, 6.1382583e-05],\n        [6.1398219e-05, 6.1359540e-05, 6.1357016e-05, ...,\n         6.1373816e-05, 6.1370185e-05, 6.1380808e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1394981e-05, 6.1348357e-05, 6.1331943e-05, ...,\n         6.1387371e-05, 6.1324776e-05, 6.1382940e-05],\n        [6.1401130e-05, 6.1351318e-05, 6.1334358e-05, ...,\n         6.1381717e-05, 6.1344195e-05, 6.1382299e-05],\n        [6.1403560e-05, 6.1354571e-05, 6.1338447e-05, ...,\n         6.1378858e-05, 6.1356877e-05, 6.1381259e-05],\n        ...,\n        [6.1400227e-05, 6.1363506e-05, 6.1352795e-05, ...,\n         6.1376493e-05, 6.1373350e-05, 6.1378327e-05],\n        [6.1397775e-05, 6.1365834e-05, 6.1356972e-05, ...,\n         6.1376355e-05, 6.1375191e-05, 6.1377687e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1408762e-05, 6.1347368e-05, 6.1339553e-05, ...,\n         6.1381506e-05, 6.1314597e-05, 6.1377395e-05],\n        [6.1412633e-05, 6.1348997e-05, 6.1339233e-05, ...,\n         6.1377228e-05, 6.1337923e-05, 6.1381288e-05],\n        [6.1413353e-05, 6.1351908e-05, 6.1341401e-05, ...,\n         6.1375285e-05, 6.1353421e-05, 6.1382401e-05],\n        ...,\n        [6.1406572e-05, 6.1361643e-05, 6.1353014e-05, ...,\n         6.1374340e-05, 6.1373990e-05, 6.1380080e-05],\n        [6.1403261e-05, 6.1364357e-05, 6.1356877e-05, ...,\n         6.1374478e-05, 6.1376333e-05, 6.1379047e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       ...,\n\n       [[6.1387917e-05, 6.1347600e-05, 6.1365055e-05, ...,\n         6.1383122e-05, 6.1360610e-05, 6.1396437e-05],\n        [6.1383820e-05, 6.1353749e-05, 6.1366758e-05, ...,\n         6.1380109e-05, 6.1361941e-05, 6.1392740e-05],\n        [6.1380808e-05, 6.1358922e-05, 6.1368170e-05, ...,\n         6.1378094e-05, 6.1362902e-05, 6.1389685e-05],\n        ...,\n        [6.1375984e-05, 6.1369181e-05, 6.1371116e-05, ...,\n         6.1375591e-05, 6.1365470e-05, 6.1383333e-05],\n        [6.1375300e-05, 6.1371196e-05, 6.1371800e-05, ...,\n         6.1375409e-05, 6.1366401e-05, 6.1381957e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1382074e-05, 6.1353174e-05, 6.1392479e-05, ...,\n         6.1396939e-05, 6.1387364e-05, 6.1380277e-05],\n        [6.1380189e-05, 6.1357838e-05, 6.1386556e-05, ...,\n         6.1388666e-05, 6.1382692e-05, 6.1385377e-05],\n        [6.1379062e-05, 6.1361767e-05, 6.1381943e-05, ...,\n         6.1383413e-05, 6.1379527e-05, 6.1387684e-05],\n        ...,\n        [6.1377803e-05, 6.1369865e-05, 6.1374441e-05, ...,\n         6.1376872e-05, 6.1375336e-05, 6.1386789e-05],\n        [6.1377687e-05, 6.1371567e-05, 6.1373408e-05, ...,\n         6.1376188e-05, 6.1374914e-05, 6.1385545e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1369676e-05, 6.1341409e-05, 6.1407809e-05, ...,\n         6.1366700e-05, 6.1415405e-05, 6.1407467e-05],\n        [6.1371495e-05, 6.1353014e-05, 6.1397353e-05, ...,\n         6.1368752e-05, 6.1402374e-05, 6.1406543e-05],\n        [6.1372768e-05, 6.1360995e-05, 6.1389379e-05, ...,\n         6.1370796e-05, 6.1393235e-05, 6.1404229e-05],\n        ...,\n        [6.1374783e-05, 6.1373088e-05, 6.1376413e-05, ...,\n         6.1375300e-05, 6.1379644e-05, 6.1394901e-05],\n        [6.1375147e-05, 6.1374936e-05, 6.1374514e-05, ...,\n         6.1376188e-05, 6.1377723e-05, 6.1391998e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "for x, y in train_data.take(1):\n",
    "    x_sample = x\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "....] - ETA: 1:40 - loss: 7.9382 - accuracy: 0.23 79/315 [======>.......................] - ETA: 1:39 - loss: 7.9271 - accuracy: 0.23 80/315 [======>.......................] - ETA: 1:40 - loss: 7.8926 - accuracy: 0.24 81/315 [======>.......................] - ETA: 1:40 - loss: 7.9021 - accuracy: 0.23 82/315 [======>.......................] - ETA: 1:40 - loss: 7.9008 - accuracy: 0.23 83/315 [======>.......................] - ETA: 1:40 - loss: 7.8542 - accuracy: 0.23 84/315 [=======>......................] - ETA: 1:40 - loss: 7.8818 - accuracy: 0.23 85/315 [=======>......................] - ETA: 1:40 - loss: 7.9179 - accuracy: 0.23 86/315 [=======>......................] - ETA: 1:40 - loss: 7.8985 - accuracy: 0.23 87/315 [=======>......................] - ETA: 1:40 - loss: 7.8998 - accuracy: 0.23 88/315 [=======>......................] - ETA: 1:39 - loss: 7.8928 - accuracy: 0.23 89/315 [=======>......................] - ETA: 1:39 - loss: 7.8784 - accuracy: 0.23 90/315 [=======>......................] - ETA: 1:38 - loss: 7.8749 - accuracy: 0.23 91/315 [=======>......................] - ETA: 1:37 - loss: 7.8258 - accuracy: 0.24 92/315 [=======>......................] - ETA: 1:37 - loss: 7.8550 - accuracy: 0.23 93/315 [=======>......................] - ETA: 1:36 - loss: 7.8663 - accuracy: 0.23 94/315 [=======>......................] - ETA: 1:35 - loss: 7.8825 - accuracy: 0.23 95/315 [========>.....................] - ETA: 1:35 - loss: 7.8872 - accuracy: 0.23 96/315 [========>.....................] - ETA: 1:35 - loss: 7.8919 - accuracy: 0.23 97/315 [========>.....................] - ETA: 1:35 - loss: 7.8441 - accuracy: 0.23 98/315 [========>.....................] - ETA: 1:34 - loss: 7.8452 - accuracy: 0.23 99/315 [========>.....................] - ETA: 1:34 - loss: 7.8568 - accuracy: 0.23100/315 [========>.....................] - ETA: 1:34 - loss: 7.8511 - accuracy: 0.23101/315 [========>.....................] - ETA: 1:34 - loss: 7.8421 - accuracy: 0.23102/315 [========>.....................] - ETA: 1:33 - loss: 7.8352 - accuracy: 0.23103/315 [========>.....................] - ETA: 1:32 - loss: 7.8429 - accuracy: 0.23104/315 [========>.....................] - ETA: 1:32 - loss: 7.8663 - accuracy: 0.22105/315 [=========>....................] - ETA: 1:32 - loss: 7.8509 - accuracy: 0.23106/315 [=========>....................] - ETA: 1:31 - loss: 7.8350 - accuracy: 0.23107/315 [=========>....................] - ETA: 1:30 - loss: 7.8039 - accuracy: 0.23108/315 [=========>....................] - ETA: 1:29 - loss: 7.8221 - accuracy: 0.23109/315 [=========>....................] - ETA: 1:28 - loss: 7.8153 - accuracy: 0.23110/315 [=========>....................] - ETA: 1:28 - loss: 7.7727 - accuracy: 0.23111/315 [=========>....................] - ETA: 1:27 - loss: 7.7284 - accuracy: 0.22112/315 [=========>....................] - ETA: 1:26 - loss: 7.7122 - accuracy: 0.23113/315 [=========>....................] - ETA: 1:25 - loss: 7.7260 - accuracy: 0.23114/315 [=========>....................] - ETA: 1:26 - loss: 7.7416 - accuracy: 0.23115/315 [=========>....................] - ETA: 1:25 - loss: 7.7411 - accuracy: 0.23116/315 [==========>...................] - ETA: 1:24 - loss: 7.7267 - accuracy: 0.23117/315 [==========>...................] - ETA: 1:23 - loss: 7.7085 - accuracy: 0.23118/315 [==========>...................] - ETA: 1:23 - loss: 7.6990 - accuracy: 0.23119/315 [==========>...................] - ETA: 1:23 - loss: 7.6565 - accuracy: 0.22120/315 [==========>...................] - ETA: 1:22 - loss: 7.6352 - accuracy: 0.23121/315 [==========>...................] - ETA: 1:22 - loss: 7.6000 - accuracy: 0.23122/315 [==========>...................] - ETA: 1:21 - loss: 7.5757 - accuracy: 0.23123/315 [==========>...................] - ETA: 1:21 - loss: 7.5736 - accuracy: 0.23124/315 [==========>...................] - ETA: 1:21 - loss: 7.5303 - accuracy: 0.23125/315 [==========>...................] - ETA: 1:20 - loss: 7.5305 - accuracy: 0.23126/315 [===========>..................] - ETA: 1:19 - loss: 7.5300 - accuracy: 0.23127/315 [===========>..................] - ETA: 1:20 - loss: 7.5340 - accuracy: 0.23128/315 [===========>..................] - ETA: 1:20 - loss: 7.4925 - accuracy: 0.23129/315 [===========>..................] - ETA: 1:19 - loss: 7.4904 - accuracy: 0.23130/315 [===========>..................] - ETA: 1:19 - loss: 7.4712 - accuracy: 0.23131/315 [===========>..................] - ETA: 1:18 - loss: 7.4947 - accuracy: 0.23132/315 [===========>..................] - ETA: 1:17 - loss: 7.4897 - accuracy: 0.23133/315 [===========>..................] - ETA: 1:17 - loss: 7.4611 - accuracy: 0.23134/315 [===========>..................] - ETA: 1:17 - loss: 7.4516 - accuracy: 0.23135/315 [===========>..................] - ETA: 1:17 - loss: 7.4611 - accuracy: 0.23136/315 [===========>..................] - ETA: 1:16 - loss: 7.4873 - accuracy: 0.23137/315 [============>.................] - ETA: 1:16 - loss: 7.4827 - accuracy: 0.23138/315 [============>.................] - ETA: 1:16 - loss: 7.4968 - accuracy: 0.23139/315 [============>.................] - ETA: 1:15 - loss: 7.4864 - accuracy: 0.22140/315 [============>.................] - ETA: 1:14 - loss: 7.4957 - accuracy: 0.22141/315 [============>.................] - ETA: 1:14 - loss: 7.4753 - accuracy: 0.23142/315 [============>.................] - ETA: 1:14 - loss: 7.4705 - accuracy: 0.23143/315 [============>.................] - ETA: 1:13 - loss: 7.4669 - accuracy: 0.23144/315 [============>.................] - ETA: 1:13 - loss: 7.4447 - accuracy: 0.23145/315 [============>.................] - ETA: 1:12 - loss: 7.4501 - accuracy: 0.23146/315 [============>.................] - ETA: 1:11 - loss: 7.4458 - accuracy: 0.23147/315 [=============>................] - ETA: 1:11 - loss: 7.4466 - accuracy: 0.23148/315 [=============>................] - ETA: 1:10 - loss: 7.4211 - accuracy: 0.23149/315 [=============>................] - ETA: 1:10 - loss: 7.4245 - accuracy: 0.23150/315 [=============>................] - ETA: 1:09 - loss: 7.3996 - accuracy: 0.23151/315 [=============>................] - ETA: 1:08 - loss: 7.4186 - accuracy: 0.23152/315 [=============>................] - ETA: 1:08 - loss: 7.4258 - accuracy: 0.23153/315 [=============>................] - ETA: 1:08 - loss: 7.4232 - accuracy: 0.23154/315 [=============>................] - ETA: 1:07 - loss: 7.4356 - accuracy: 0.23155/315 [=============>................] - ETA: 1:07 - loss: 7.4504 - accuracy: 0.23156/315 [=============>................] - ETA: 1:07 - loss: 7.4543 - accuracy: 0.23157/315 [=============>................] - ETA: 1:06 - loss: 7.4287 - accuracy: 0.23158/315 [==============>...............] - ETA: 1:06 - loss: 7.4359 - accuracy: 0.23159/315 [==============>...............] - ETA: 1:05 - loss: 7.4479 - accuracy: 0.23160/315 [==============>...............] - ETA: 1:05 - loss: 7.4619 - accuracy: 0.23161/315 [==============>...............] - ETA: 1:05 - loss: 7.4545 - accuracy: 0.23162/315 [==============>...............] - ETA: 1:04 - loss: 7.4390 - accuracy: 0.23163/315 [==============>...............] - ETA: 1:04 - loss: 7.4473 - accuracy: 0.23164/315 [==============>...............] - ETA: 1:03 - loss: 7.4630 - accuracy: 0.23165/315 [==============>...............] - ETA: 1:03 - loss: 7.4516 - accuracy: 0.23166/315 [==============>...............] - ETA: 1:02 - loss: 7.4273 - accuracy: 0.23167/315 [==============>...............] - ETA: 1:01 - loss: 7.4390 - accuracy: 0.23168/315 [===============>..............] - ETA: 1:01 - loss: 7.4215 - accuracy: 0.23169/315 [===============>..............] - ETA: 1:00 - loss: 7.4133 - accuracy: 0.23170/315 [===============>..............] - ETA: 1:00 - loss: 7.4238 - accuracy: 0.23171/315 [===============>..............] - ETA: 59s - loss: 7.4178 - accuracy: 0.234172/315 [===============>..............] - ETA: 59s - loss: 7.4150 - accuracy: 0.235173/315 [===============>..............] - ETA: 58s - loss: 7.3964 - accuracy: 0.234174/315 [===============>..............] - ETA: 58s - loss: 7.3988 - accuracy: 0.234175/315 [===============>..............] - ETA: 58s - loss: 7.3936 - accuracy: 0.235176/315 [===============>..............] - ETA: 57s - loss: 7.4043 - accuracy: 0.235177/315 [===============>..............] - ETA: 56s - loss: 7.4032 - accuracy: 0.234178/315 [===============>..............] - ETA: 56s - loss: 7.3927 - accuracy: 0.236179/315 [================>.............] - ETA: 56s - loss: 7.4049 - accuracy: 0.234180/315 [================>.............] - ETA: 55s - loss: 7.4161 - accuracy: 0.234181/315 [================>.............] - ETA: 55s - loss: 7.3979 - accuracy: 0.233182/315 [================>.............] - ETA: 55s - loss: 7.4086 - accuracy: 0.232183/315 [================>.............] - ETA: 54s - loss: 7.3896 - accuracy: 0.231184/315 [================>.............] - ETA: 54s - loss: 7.3692 - accuracy: 0.234185/315 [================>.............] - ETA: 53s - loss: 7.3545 - accuracy: 0.233186/315 [================>.............] - ETA: 53s - loss: 7.3428 - accuracy: 0.235187/315 [================>.............] - ETA: 52s - loss: 7.3541 - accuracy: 0.234188/315 [================>.............] - ETA: 52s - loss: 7.3611 - accuracy: 0.234189/315 [=================>............] - ETA: 51s - loss: 7.3586 - accuracy: 0.234190/315 [=================>............] - ETA: 51s - loss: 7.3665 - accuracy: 0.234191/315 [=================>............] - ETA: 50s - loss: 7.3837 - accuracy: 0.233192/315 [=================>............] - ETA: 50s - loss: 7.3819 - accuracy: 0.234193/315 [=================>............] - ETA: 49s - loss: 7.3897 - accuracy: 0.233194/315 [=================>............] - ETA: 49s - loss: 7.3994 - accuracy: 0.233195/315 [=================>............] - ETA: 49s - loss: 7.4039 - accuracy: 0.233196/315 [=================>............] - ETA: 48s - loss: 7.4056 - accuracy: 0.233197/315 [=================>............] - ETA: 48s - loss: 7.3853 - accuracy: 0.232198/315 [=================>............] - ETA: 48s - loss: 7.3704 - accuracy: 0.234199/315 [=================>............] - ETA: 48s - loss: 7.3597 - accuracy: 0.236200/315 [==================>...........] - ETA: 47s - loss: 7.3461 - accuracy: 0.239201/315 [==================>...........] - ETA: 47s - loss: 7.3586 - accuracy: 0.238202/315 [==================>...........] - ETA: 46s - loss: 7.3558 - accuracy: 0.239203/315 [==================>...........] - ETA: 46s - loss: 7.3767 - accuracy: 0.238204/315 [==================>...........] - ETA: 46s - loss: 7.3576 - accuracy: 0.237205/315 [==================>...........] - ETA: 45s - loss: 7.3365 - accuracy: 0.236206/315 [==================>...........] - ETA: 45s - loss: 7.3396 - accuracy: 0.236207/315 [==================>...........] - ETA: 45s - loss: 7.3405 - accuracy: 0.236208/315 [==================>...........] - ETA: 44s - loss: 7.3584 - accuracy: 0.235209/315 [==================>...........] - ETA: 44s - loss: 7.3606 - accuracy: 0.235210/315 [===================>..........] - ETA: 43s - loss: 7.3611 - accuracy: 0.236211/315 [===================>..........] - ETA: 43s - loss: 7.3666 - accuracy: 0.235212/315 [===================>..........] - ETA: 42s - loss: 7.3486 - accuracy: 0.237213/315 [===================>..........] - ETA: 42s - loss: 7.3541 - accuracy: 0.237214/315 [===================>..........] - ETA: 41s - loss: 7.3645 - accuracy: 0.236215/315 [===================>..........] - ETA: 41s - loss: 7.3510 - accuracy: 0.235216/315 [===================>..........] - ETA: 40s - loss: 7.3615 - accuracy: 0.235217/315 [===================>..........] - ETA: 40s - loss: 7.3662 - accuracy: 0.235218/315 [===================>..........] - ETA: 39s - loss: 7.3535 - accuracy: 0.238219/315 [===================>..........] - ETA: 39s - loss: 7.3668 - accuracy: 0.237220/315 [===================>..........] - ETA: 38s - loss: 7.3480 - accuracy: 0.235221/315 [====================>.........] - ETA: 38s - loss: 7.3538 - accuracy: 0.235222/315 [====================>.........] - ETA: 38s - loss: 7.3574 - accuracy: 0.235223/315 [====================>.........] - ETA: 37s - loss: 7.3463 - accuracy: 0.235224/315 [====================>.........] - ETA: 37s - loss: 7.3272 - accuracy: 0.238225/315 [====================>.........] - ETA: 36s - loss: 7.3193 - accuracy: 0.239226/315 [====================>.........] - ETA: 36s - loss: 7.3217 - accuracy: 0.239227/315 [====================>.........] - ETA: 35s - loss: 7.3266 - accuracy: 0.239228/315 [====================>.........] - ETA: 35s - loss: 7.3142 - accuracy: 0.241229/315 [====================>.........] - ETA: 35s - loss: 7.3126 - accuracy: 0.242230/315 [====================>.........] - ETA: 34s - loss: 7.3179 - accuracy: 0.241231/315 [=====================>........] - ETA: 34s - loss: 7.3055 - accuracy: 0.243232/315 [=====================>........] - ETA: 34s - loss: 7.3113 - accuracy: 0.243233/315 [=====================>........] - ETA: 33s - loss: 7.3198 - accuracy: 0.242234/315 [=====================>........] - ETA: 33s - loss: 7.3216 - accuracy: 0.242235/315 [=====================>........] - ETA: 33s - loss: 7.3294 - accuracy: 0.242236/315 [=====================>........] - ETA: 32s - loss: 7.3383 - accuracy: 0.241237/315 [=====================>........] - ETA: 32s - loss: 7.3307 - accuracy: 0.243238/315 [=====================>........] - ETA: 31s - loss: 7.3187 - accuracy: 0.242239/315 [=====================>........] - ETA: 31s - loss: 7.3216 - accuracy: 0.242240/315 [=====================>........] - ETA: 30s - loss: 7.3198 - accuracy: 0.243241/315 [=====================>........] - ETA: 30s - loss: 7.3290 - accuracy: 0.242242/315 [======================>.......] - ETA: 30s - loss: 7.3341 - accuracy: 0.242243/315 [======================>.......] - ETA: 29s - loss: 7.3263 - accuracy: 0.243244/315 [======================>.......] - ETA: 29s - loss: 7.3378 - accuracy: 0.243245/315 [======================>.......] - ETA: 28s - loss: 7.3223 - accuracy: 0.242246/315 [======================>.......] - ETA: 28s - loss: 7.3047 - accuracy: 0.244247/315 [======================>.......] - ETA: 27s - loss: 7.2991 - accuracy: 0.245248/315 [======================>.......] - ETA: 27s - loss: 7.2994 - accuracy: 0.245249/315 [======================>.......] - ETA: 27s - loss: 7.2995 - accuracy: 0.246250/315 [======================>.......] - ETA: 26s - loss: 7.2983 - accuracy: 0.246251/315 [======================>.......] - ETA: 26s - loss: 7.3062 - accuracy: 0.246252/315 [=======================>......] - ETA: 25s - loss: 7.3164 - accuracy: 0.245253/315 [=======================>......] - ETA: 25s - loss: 7.3014 - accuracy: 0.244254/315 [=======================>......] - ETA: 24s - loss: 7.3070 - accuracy: 0.244255/315 [=======================>......] - ETA: 24s - loss: 7.3091 - accuracy: 0.244256/315 [=======================>......] - ETA: 24s - loss: 7.3146 - accuracy: 0.244257/315 [=======================>......] - ETA: 23s - loss: 7.3162 - accuracy: 0.244258/315 [=======================>......] - ETA: 23s - loss: 7.3037 - accuracy: 0.246259/315 [=======================>......] - ETA: 22s - loss: 7.2882 - accuracy: 0.245260/315 [=======================>......] - ETA: 22s - loss: 7.2719 - accuracy: 0.244261/315 [=======================>......] - ETA: 21s - loss: 7.2549 - accuracy: 0.243262/315 [=======================>......] - ETA: 21s - loss: 7.2542 - accuracy: 0.244263/315 [========================>.....] - ETA: 21s - loss: 7.2452 - accuracy: 0.245264/315 [========================>.....] - ETA: 20s - loss: 7.2444 - accuracy: 0.246265/315 [========================>.....] - ETA: 20s - loss: 7.2546 - accuracy: 0.245266/315 [========================>.....] - ETA: 19s - loss: 7.2373 - accuracy: 0.244267/315 [========================>.....] - ETA: 19s - loss: 7.2460 - accuracy: 0.243268/315 [========================>.....] - ETA: 19s - loss: 7.2478 - accuracy: 0.243269/315 [========================>.....] - ETA: 18s - loss: 7.2563 - accuracy: 0.242270/315 [========================>.....] - ETA: 18s - loss: 7.2495 - accuracy: 0.244271/315 [========================>.....] - ETA: 17s - loss: 7.2420 - accuracy: 0.245272/315 [========================>.....] - ETA: 17s - loss: 7.2378 - accuracy: 0.246273/315 [=========================>....] - ETA: 16s - loss: 7.2424 - accuracy: 0.246274/315 [=========================>....] - ETA: 16s - loss: 7.2354 - accuracy: 0.247275/315 [=========================>....] - ETA: 16s - loss: 7.2370 - accuracy: 0.247276/315 [=========================>....] - ETA: 15s - loss: 7.2441 - accuracy: 0.247277/315 [=========================>....] - ETA: 15s - loss: 7.2287 - accuracy: 0.246278/315 [=========================>....] - ETA: 14s - loss: 7.2347 - accuracy: 0.245279/315 [=========================>....] - ETA: 14s - loss: 7.2440 - accuracy: 0.244280/315 [=========================>....] - ETA: 14s - loss: 7.2511 - accuracy: 0.243281/315 [=========================>....] - ETA: 13s - loss: 7.2410 - accuracy: 0.245282/315 [=========================>....] - ETA: 13s - loss: 7.2299 - accuracy: 0.244283/315 [=========================>....] - ETA: 12s - loss: 7.2339 - accuracy: 0.244284/315 [==========================>...] - ETA: 12s - loss: 7.2272 - accuracy: 0.245285/315 [==========================>...] - ETA: 12s - loss: 7.2319 - accuracy: 0.245286/315 [==========================>...] - ETA: 11s - loss: 7.2366 - accuracy: 0.244287/315 [==========================>...] - ETA: 11s - loss: 7.2393 - accuracy: 0.245288/315 [==========================>...] - ETA: 10s - loss: 7.2443 - accuracy: 0.245289/315 [==========================>...] - ETA: 10s - loss: 7.2528 - accuracy: 0.244290/315 [==========================>...] - ETA: 10s - loss: 7.2577 - accuracy: 0.244291/315 [==========================>...] - ETA: 9s - loss: 7.2448 - accuracy: 0.24292/315 [==========================>...] - ETA: 9s - loss: 7.2514 - accuracy: 0.24293/315 [==========================>...] - ETA: 8s - loss: 7.2564 - accuracy: 0.24294/315 [===========================>..] - ETA: 8s - loss: 7.2604 - accuracy: 0.24295/315 [===========================>..] - ETA: 8s - loss: 7.2641 - accuracy: 0.24296/315 [===========================>..] - ETA: 7s - loss: 7.2520 - accuracy: 0.24297/315 [===========================>..] - ETA: 7s - loss: 7.2494 - accuracy: 0.24298/315 [===========================>..] - ETA: 6s - loss: 7.2396 - accuracy: 0.24299/315 [===========================>..] - ETA: 6s - loss: 7.2475 - accuracy: 0.24300/315 [===========================>..] - ETA: 6s - loss: 7.2338 - accuracy: 0.24301/315 [===========================>..] - ETA: 5s - loss: 7.2238 - accuracy: 0.24302/315 [===========================>..] - ETA: 5s - loss: 7.2315 - accuracy: 0.24303/315 [===========================>..] - ETA: 4s - loss: 7.2287 - accuracy: 0.24304/315 [===========================>..] - ETA: 4s - loss: 7.2333 - accuracy: 0.24305/315 [============================>.] - ETA: 4s - loss: 7.2364 - accuracy: 0.24306/315 [============================>.] - ETA: 3s - loss: 7.2406 - accuracy: 0.24307/315 [============================>.] - ETA: 3s - loss: 7.2296 - accuracy: 0.24308/315 [============================>.] - ETA: 2s - loss: 7.2275 - accuracy: 0.24309/315 [============================>.] - ETA: 2s - loss: 7.2302 - accuracy: 0.24310/315 [============================>.] - ETA: 2s - loss: 7.2318 - accuracy: 0.24311/315 [============================>.] - ETA: 1s - loss: 7.2326 - accuracy: 0.24312/315 [============================>.] - ETA: 1s - loss: 7.2346 - accuracy: 0.24313/315 [============================>.] - ETA: 0s - loss: 7.2442 - accuracy: 0.24314/315 [============================>.] - ETA: 0s - loss: 7.2464 - accuracy: 0.24315/315 [==============================] - ETA: 0s - loss: 7.2455 - accuracy: 0.24315/315 [==============================] - 128s 405ms/step - loss: 7.2455 - accuracy: 0.2410\n"
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"seg_auto_regressive_35\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nlstm_cell_34 (LSTMCell)      multiple                  8408064\n_________________________________________________________________\nrnn_34 (RNN)                 multiple                  8408064\n_________________________________________________________________\ndense_34 (Dense)             multiple                  2101797\n=================================================================\nTotal params: 10,509,861\nTrainable params: 10,509,861\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: ex/model_4\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"ex/model_4\", \n",
    "    signatures=model.call.get_concrete_function(tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32, name=\"input\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load(\"ex/model_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ItemsView(_SignatureMap({'serving_default': <tensorflow.python.saved_model.load._WrapperFunction object at 0x00000281F5C235C8>}))"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "new_model.signatures.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'output_0': <tf.Tensor: shape=(1, 8, 16293), dtype=float32, numpy=\n array([[[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n          6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n         [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n          6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n         [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n          6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n         ...,\n         [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n          6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n         [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n          6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)>}"
     },
     "metadata": {},
     "execution_count": 294
    }
   ],
   "source": [
    "inference = new_model.signatures[\"serving_default\"]\n",
    "inference(tf.constant(x_train[0].reshape(1, -1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: version/20200916-205849\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Looks like there is an object (perhaps variable or layer) that is shared between different layers/models. This may cause issues when restoring the variable values.Object: <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61BDFC8>\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61B91C8> and <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61BDFC8>).\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.00730717, -0.0198552 ,  0.06291562, ..., -0.00453906,\n        -0.02963837,  0.05556399],\n       [-0.08210181,  0.01879025,  0.02236442, ...,  0.07328226,\n         0.03787373,  0.01388676],\n       [-0.00642601, -0.05339912, -0.00547218, ...,  0.05781174,\n        -0.05629285,  0.02388919],\n       ...,\n       [-0.03962746, -0.01832095,  0.04701049, ...,  0.07243678,\n         0.01312423,  0.01554349],\n       [-0.01769733, -0.03605518, -0.0195809 , ..., -0.0357281 ,\n        -0.01636354,  0.05007046],\n       [ 0.03688146,  0.06794242, -0.06881212, ...,  0.04910905,\n        -0.02574294,  0.01410268]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00083591,  0.0063998 ,  0.00388604, ..., -0.00505481,\n        -0.0074309 , -0.0105841 ],\n       [ 0.00651316, -0.01728297, -0.00512317, ...,  0.00266844,\n         0.00772635, -0.0064606 ],\n       [ 0.01302376, -0.01636562,  0.00464472, ...,  0.00846572,\n         0.00637446, -0.01419988],\n       ...,\n       [ 0.00821935, -0.00052315,  0.01726941, ..., -0.00361796,\n        -0.01594081, -0.00454655],\n       [-0.00550627, -0.0138326 , -0.00877023, ..., -0.0005246 ,\n        -0.00196624, -0.00357318],\n       [-0.0163309 ,  0.0125559 ,  0.00641901, ..., -0.01629131,\n         0.00078054, -0.00967654]], dtype=float32)>]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e7f950a97013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m   raise IOError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;31m# TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[0;32m    602\u001b[0m       loader = loader_cls(object_graph_proto,\n\u001b[0;32m    603\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m                           export_dir)\n\u001b[0m\u001b[0;32m    605\u001b[0m       \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m       \u001b[0mload_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_existing_objects_matched\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m           (\"Some Python objects were not bound to checkpointed values, likely \"\n\u001b[0;32m    782\u001b[0m            \"due to changes in the Python program: %s\") %\n\u001b[1;32m--> 783\u001b[1;33m           (list(unused_python_objects),))\n\u001b[0m\u001b[0;32m    784\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.00730717, -0.0198552 ,  0.06291562, ..., -0.00453906,\n        -0.02963837,  0.05556399],\n       [-0.08210181,  0.01879025,  0.02236442, ...,  0.07328226,\n         0.03787373,  0.01388676],\n       [-0.00642601, -0.05339912, -0.00547218, ...,  0.05781174,\n        -0.05629285,  0.02388919],\n       ...,\n       [-0.03962746, -0.01832095,  0.04701049, ...,  0.07243678,\n         0.01312423,  0.01554349],\n       [-0.01769733, -0.03605518, -0.0195809 , ..., -0.0357281 ,\n        -0.01636354,  0.05007046],\n       [ 0.03688146,  0.06794242, -0.06881212, ...,  0.04910905,\n        -0.02574294,  0.01410268]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00083591,  0.0063998 ,  0.00388604, ..., -0.00505481,\n        -0.0074309 , -0.0105841 ],\n       [ 0.00651316, -0.01728297, -0.00512317, ...,  0.00266844,\n         0.00772635, -0.0064606 ],\n       [ 0.01302376, -0.01636562,  0.00464472, ...,  0.00846572,\n         0.00637446, -0.01419988],\n       ...,\n       [ 0.00821935, -0.00052315,  0.01726941, ..., -0.00361796,\n        -0.01594081, -0.00454655],\n       [-0.00550627, -0.0138326 , -0.00877023, ..., -0.0005246 ,\n        -0.00196624, -0.00357318],\n       [-0.0163309 ,  0.0125559 ,  0.00641901, ..., -0.01629131,\n         0.00078054, -0.00967654]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.load_model(version_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(version_dir + \"/weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c3b0db8dd40e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "model_1.load_weights(version_dir + \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}