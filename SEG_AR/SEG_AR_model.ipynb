{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600257090518",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TimeDistributed, \n",
    "    Dense, \n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Bidirectional, \n",
    "    LSTM, \n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'20200917-120410'"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/\" + timestamp\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(log_dir)\n",
    "os.makedirs(version_dir)\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = {\n",
    "    'PAST_HISTORY': 16,\n",
    "    'FUTURE_TARGET': 8,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'BUFFER_SIZE': 200000,\n",
    "    'EPOCHS': 500,\n",
    "    'VOCAB_SIZE': 16293\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"HP_LSTM_1_UNITS\" : 128,\n",
    "    \"HP_LSTM_1_DROPOUT\" : 0.0,\n",
    "    \"HP_LEARNING_RATE\" : 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/SEG_train_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_train, y_train = generate_timeseries(train_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().batch(static_params[\"BATCH_SIZE\"]).shuffle(static_params[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/SEG_val_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_val, y_val = generate_timeseries(val_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.cache().batch(static_params[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGAutoRegressive(keras.Model):\n",
    "    def __init__(self, units, dropout, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = keras.layers.LSTMCell(units, dropout=dropout)\n",
    "\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = Dense(static_params[\"VOCAB_SIZE\"], activation=\"softmax\")\n",
    "\n",
    "    def warmup(self, inputs):\n",
    "        onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(onehot_inputs)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "\n",
    "        return prediction, state\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        predictions = []\n",
    "        # Initialize the lstm state\n",
    "        prediction, state = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps\n",
    "        for _ in range(self.out_steps - 1):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "\n",
    "            # Add the prediction to the output\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        predictions = tf.stack(predictions)\n",
    "\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEGAutoRegressive(\n",
    "    units=hparams[\"HP_LSTM_1_UNITS\"], dropout=hparams[\"HP_LSTM_1_DROPOUT\"], \n",
    "    out_steps=static_params[\"FUTURE_TARGET\"])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(hparams[\"HP_LEARNING_RATE\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...............] - ETA: 19s - loss: 0.6860 - accuracy: 0.776 81/158 [==============>...............] - ETA: 18s - loss: 0.6870 - accuracy: 0.776 82/158 [==============>...............] - ETA: 18s - loss: 0.6881 - accuracy: 0.776 83/158 [==============>...............] - ETA: 18s - loss: 0.6916 - accuracy: 0.774 84/158 [==============>...............] - ETA: 18s - loss: 0.6993 - accuracy: 0.771 85/158 [===============>..............] - ETA: 17s - loss: 0.7024 - accuracy: 0.769 86/158 [===============>..............] - ETA: 17s - loss: 0.7040 - accuracy: 0.769 87/158 [===============>..............] - ETA: 17s - loss: 0.7036 - accuracy: 0.769 88/158 [===============>..............] - ETA: 17s - loss: 0.7044 - accuracy: 0.769 89/158 [===============>..............] - ETA: 16s - loss: 0.7035 - accuracy: 0.770 90/158 [================>.............] - ETA: 16s - loss: 0.7049 - accuracy: 0.769 91/158 [================>.............] - ETA: 16s - loss: 0.7042 - accuracy: 0.769 92/158 [================>.............] - ETA: 16s - loss: 0.7057 - accuracy: 0.769 93/158 [================>.............] - ETA: 15s - loss: 0.7053 - accuracy: 0.769 94/158 [================>.............] - ETA: 15s - loss: 0.7046 - accuracy: 0.768 95/158 [=================>............] - ETA: 15s - loss: 0.7044 - accuracy: 0.767 96/158 [=================>............] - ETA: 15s - loss: 0.6972 - accuracy: 0.769 97/158 [=================>............] - ETA: 14s - loss: 0.6979 - accuracy: 0.769 98/158 [=================>............] - ETA: 14s - loss: 0.6993 - accuracy: 0.769 99/158 [=================>............] - ETA: 14s - loss: 0.6993 - accuracy: 0.769100/158 [=================>............] - ETA: 14s - loss: 0.6991 - accuracy: 0.769101/158 [==================>...........] - ETA: 13s - loss: 0.7001 - accuracy: 0.769102/158 [==================>...........] - ETA: 13s - loss: 0.6995 - accuracy: 0.769103/158 [==================>...........] - ETA: 13s - loss: 0.6990 - accuracy: 0.770104/158 [==================>...........] - ETA: 13s - loss: 0.7005 - accuracy: 0.769105/158 [==================>...........] - ETA: 12s - loss: 0.7001 - accuracy: 0.769106/158 [===================>..........] - ETA: 12s - loss: 0.7025 - accuracy: 0.768107/158 [===================>..........] - ETA: 12s - loss: 0.7018 - accuracy: 0.769108/158 [===================>..........] - ETA: 12s - loss: 0.6992 - accuracy: 0.770109/158 [===================>..........] - ETA: 12s - loss: 0.6985 - accuracy: 0.770110/158 [===================>..........] - ETA: 11s - loss: 0.6995 - accuracy: 0.770111/158 [====================>.........] - ETA: 11s - loss: 0.6997 - accuracy: 0.769112/158 [====================>.........] - ETA: 11s - loss: 0.6999 - accuracy: 0.768113/158 [====================>.........] - ETA: 11s - loss: 0.6953 - accuracy: 0.769114/158 [====================>.........] - ETA: 10s - loss: 0.6952 - accuracy: 0.769115/158 [====================>.........] - ETA: 10s - loss: 0.6959 - accuracy: 0.769116/158 [=====================>........] - ETA: 10s - loss: 0.6968 - accuracy: 0.769117/158 [=====================>........] - ETA: 10s - loss: 0.6945 - accuracy: 0.769118/158 [=====================>........] - ETA: 9s - loss: 0.6956 - accuracy: 0.76119/158 [=====================>........] - ETA: 9s - loss: 0.6975 - accuracy: 0.76120/158 [=====================>........] - ETA: 9s - loss: 0.6956 - accuracy: 0.76121/158 [=====================>........] - ETA: 9s - loss: 0.6960 - accuracy: 0.76122/158 [======================>.......] - ETA: 8s - loss: 0.6973 - accuracy: 0.76123/158 [======================>.......] - ETA: 8s - loss: 0.6976 - accuracy: 0.76124/158 [======================>.......] - ETA: 8s - loss: 0.6990 - accuracy: 0.76125/158 [======================>.......] - ETA: 8s - loss: 0.6982 - accuracy: 0.76126/158 [======================>.......] - ETA: 7s - loss: 0.7001 - accuracy: 0.76127/158 [=======================>......] - ETA: 7s - loss: 0.6947 - accuracy: 0.77128/158 [=======================>......] - ETA: 7s - loss: 0.6894 - accuracy: 0.77129/158 [=======================>......] - ETA: 7s - loss: 0.6899 - accuracy: 0.77130/158 [=======================>......] - ETA: 6s - loss: 0.6847 - accuracy: 0.77131/158 [=======================>......] - ETA: 6s - loss: 0.6845 - accuracy: 0.77132/158 [========================>.....] - ETA: 6s - loss: 0.6855 - accuracy: 0.77133/158 [========================>.....] - ETA: 6s - loss: 0.6875 - accuracy: 0.77134/158 [========================>.....] - ETA: 5s - loss: 0.6831 - accuracy: 0.77135/158 [========================>.....] - ETA: 5s - loss: 0.6849 - accuracy: 0.77136/158 [========================>.....] - ETA: 5s - loss: 0.6869 - accuracy: 0.77137/158 [=========================>....] - ETA: 5s - loss: 0.6879 - accuracy: 0.77138/158 [=========================>....] - ETA: 4s - loss: 0.6872 - accuracy: 0.77139/158 [=========================>....] - ETA: 4s - loss: 0.6872 - accuracy: 0.77140/158 [=========================>....] - ETA: 4s - loss: 0.6876 - accuracy: 0.77141/158 [=========================>....] - ETA: 4s - loss: 0.6885 - accuracy: 0.77142/158 [=========================>....] - ETA: 3s - loss: 0.6885 - accuracy: 0.77143/158 [==========================>...] - ETA: 3s - loss: 0.6870 - accuracy: 0.77144/158 [==========================>...] - ETA: 3s - loss: 0.6900 - accuracy: 0.77145/158 [==========================>...] - ETA: 3s - loss: 0.6907 - accuracy: 0.77146/158 [==========================>...] - ETA: 2s - loss: 0.6878 - accuracy: 0.77147/158 [==========================>...] - ETA: 2s - loss: 0.6869 - accuracy: 0.77148/158 [===========================>..] - ETA: 2s - loss: 0.6875 - accuracy: 0.77149/158 [===========================>..] - ETA: 2s - loss: 0.6865 - accuracy: 0.77150/158 [===========================>..] - ETA: 1s - loss: 0.6862 - accuracy: 0.77151/158 [===========================>..] - ETA: 1s - loss: 0.6890 - accuracy: 0.77152/158 [===========================>..] - ETA: 1s - loss: 0.6917 - accuracy: 0.77153/158 [============================>.] - ETA: 1s - loss: 0.6929 - accuracy: 0.77154/158 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.77155/158 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.77156/158 [============================>.] - ETA: 0s - loss: 0.6915 - accuracy: 0.77157/158 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.77158/158 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.77158/158 [==============================] - 43s 274ms/step - loss: 0.6952 - accuracy: 0.7711 - val_loss: 1.7466 - val_accuracy: 0.5995\nEpoch 117/500\n  1/158 [..............................] - ETA: 0s - loss: 0.5754 - accuracy: 0.81  2/158 [..............................] - ETA: 19s - loss: 0.6811 - accuracy: 0.787  3/158 [..............................] - ETA: 25s - loss: 0.6323 - accuracy: 0.811  4/158 [..............................] - ETA: 28s - loss: 0.6597 - accuracy: 0.799  5/158 [..............................] - ETA: 30s - loss: 0.6450 - accuracy: 0.803  6/158 [>.............................] - ETA: 31s - loss: 0.6849 - accuracy: 0.779  7/158 [>.............................] - ETA: 32s - loss: 0.6983 - accuracy: 0.779  8/158 [>.............................] - ETA: 32s - loss: 0.7202 - accuracy: 0.769  9/158 [>.............................] - ETA: 32s - loss: 0.7087 - accuracy: 0.776 10/158 [>.............................] - ETA: 33s - loss: 0.7062 - accuracy: 0.777 11/158 [=>............................] - ETA: 33s - loss: 0.7249 - accuracy: 0.771 12/158 [=>............................] - ETA: 33s - loss: 0.7302 - accuracy: 0.766 13/158 [=>............................] - ETA: 33s - loss: 0.7290 - accuracy: 0.766 14/158 [=>............................] - ETA: 33s - loss: 0.7634 - accuracy: 0.750 15/158 [=>............................] - ETA: 33s - loss: 0.7137 - accuracy: 0.767 16/158 [==>...........................] - ETA: 33s - loss: 0.7115 - accuracy: 0.769 17/158 [==>...........................] - ETA: 32s - loss: 0.6972 - accuracy: 0.776 18/158 [==>...........................] - ETA: 32s - loss: 0.6589 - accuracy: 0.789 19/158 [==>...........................] - ETA: 32s - loss: 0.6251 - accuracy: 0.800 20/158 [==>...........................] - ETA: 32s - loss: 0.6312 - accuracy: 0.798 21/158 [==>...........................] - ETA: 32s - loss: 0.6566 - accuracy: 0.787 22/158 [===>..........................] - ETA: 32s - loss: 0.6615 - accuracy: 0.785 23/158 [===>..........................] - ETA: 32s - loss: 0.6632 - accuracy: 0.784 24/158 [===>..........................] - ETA: 31s - loss: 0.6602 - accuracy: 0.786 25/158 [===>..........................] - ETA: 31s - loss: 0.6644 - accuracy: 0.784 26/158 [===>..........................] - ETA: 31s - loss: 0.6584 - accuracy: 0.787 27/158 [====>.........................] - ETA: 31s - loss: 0.6491 - accuracy: 0.792 28/158 [====>.........................] - ETA: 31s - loss: 0.6531 - accuracy: 0.790 29/158 [====>.........................] - ETA: 30s - loss: 0.6519 - accuracy: 0.792 30/158 [====>.........................] - ETA: 30s - loss: 0.6519 - accuracy: 0.792 31/158 [====>.........................] - ETA: 30s - loss: 0.6570 - accuracy: 0.790 32/158 [=====>........................] - ETA: 30s - loss: 0.6617 - accuracy: 0.789 33/158 [=====>........................] - ETA: 30s - loss: 0.6661 - accuracy: 0.787 34/158 [=====>........................] - ETA: 29s - loss: 0.6724 - accuracy: 0.785 35/158 [=====>........................] - ETA: 29s - loss: 0.6807 - accuracy: 0.781 36/158 [=====>........................] - ETA: 29s - loss: 0.6622 - accuracy: 0.787 37/158 [======>.......................] - ETA: 29s - loss: 0.6491 - accuracy: 0.792 38/158 [======>.......................] - ETA: 29s - loss: 0.6554 - accuracy: 0.789 39/158 [======>.......................] - ETA: 28s - loss: 0.6624 - accuracy: 0.786 40/158 [======>.......................] - ETA: 28s - loss: 0.6638 - accuracy: 0.785 41/158 [======>.......................] - ETA: 28s - loss: 0.6631 - accuracy: 0.786 42/158 [======>.......................] - ETA: 28s - loss: 0.6650 - accuracy: 0.786 43/158 [=======>......................] - ETA: 27s - loss: 0.6690 - accuracy: 0.784 44/158 [=======>......................] - ETA: 27s - loss: 0.6748 - accuracy: 0.781 45/158 [=======>......................] - ETA: 27s - loss: 0.6768 - accuracy: 0.780 46/158 [=======>......................] - ETA: 27s - loss: 0.6862 - accuracy: 0.776 47/158 [=======>......................] - ETA: 27s - loss: 0.6816 - accuracy: 0.778 48/158 [========>.....................] - ETA: 26s - loss: 0.6812 - accuracy: 0.778 49/158 [========>.....................] - ETA: 26s - loss: 0.6817 - accuracy: 0.778 50/158 [========>.....................] - ETA: 26s - loss: 0.6893 - accuracy: 0.776 51/158 [========>.....................] - ETA: 26s - loss: 0.6873 - accuracy: 0.777 52/158 [========>.....................] - ETA: 25s - loss: 0.6812 - accuracy: 0.779 53/158 [=========>....................] - ETA: 25s - loss: 0.6848 - accuracy: 0.778 54/158 [=========>....................] - ETA: 25s - loss: 0.6870 - accuracy: 0.778 55/158 [=========>....................] - ETA: 25s - loss: 0.6871 - accuracy: 0.778 56/158 [=========>....................] - ETA: 24s - loss: 0.6839 - accuracy: 0.779 57/158 [=========>....................] - ETA: 24s - loss: 0.6851 - accuracy: 0.778 58/158 [==========>...................] - ETA: 24s - loss: 0.6864 - accuracy: 0.778 59/158 [==========>...................] - ETA: 24s - loss: 0.6877 - accuracy: 0.777 60/158 [==========>...................] - ETA: 23s - loss: 0.6838 - accuracy: 0.779 61/158 [==========>...................] - ETA: 23s - loss: 0.6814 - accuracy: 0.780 62/158 [==========>...................] - ETA: 23s - loss: 0.6812 - accuracy: 0.780 63/158 [==========>...................] - ETA: 23s - loss: 0.6807 - accuracy: 0.780 64/158 [===========>..................] - ETA: 23s - loss: 0.6825 - accuracy: 0.780 65/158 [===========>..................] - ETA: 22s - loss: 0.6850 - accuracy: 0.779 66/158 [===========>..................] - ETA: 22s - loss: 0.6858 - accuracy: 0.779 67/158 [===========>..................] - ETA: 22s - loss: 0.6793 - accuracy: 0.781 68/158 [===========>..................] - ETA: 22s - loss: 0.6832 - accuracy: 0.779 69/158 [============>.................] - ETA: 21s - loss: 0.6849 - accuracy: 0.779 70/158 [============>.................] - ETA: 21s - loss: 0.6812 - accuracy: 0.780 71/158 [============>.................] - ETA: 21s - loss: 0.6833 - accuracy: 0.779 72/158 [============>.................] - ETA: 21s - loss: 0.6827 - accuracy: 0.779 73/158 [============>.................] - ETA: 20s - loss: 0.6735 - accuracy: 0.782 74/158 [=============>................] - ETA: 20s - loss: 0.6751 - accuracy: 0.781 75/158 [=============>................] - ETA: 20s - loss: 0.6663 - accuracy: 0.784 76/158 [=============>................] - ETA: 20s - loss: 0.6695 - accuracy: 0.783 77/158 [=============>................] - ETA: 19s - loss: 0.6747 - accuracy: 0.780 78/158 [=============>................] - ETA: 19s - loss: 0.6745 - accuracy: 0.780 79/158 [==============>...............] - ETA: 19s - loss: 0.6743 - accuracy: 0.778 80/158 [==============>...............] - ETA: 19s - loss: 0.6733 - accuracy: 0.779 81/158 [==============>...............] - ETA: 18s - loss: 0.6742 - accuracy: 0.779 82/158 [==============>...............] - ETA: 18s - loss: 0.6740 - accuracy: 0.779 83/158 [==============>...............] - ETA: 18s - loss: 0.6788 - accuracy: 0.777 84/158 [==============>...............] - ETA: 18s - loss: 0.6787 - accuracy: 0.777 85/158 [===============>..............] - ETA: 17s - loss: 0.6821 - accuracy: 0.776 86/158 [===============>..............] - ETA: 17s - loss: 0.6833 - accuracy: 0.775 87/158 [===============>..............] - ETA: 17s - loss: 0.6849 - accuracy: 0.775 88/158 [===============>..............] - ETA: 17s - loss: 0.6844 - accuracy: 0.775 89/158 [===============>..............] - ETA: 16s - loss: 0.6854 - accuracy: 0.774 90/158 [================>.............] - ETA: 16s - loss: 0.6838 - accuracy: 0.775 91/158 [================>.............] - ETA: 16s - loss: 0.6766 - accuracy: 0.778 92/158 [================>.............] - ETA: 16s - loss: 0.6770 - accuracy: 0.778 93/158 [================>.............] - ETA: 15s - loss: 0.6769 - accuracy: 0.778 94/158 [================>.............] - ETA: 15s - loss: 0.6698 - accuracy: 0.781 95/158 [=================>............] - ETA: 15s - loss: 0.6718 - accuracy: 0.780 96/158 [=================>............] - ETA: 15s - loss: 0.6724 - accuracy: 0.780 97/158 [=================>............] - ETA: 15s - loss: 0.6736 - accuracy: 0.779 98/158 [=================>............] - ETA: 14s - loss: 0.6734 - accuracy: 0.779 99/158 [=================>............] - ETA: 14s - loss: 0.6733 - accuracy: 0.778100/158 [=================>............] - ETA: 14s - loss: 0.6746 - accuracy: 0.778101/158 [==================>...........] - ETA: 14s - loss: 0.6754 - accuracy: 0.778102/158 [==================>...........] - ETA: 13s - loss: 0.6763 - accuracy: 0.778103/158 [==================>...........] - ETA: 13s - loss: 0.6758 - accuracy: 0.778104/158 [==================>...........] - ETA: 13s - loss: 0.6770 - accuracy: 0.778105/158 [==================>...........] - ETA: 13s - loss: 0.6756 - accuracy: 0.779106/158 [===================>..........] - ETA: 12s - loss: 0.6768 - accuracy: 0.778107/158 [===================>..........] - ETA: 12s - loss: 0.6772 - accuracy: 0.777108/158 [===================>..........] - ETA: 12s - loss: 0.6800 - accuracy: 0.776109/158 [===================>..........] - ETA: 12s - loss: 0.6744 - accuracy: 0.777110/158 [===================>..........] - ETA: 11s - loss: 0.6767 - accuracy: 0.777111/158 [====================>.........] - ETA: 11s - loss: 0.6804 - accuracy: 0.775112/158 [====================>.........] - ETA: 11s - loss: 0.6815 - accuracy: 0.775113/158 [====================>.........] - ETA: 11s - loss: 0.6801 - accuracy: 0.775114/158 [====================>.........] - ETA: 10s - loss: 0.6816 - accuracy: 0.775115/158 [====================>.........] - ETA: 10s - loss: 0.6830 - accuracy: 0.774116/158 [=====================>........] - ETA: 10s - loss: 0.6860 - accuracy: 0.773117/158 [=====================>........] - ETA: 10s - loss: 0.6863 - accuracy: 0.773118/158 [=====================>........] - ETA: 9s - loss: 0.6857 - accuracy: 0.77119/158 [=====================>........] - ETA: 9s - loss: 0.6856 - accuracy: 0.77120/158 [=====================>........] - ETA: 9s - loss: 0.6820 - accuracy: 0.77121/158 [=====================>........] - ETA: 9s - loss: 0.6824 - accuracy: 0.77122/158 [======================>.......] - ETA: 8s - loss: 0.6829 - accuracy: 0.77123/158 [======================>.......] - ETA: 8s - loss: 0.6839 - accuracy: 0.77124/158 [======================>.......] - ETA: 8s - loss: 0.6843 - accuracy: 0.77125/158 [======================>.......] - ETA: 8s - loss: 0.6825 - accuracy: 0.77126/158 [======================>.......] - ETA: 7s - loss: 0.6859 - accuracy: 0.77127/158 [=======================>......] - ETA: 7s - loss: 0.6809 - accuracy: 0.77128/158 [=======================>......] - ETA: 7s - loss: 0.6815 - accuracy: 0.77129/158 [=======================>......] - ETA: 7s - loss: 0.6827 - accuracy: 0.77130/158 [=======================>......] - ETA: 6s - loss: 0.6782 - accuracy: 0.77131/158 [=======================>......] - ETA: 6s - loss: 0.6791 - accuracy: 0.77132/158 [========================>.....] - ETA: 6s - loss: 0.6790 - accuracy: 0.77133/158 [========================>.....] - ETA: 6s - loss: 0.6803 - accuracy: 0.77134/158 [========================>.....] - ETA: 5s - loss: 0.6804 - accuracy: 0.77135/158 [========================>.....] - ETA: 5s - loss: 0.6797 - accuracy: 0.77136/158 [========================>.....] - ETA: 5s - loss: 0.6794 - accuracy: 0.77137/158 [=========================>....] - ETA: 5s - loss: 0.6793 - accuracy: 0.77138/158 [=========================>....] - ETA: 4s - loss: 0.6803 - accuracy: 0.77139/158 [=========================>....] - ETA: 4s - loss: 0.6807 - accuracy: 0.77140/158 [=========================>....] - ETA: 4s - loss: 0.6805 - accuracy: 0.77141/158 [=========================>....] - ETA: 4s - loss: 0.6824 - accuracy: 0.77142/158 [=========================>....] - ETA: 3s - loss: 0.6821 - accuracy: 0.77143/158 [==========================>...] - ETA: 3s - loss: 0.6833 - accuracy: 0.77144/158 [==========================>...] - ETA: 3s - loss: 0.6844 - accuracy: 0.77145/158 [==========================>...] - ETA: 3s - loss: 0.6861 - accuracy: 0.77146/158 [==========================>...] - ETA: 2s - loss: 0.6849 - accuracy: 0.77147/158 [==========================>...] - ETA: 2s - loss: 0.6803 - accuracy: 0.77148/158 [===========================>..] - ETA: 2s - loss: 0.6805 - accuracy: 0.77149/158 [===========================>..] - ETA: 2s - loss: 0.6810 - accuracy: 0.77150/158 [===========================>..] - ETA: 1s - loss: 0.6823 - accuracy: 0.77151/158 [===========================>..] - ETA: 1s - loss: 0.6852 - accuracy: 0.77152/158 [===========================>..] - ETA: 1s - loss: 0.6848 - accuracy: 0.77153/158 [============================>.] - ETA: 1s - loss: 0.6850 - accuracy: 0.77154/158 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.77155/158 [============================>.] - ETA: 0s - loss: 0.6841 - accuracy: 0.77156/158 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.77157/158 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.77158/158 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.77158/158 [==============================] - 43s 274ms/step - loss: 0.6887 - accuracy: 0.7727 - val_loss: 1.7432 - val_accuracy: 0.6102\n"
    }
   ],
   "source": [
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp_api.hparams(hparams)\n",
    "    history = model.fit(train_data, validation_data=val_data, epochs=500, callbacks=[\n",
    "        keras.callbacks.EarlyStopping('val_accuracy', patience=5),\n",
    "        keras.callbacks.TensorBoard(log_dir)\n",
    "        ])\n",
    "    #tf.summary.scalar('accuracy', accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: version/20200917-120410\\assets\n"
    }
   ],
   "source": [
    "model.save(version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Looks like there is an object (perhaps variable or layer) that is shared between different layers/models. This may cause issues when restoring the variable values.Object: <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018C8ADACD08>\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018C8ADA9DC8> and <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018C8ADACD08>).\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00093962,  0.01685457, -0.00583172, ..., -0.00669129,\n        -0.00767045, -0.00359495],\n       [-0.00399957,  0.00933405, -0.00076441, ..., -0.01293601,\n         0.01461778, -0.01425246],\n       [ 0.00769919,  0.00736899, -0.01319111, ...,  0.00185939,\n        -0.00596508, -0.00581026],\n       ...,\n       [-0.0133435 , -0.00114346,  0.01317015, ...,  0.00837968,\n        -0.01000525,  0.00883356],\n       [ 0.00267314,  0.00964596, -0.01599753, ...,  0.00599556,\n         0.00175181,  0.01269117],\n       [-0.00441695, -0.01714835, -0.00555437, ..., -0.01201999,\n        -0.01785416, -0.01386872]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.05559206, -0.03301726, -0.01646268, ...,  0.02500414,\n         0.01045272, -0.01133694],\n       [-0.06149707, -0.04905558, -0.01345656, ...,  0.01717276,\n        -0.0592617 , -0.06165091],\n       [ 0.04440522,  0.00500455, -0.00488806, ..., -0.00153356,\n        -0.02387191,  0.01460617],\n       ...,\n       [ 0.01077382,  0.02391537,  0.04324631, ...,  0.02716968,\n        -0.04910111,  0.06719807],\n       [-0.00891744,  0.02139229,  0.01219342, ..., -0.03232003,\n        -0.00695406,  0.0069576 ],\n       [-0.03075593, -0.02856579,  0.01786736, ...,  0.00336823,\n         0.0068219 ,  0.00551834]], dtype=float32)>]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e7f950a97013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m   raise IOError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;31m# TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[0;32m    602\u001b[0m       loader = loader_cls(object_graph_proto,\n\u001b[0;32m    603\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m                           export_dir)\n\u001b[0m\u001b[0;32m    605\u001b[0m       \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m       \u001b[0mload_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_existing_objects_matched\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m           (\"Some Python objects were not bound to checkpointed values, likely \"\n\u001b[0;32m    782\u001b[0m            \"due to changes in the Python program: %s\") %\n\u001b[1;32m--> 783\u001b[1;33m           (list(unused_python_objects),))\n\u001b[0m\u001b[0;32m    784\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00093962,  0.01685457, -0.00583172, ..., -0.00669129,\n        -0.00767045, -0.00359495],\n       [-0.00399957,  0.00933405, -0.00076441, ..., -0.01293601,\n         0.01461778, -0.01425246],\n       [ 0.00769919,  0.00736899, -0.01319111, ...,  0.00185939,\n        -0.00596508, -0.00581026],\n       ...,\n       [-0.0133435 , -0.00114346,  0.01317015, ...,  0.00837968,\n        -0.01000525,  0.00883356],\n       [ 0.00267314,  0.00964596, -0.01599753, ...,  0.00599556,\n         0.00175181,  0.01269117],\n       [-0.00441695, -0.01714835, -0.00555437, ..., -0.01201999,\n        -0.01785416, -0.01386872]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.05559206, -0.03301726, -0.01646268, ...,  0.02500414,\n         0.01045272, -0.01133694],\n       [-0.06149707, -0.04905558, -0.01345656, ...,  0.01717276,\n        -0.0592617 , -0.06165091],\n       [ 0.04440522,  0.00500455, -0.00488806, ..., -0.00153356,\n        -0.02387191,  0.01460617],\n       ...,\n       [ 0.01077382,  0.02391537,  0.04324631, ...,  0.02716968,\n        -0.04910111,  0.06719807],\n       [-0.00891744,  0.02139229,  0.01219342, ..., -0.03232003,\n        -0.00695406,  0.0069576 ],\n       [-0.03075593, -0.02856579,  0.01786736, ...,  0.00336823,\n         0.0068219 ,  0.00551834]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.load_model(version_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}