{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600405829009",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Dropout,\n",
    "    LSTMCell,\n",
    "    RNN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'20200918-142458'"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/\" + timestamp\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "os.makedirs(log_dir)\n",
    "os.makedirs(version_dir)\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_AR_Multiple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = {\n",
    "    'PAST_HISTORY': 16,\n",
    "    'FUTURE_TARGET': 8,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'BUFFER_SIZE': 200000,\n",
    "    'EPOCHS': 500,\n",
    "    'VOCAB_SIZE': 16293\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_simple = {\n",
    "    \"HP_LSTM_1_UNITS\" : 128,\n",
    "    \"HP_LSTM_1_DROPOUT\" : 0.0,\n",
    "    \"HP_LEARNING_RATE\" : 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_multiple = {\n",
    "    \"HP_LSTM_1_UNITS\" : 32,\n",
    "    \"HP_LSTM_2_UNITS\" : 32,\n",
    "    \"HP_LSTM_1_DROPOUT\" : 0.0,\n",
    "    \"HP_LSTM_2_DROPOUT\" : 0.0,\n",
    "    \"HP_LEARNING_RATE\" : 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/SEG_train_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_train, y_train = generate_timeseries(train_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().batch(static_params[\"BATCH_SIZE\"]).shuffle(static_params[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/SEG_val_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_val, y_val = generate_timeseries(val_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.cache().batch(static_params[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGARSimple(keras.Model):\n",
    "    def __init__(self, units, dropout, output_steps, output_size):\n",
    "        super().__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = LSTMCell(units, dropout=dropout)\n",
    "\n",
    "        self.lstm_rnn = RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = Dense(output_size, activation=\"softmax\")\n",
    "\n",
    "    @tf.function\n",
    "    def warmup(self, inputs):\n",
    "        onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(onehot_inputs)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "\n",
    "        return prediction, state\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        #predictions = []\n",
    "        predictions = tf.TensorArray(tf.float32, size=self.output_steps, clear_after_read=False)\n",
    "        # Initialize the lstm state\n",
    "        prediction, state = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction\n",
    "        #predictions.append(prediction)\n",
    "        predictions = predictions.write(0, prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps\n",
    "        for i in tf.range(1, self.output_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "\n",
    "            # Add the prediction to the output\n",
    "            #predictions.append(prediction)\n",
    "            predictions = predictions.write(i, prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        #predictions = tf.stack(predictions)\n",
    "        predictions = predictions.stack()\n",
    "\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGARMultiple(keras.Model):\n",
    "    def __init__(self, units_1, units_2, dropout_1, dropout_2, output_steps, output_size):\n",
    "        super().__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.units_1 = units_1\n",
    "        self.units_2 = units_2\n",
    "        self.dropout_1 = dropout_1\n",
    "        self.dropout_2 = dropout_2\n",
    "\n",
    "        self.lstm_cell_1 = LSTMCell(units_1, dropout=dropout_1)\n",
    "        self.lstm_cell_2 = LSTMCell(units_2, dropout=dropout_2)\n",
    "\n",
    "        self.lstm_rnn_1 = RNN(self.lstm_cell_1, return_state=True, return_sequences=True)\n",
    "        self.lstm_rnn_2 = RNN(self.lstm_cell_2, return_state=True)\n",
    "        self.dense = Dense(output_size, activation=\"softmax\")\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def warmup(self, inputs):\n",
    "        onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x_1, *state_1 = self.lstm_rnn_1(onehot_inputs)\n",
    "        x_2, *state_2 = self.lstm_rnn_2(x_1)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x_2)\n",
    "\n",
    "        return prediction, state_1, state_2\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        #predictions = []\n",
    "        predictions = tf.TensorArray(tf.float32, size=self.output_steps, clear_after_read=False)\n",
    "\n",
    "        # Initialize the lstm state\n",
    "        prediction, state_1, state_2 = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction\n",
    "        #predictions.append(prediction)\n",
    "        predictions = predictions.write(0, prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps\n",
    "        for i in tf.range(1, self.output_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "\n",
    "            # Execute one lstm step.\n",
    "            x_1, state_1 = self.lstm_cell_1(x, states=state_1, training=training)\n",
    "            x_2, state_2 = self.lstm_cell_2(x_1, states=state_2, training=training)\n",
    "\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x_2)\n",
    "\n",
    "            # Add the prediction to the output\n",
    "            #predictions.append(prediction)\n",
    "            predictions = predictions.write(i, prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        #predictions = tf.stack(predictions)\n",
    "        predictions = predictions.stack()\n",
    "\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEGARSimple(\n",
    "    units=hparams_multiple[\"HP_LSTM_1_UNITS\"], dropout=hparams_multiple[\"HP_LSTM_1_DROPOUT\"], \n",
    "    output_steps=static_params[\"FUTURE_TARGET\"], output_size=static_params[\"VOCAB_SIZE\"])"
   ]
  },
  {
   "source": [
    "model = SEGARMultiple(\n",
    "    units_1=hparams_multiple[\"HP_LSTM_1_UNITS\"], units_2=hparams_multiple[\"HP_LSTM_2_UNITS\"], dropout_1=hparams_multiple[\"HP_LSTM_1_DROPOUT\"], \n",
    "    dropout_2=hparams_multiple[\"HP_LSTM_2_DROPOUT\"], output_steps=static_params[\"FUTURE_TARGET\"], output_size=static_params[\"VOCAB_SIZE\"])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(hparams_multiple[\"HP_LEARNING_RATE\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"static/test_pipeline.pkl\", \"rb\") as p:\n",
    "    test_pipeline = dill.load(p)\n",
    "\n",
    "test_set = np.genfromtxt(\"data/SEG_test_set_original.csv\", delimiter=\"\\n\", dtype=np.int64)\n",
    "processed_test_set = test_pipeline.transform(test_set.copy())\n",
    "x_test, y_test = generate_timeseries(processed_test_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===============>..] - ETA: 1s - loss: 7.4290 - accuracy: 0.23305/315 [============================>.] - ETA: 1s - loss: 7.4314 - accuracy: 0.23306/315 [============================>.] - ETA: 1s - loss: 7.4157 - accuracy: 0.23307/315 [============================>.] - ETA: 1s - loss: 7.3999 - accuracy: 0.23308/315 [============================>.] - ETA: 0s - loss: 7.3941 - accuracy: 0.23309/315 [============================>.] - ETA: 0s - loss: 7.3857 - accuracy: 0.23310/315 [============================>.] - ETA: 0s - loss: 7.3916 - accuracy: 0.23311/315 [============================>.] - ETA: 0s - loss: 7.3755 - accuracy: 0.23312/315 [============================>.] - ETA: 0s - loss: 7.3803 - accuracy: 0.23313/315 [============================>.] - ETA: 0s - loss: 7.3690 - accuracy: 0.23314/315 [============================>.] - ETA: 0s - loss: 7.3608 - accuracy: 0.23315/315 [==============================] - ETA: 0s - loss: 7.3606 - accuracy: 0.23315/315 [==============================] - 45s 143ms/step - loss: 7.3606 - accuracy: 0.2347 - val_loss: 7.8424 - val_accuracy: 0.1940\n   1/1112 [..............................] - ETA: 0s - loss: 9.4067 - accuracy: 0.0000e+   6/1112 [..............................] - ETA: 9s - loss: 9.3547 - accuracy: 0.01  11/1112 [..............................] - ETA: 10s - loss: 9.4860 - accuracy: 0.005  16/1112 [..............................] - ETA: 11s - loss: 9.1800 - accuracy: 0.052  21/1112 [..............................] - ETA: 11s - loss: 9.2638 - accuracy: 0.055  26/1112 [..............................] - ETA: 11s - loss: 9.0249 - accuracy: 0.082  31/1112 [..............................] - ETA: 11s - loss: 9.0680 - accuracy: 0.080  36/1112 [..............................] - ETA: 11s - loss: 9.1207 - accuracy: 0.076  41/1112 [>.............................] - ETA: 11s - loss: 8.9144 - accuracy: 0.085  46/1112 [>.............................] - ETA: 11s - loss: 8.6475 - accuracy: 0.090  51/1112 [>.............................] - ETA: 11s - loss: 8.3125 - accuracy: 0.093  56/1112 [>.............................] - ETA: 11s - loss: 8.0233 - accuracy: 0.112  61/1112 [>.............................] - ETA: 11s - loss: 7.7879 - accuracy: 0.111  66/1112 [>.............................] - ETA: 11s - loss: 7.8508 - accuracy: 0.112  71/1112 [>.............................] - ETA: 11s - loss: 7.6689 - accuracy: 0.123  76/1112 [=>............................] - ETA: 11s - loss: 7.5254 - accuracy: 0.125  81/1112 [=>............................] - ETA: 11s - loss: 7.5592 - accuracy: 0.126  86/1112 [=>............................] - ETA: 11s - loss: 7.4458 - accuracy: 0.120  91/1112 [=>............................] - ETA: 10s - loss: 7.3515 - accuracy: 0.119  96/1112 [=>............................] - ETA: 10s - loss: 7.3475 - accuracy: 0.119 101/1112 [=>............................] - ETA: 10s - loss: 7.4560 - accuracy: 0.117 106/1112 [=>............................] - ETA: 10s - loss: 7.4628 - accuracy: 0.125 111/1112 [=>............................] - ETA: 10s - loss: 7.5129 - accuracy: 0.126 116/1112 [==>...........................] - ETA: 10s - loss: 7.5665 - accuracy: 0.125 121/1112 [==>...........................] - ETA: 10s - loss: 7.6033 - accuracy: 0.127 126/1112 [==>...........................] - ETA: 10s - loss: 7.7063 - accuracy: 0.122 131/1112 [==>...........................] - ETA: 10s - loss: 7.7958 - accuracy: 0.118 136/1112 [==>...........................] - ETA: 10s - loss: 7.8481 - accuracy: 0.115 141/1112 [==>...........................] - ETA: 10s - loss: 7.8584 - accuracy: 0.112 146/1112 [==>...........................] - ETA: 10s - loss: 7.8580 - accuracy: 0.108 151/1112 [===>..........................] - ETA: 10s - loss: 7.8801 - accuracy: 0.104 156/1112 [===>..........................] - ETA: 10s - loss: 7.9378 - accuracy: 0.101 161/1112 [===>..........................] - ETA: 10s - loss: 7.9370 - accuracy: 0.098 166/1112 [===>..........................] - ETA: 10s - loss: 7.9431 - accuracy: 0.095 171/1112 [===>..........................] - ETA: 10s - loss: 7.8981 - accuracy: 0.092 176/1112 [===>..........................] - ETA: 10s - loss: 7.9180 - accuracy: 0.089 181/1112 [===>..........................] - ETA: 10s - loss: 7.9745 - accuracy: 0.087 186/1112 [====>.........................] - ETA: 10s - loss: 8.0149 - accuracy: 0.085 191/1112 [====>.........................] - ETA: 10s - loss: 8.0516 - accuracy: 0.083 196/1112 [====>.........................] - ETA: 9s - loss: 8.0772 - accuracy: 0.08 201/1112 [====>.........................] - ETA: 9s - loss: 8.1091 - accuracy: 0.08 206/1112 [====>.........................] - ETA: 9s - loss: 8.1499 - accuracy: 0.08 211/1112 [====>.........................] - ETA: 9s - loss: 8.1872 - accuracy: 0.08 216/1112 [====>.........................] - ETA: 9s - loss: 8.2204 - accuracy: 0.08 221/1112 [====>.........................] - ETA: 9s - loss: 8.2511 - accuracy: 0.08 226/1112 [=====>........................] - ETA: 9s - loss: 8.2753 - accuracy: 0.08 231/1112 [=====>........................] - ETA: 9s - loss: 8.3076 - accuracy: 0.08 236/1112 [=====>........................] - ETA: 9s - loss: 8.3379 - accuracy: 0.08 241/1112 [=====>........................] - ETA: 9s - loss: 8.3687 - accuracy: 0.07 246/1112 [=====>........................] - ETA: 9s - loss: 8.3886 - accuracy: 0.07 251/1112 [=====>........................] - ETA: 9s - loss: 8.4183 - accuracy: 0.07 256/1112 [=====>........................] - ETA: 9s - loss: 8.4521 - accuracy: 0.07 261/1112 [======>.......................] - ETA: 9s - loss: 8.4785 - accuracy: 0.07 266/1112 [======>.......................] - ETA: 9s - loss: 8.5058 - accuracy: 0.07 271/1112 [======>.......................] - ETA: 9s - loss: 8.5320 - accuracy: 0.07 276/1112 [======>.......................] - ETA: 9s - loss: 8.5512 - accuracy: 0.07 281/1112 [======>.......................] - ETA: 9s - loss: 8.5786 - accuracy: 0.07 286/1112 [======>.......................] - ETA: 9s - loss: 8.6021 - accuracy: 0.07 291/1112 [======>.......................] - ETA: 8s - loss: 8.5566 - accuracy: 0.07 296/1112 [======>.......................] - ETA: 8s - loss: 8.5437 - accuracy: 0.07 301/1112 [=======>......................] - ETA: 8s - loss: 8.5095 - accuracy: 0.08 306/1112 [=======>......................] - ETA: 8s - loss: 8.4578 - accuracy: 0.08 311/1112 [=======>......................] - ETA: 8s - loss: 8.4217 - accuracy: 0.09 316/1112 [=======>......................] - ETA: 8s - loss: 8.4215 - accuracy: 0.09 321/1112 [=======>......................] - ETA: 8s - loss: 8.4111 - accuracy: 0.09 326/1112 [=======>......................] - ETA: 8s - loss: 8.3996 - accuracy: 0.09 331/1112 [=======>......................] - ETA: 8s - loss: 8.4052 - accuracy: 0.09 336/1112 [========>.....................] - ETA: 8s - loss: 8.3919 - accuracy: 0.09 341/1112 [========>.....................] - ETA: 8s - loss: 8.3900 - accuracy: 0.09 346/1112 [========>.....................] - ETA: 8s - loss: 8.3712 - accuracy: 0.09 351/1112 [========>.....................] - ETA: 8s - loss: 8.3859 - accuracy: 0.09 356/1112 [========>.....................] - ETA: 8s - loss: 8.3652 - accuracy: 0.10 361/1112 [========>.....................] - ETA: 8s - loss: 8.2857 - accuracy: 0.11 366/1112 [========>.....................] - ETA: 8s - loss: 8.1979 - accuracy: 0.12 371/1112 [=========>....................] - ETA: 8s - loss: 8.1114 - accuracy: 0.13 376/1112 [=========>....................] - ETA: 8s - loss: 8.0243 - accuracy: 0.14 381/1112 [=========>....................] - ETA: 7s - loss: 7.9462 - accuracy: 0.15 386/1112 [=========>....................] - ETA: 7s - loss: 7.8737 - accuracy: 0.16 391/1112 [=========>....................] - ETA: 7s - loss: 7.8001 - accuracy: 0.17 396/1112 [=========>....................] - ETA: 7s - loss: 7.7712 - accuracy: 0.17 401/1112 [=========>....................] - ETA: 7s - loss: 7.7350 - accuracy: 0.18 406/1112 [=========>....................] - ETA: 7s - loss: 7.6875 - accuracy: 0.18 411/1112 [==========>...................] - ETA: 7s - loss: 7.6994 - accuracy: 0.18 416/1112 [==========>...................] - ETA: 7s - loss: 7.7313 - accuracy: 0.18 421/1112 [==========>...................] - ETA: 7s - loss: 7.7337 - accuracy: 0.18 426/1112 [==========>...................] - ETA: 7s - loss: 7.7252 - accuracy: 0.18 431/1112 [==========>...................] - ETA: 7s - loss: 7.7137 - accuracy: 0.18 436/1112 [==========>...................] - ETA: 7s - loss: 7.6562 - accuracy: 0.19 441/1112 [==========>...................] - ETA: 7s - loss: 7.6076 - accuracy: 0.20 446/1112 [===========>..................] - ETA: 7s - loss: 7.5620 - accuracy: 0.20 451/1112 [===========>..................] - ETA: 7s - loss: 7.5314 - accuracy: 0.21 456/1112 [===========>..................] - ETA: 7s - loss: 7.5054 - accuracy: 0.21 461/1112 [===========>..................] - ETA: 7s - loss: 7.4903 - accuracy: 0.21 466/1112 [===========>..................] - ETA: 7s - loss: 7.4789 - accuracy: 0.21 471/1112 [===========>..................] - ETA: 7s - loss: 7.4883 - accuracy: 0.21 476/1112 [===========>..................] - ETA: 6s - loss: 7.5032 - accuracy: 0.21 481/1112 [===========>..................] - ETA: 6s - loss: 7.4782 - accuracy: 0.22 486/1112 [============>.................] - ETA: 6s - loss: 7.4775 - accuracy: 0.22 491/1112 [============>.................] - ETA: 6s - loss: 7.4896 - accuracy: 0.21 496/1112 [============>.................] - ETA: 6s - loss: 7.4907 - accuracy: 0.21 501/1112 [============>.................] - ETA: 6s - loss: 7.5067 - accuracy: 0.21 506/1112 [============>.................] - ETA: 6s - loss: 7.5041 - accuracy: 0.21 511/1112 [============>.................] - ETA: 6s - loss: 7.5271 - accuracy: 0.21 516/1112 [============>.................] - ETA: 6s - loss: 7.5469 - accuracy: 0.21 521/1112 [=============>................] - ETA: 6s - loss: 7.5563 - accuracy: 0.21 526/1112 [=============>................] - ETA: 6s - loss: 7.5396 - accuracy: 0.21 531/1112 [=============>................] - ETA: 6s - loss: 7.5560 - accuracy: 0.21 536/1112 [=============>................] - ETA: 6s - loss: 7.5736 - accuracy: 0.21 541/1112 [=============>................] - ETA: 6s - loss: 7.5781 - accuracy: 0.21 546/1112 [=============>................] - ETA: 6s - loss: 7.5977 - accuracy: 0.21 551/1112 [=============>................] - ETA: 6s - loss: 7.6082 - accuracy: 0.21 556/1112 [==============>...............] - ETA: 6s - loss: 7.6202 - accuracy: 0.21 561/1112 [==============>...............] - ETA: 6s - loss: 7.6380 - accuracy: 0.20 566/1112 [==============>...............] - ETA: 5s - loss: 7.6529 - accuracy: 0.20 571/1112 [==============>...............] - ETA: 5s - loss: 7.6684 - accuracy: 0.20 576/1112 [==============>...............] - ETA: 5s - loss: 7.6855 - accuracy: 0.20 581/1112 [==============>...............] - ETA: 5s - loss: 7.6980 - accuracy: 0.20 586/1112 [==============>...............] - ETA: 5s - loss: 7.7120 - accuracy: 0.20 591/1112 [==============>...............] - ETA: 5s - loss: 7.7131 - accuracy: 0.20 596/1112 [===============>..............] - ETA: 5s - loss: 7.7289 - accuracy: 0.19 601/1112 [===============>..............] - ETA: 5s - loss: 7.7431 - accuracy: 0.19 606/1112 [===============>..............] - ETA: 5s - loss: 7.7476 - accuracy: 0.19 611/1112 [===============>..............] - ETA: 5s - loss: 7.7395 - accuracy: 0.19 616/1112 [===============>..............] - ETA: 5s - loss: 7.7248 - accuracy: 0.19 621/1112 [===============>..............] - ETA: 5s - loss: 7.6985 - accuracy: 0.20 626/1112 [===============>..............] - ETA: 5s - loss: 7.6978 - accuracy: 0.20 631/1112 [================>.............] - ETA: 5s - loss: 7.6930 - accuracy: 0.20 636/1112 [================>.............] - ETA: 5s - loss: 7.6767 - accuracy: 0.20 641/1112 [================>.............] - ETA: 5s - loss: 7.6763 - accuracy: 0.20 646/1112 [================>.............] - ETA: 5s - loss: 7.6736 - accuracy: 0.20 651/1112 [================>.............] - ETA: 5s - loss: 7.6567 - accuracy: 0.20 656/1112 [================>.............] - ETA: 4s - loss: 7.6527 - accuracy: 0.20 661/1112 [================>.............] - ETA: 4s - loss: 7.6668 - accuracy: 0.20 666/1112 [================>.............] - ETA: 4s - loss: 7.6701 - accuracy: 0.20 671/1112 [=================>............] - ETA: 4s - loss: 7.6687 - accuracy: 0.20 676/1112 [=================>............] - ETA: 4s - loss: 7.6719 - accuracy: 0.20 681/1112 [=================>............] - ETA: 4s - loss: 7.6798 - accuracy: 0.19 686/1112 [=================>............] - ETA: 4s - loss: 7.6874 - accuracy: 0.19 691/1112 [=================>............] - ETA: 4s - loss: 7.6976 - accuracy: 0.19 696/1112 [=================>............] - ETA: 4s - loss: 7.7055 - accuracy: 0.19 701/1112 [=================>............] - ETA: 4s - loss: 7.7206 - accuracy: 0.19 706/1112 [==================>...........] - ETA: 4s - loss: 7.7385 - accuracy: 0.19 711/1112 [==================>...........] - ETA: 4s - loss: 7.7535 - accuracy: 0.19 716/1112 [==================>...........] - ETA: 4s - loss: 7.7602 - accuracy: 0.19 721/1112 [==================>...........] - ETA: 4s - loss: 7.7608 - accuracy: 0.19 726/1112 [==================>...........] - ETA: 4s - loss: 7.7584 - accuracy: 0.18 731/1112 [==================>...........] - ETA: 4s - loss: 7.7707 - accuracy: 0.18 736/1112 [==================>...........] - ETA: 4s - loss: 7.7744 - accuracy: 0.18 741/1112 [==================>...........] - ETA: 4s - loss: 7.7761 - accuracy: 0.18 746/1112 [===================>..........] - ETA: 3s - loss: 7.7842 - accuracy: 0.18 751/1112 [===================>..........] - ETA: 3s - loss: 7.7867 - accuracy: 0.18 756/1112 [===================>..........] - ETA: 3s - loss: 7.8007 - accuracy: 0.18 761/1112 [===================>..........] - ETA: 3s - loss: 7.8157 - accuracy: 0.18 766/1112 [===================>..........] - ETA: 3s - loss: 7.8224 - accuracy: 0.17 771/1112 [===================>..........] - ETA: 3s - loss: 7.8312 - accuracy: 0.17 776/1112 [===================>..........] - ETA: 3s - loss: 7.8406 - accuracy: 0.17 781/1112 [====================>.........] - ETA: 3s - loss: 7.8505 - accuracy: 0.17 786/1112 [====================>.........] - ETA: 3s - loss: 7.8601 - accuracy: 0.17 791/1112 [====================>.........] - ETA: 3s - loss: 7.8716 - accuracy: 0.17 796/1112 [====================>.........] - ETA: 3s - loss: 7.8828 - accuracy: 0.17 801/1112 [====================>.........] - ETA: 3s - loss: 7.8917 - accuracy: 0.17 806/1112 [====================>.........] - ETA: 3s - loss: 7.9026 - accuracy: 0.17 811/1112 [====================>.........] - ETA: 3s - loss: 7.9146 - accuracy: 0.17 816/1112 [=====================>........] - ETA: 3s - loss: 7.9254 - accuracy: 0.17 821/1112 [=====================>........] - ETA: 3s - loss: 7.9330 - accuracy: 0.17 826/1112 [=====================>........] - ETA: 3s - loss: 7.9449 - accuracy: 0.17 831/1112 [=====================>........] - ETA: 3s - loss: 7.9529 - accuracy: 0.17 836/1112 [=====================>........] - ETA: 3s - loss: 7.9625 - accuracy: 0.17 841/1112 [=====================>........] - ETA: 2s - loss: 7.9741 - accuracy: 0.16 846/1112 [=====================>........] - ETA: 2s - loss: 7.9859 - accuracy: 0.16 851/1112 [=====================>........] - ETA: 2s - loss: 7.9939 - accuracy: 0.16 856/1112 [======================>.......] - ETA: 2s - loss: 8.0061 - accuracy: 0.16 861/1112 [======================>.......] - ETA: 2s - loss: 8.0176 - accuracy: 0.16 866/1112 [======================>.......] - ETA: 2s - loss: 8.0097 - accuracy: 0.16 871/1112 [======================>.......] - ETA: 2s - loss: 8.0037 - accuracy: 0.16 876/1112 [======================>.......] - ETA: 2s - loss: 7.9989 - accuracy: 0.16 881/1112 [======================>.......] - ETA: 2s - loss: 7.9870 - accuracy: 0.16 886/1112 [======================>.......] - ETA: 2s - loss: 7.9659 - accuracy: 0.17 891/1112 [=======================>......] - ETA: 2s - loss: 7.9574 - accuracy: 0.17 896/1112 [=======================>......] - ETA: 2s - loss: 7.9587 - accuracy: 0.17 901/1112 [=======================>......] - ETA: 2s - loss: 7.9572 - accuracy: 0.17 906/1112 [=======================>......] - ETA: 2s - loss: 7.9605 - accuracy: 0.17 911/1112 [=======================>......] - ETA: 2s - loss: 7.9569 - accuracy: 0.17 916/1112 [=======================>......] - ETA: 2s - loss: 7.9588 - accuracy: 0.17 921/1112 [=======================>......] - ETA: 2s - loss: 7.9585 - accuracy: 0.17 926/1112 [=======================>......] - ETA: 2s - loss: 7.9658 - accuracy: 0.17 931/1112 [========================>.....] - ETA: 1s - loss: 7.9632 - accuracy: 0.17 936/1112 [========================>.....] - ETA: 1s - loss: 7.9656 - accuracy: 0.17 941/1112 [========================>.....] - ETA: 1s - loss: 7.9652 - accuracy: 0.17 946/1112 [========================>.....] - ETA: 1s - loss: 7.9751 - accuracy: 0.17 951/1112 [========================>.....] - ETA: 1s - loss: 7.9825 - accuracy: 0.17 956/1112 [========================>.....] - ETA: 1s - loss: 7.9803 - accuracy: 0.17 961/1112 [========================>.....] - ETA: 1s - loss: 7.9694 - accuracy: 0.17 966/1112 [=========================>....] - ETA: 1s - loss: 7.9414 - accuracy: 0.17 971/1112 [=========================>....] - ETA: 1s - loss: 7.9178 - accuracy: 0.17 976/1112 [=========================>....] - ETA: 1s - loss: 7.9120 - accuracy: 0.18 981/1112 [=========================>....] - ETA: 1s - loss: 7.9127 - accuracy: 0.18 986/1112 [=========================>....] - ETA: 1s - loss: 7.8916 - accuracy: 0.18 991/1112 [=========================>....] - ETA: 1s - loss: 7.8770 - accuracy: 0.18 996/1112 [=========================>....] - ETA: 1s - loss: 7.8596 - accuracy: 0.181001/1112 [==========================>...] - ETA: 1s - loss: 7.8529 - accuracy: 0.181006/1112 [==========================>...] - ETA: 1s - loss: 7.8502 - accuracy: 0.181011/1112 [==========================>...] - ETA: 1s - loss: 7.8331 - accuracy: 0.181016/1112 [==========================>...] - ETA: 1s - loss: 7.8309 - accuracy: 0.181021/1112 [==========================>...] - ETA: 0s - loss: 7.8184 - accuracy: 0.181026/1112 [==========================>...] - ETA: 0s - loss: 7.8060 - accuracy: 0.181031/1112 [==========================>...] - ETA: 0s - loss: 7.7802 - accuracy: 0.181036/1112 [==========================>...] - ETA: 0s - loss: 7.7563 - accuracy: 0.191041/1112 [===========================>..] - ETA: 0s - loss: 7.7355 - accuracy: 0.191046/1112 [===========================>..] - ETA: 0s - loss: 7.7087 - accuracy: 0.191051/1112 [===========================>..] - ETA: 0s - loss: 7.6800 - accuracy: 0.201056/1112 [===========================>..] - ETA: 0s - loss: 7.6508 - accuracy: 0.201061/1112 [===========================>..] - ETA: 0s - loss: 7.6285 - accuracy: 0.201066/1112 [===========================>..] - ETA: 0s - loss: 7.6096 - accuracy: 0.211071/1112 [===========================>..] - ETA: 0s - loss: 7.5872 - accuracy: 0.211076/1112 [============================>.] - ETA: 0s - loss: 7.5623 - accuracy: 0.211081/1112 [============================>.] - ETA: 0s - loss: 7.5384 - accuracy: 0.211086/1112 [============================>.] - ETA: 0s - loss: 7.5253 - accuracy: 0.211091/1112 [============================>.] - ETA: 0s - loss: 7.5243 - accuracy: 0.211096/1112 [============================>.] - ETA: 0s - loss: 7.5182 - accuracy: 0.211101/1112 [============================>.] - ETA: 0s - loss: 7.5074 - accuracy: 0.211106/1112 [============================>.] - ETA: 0s - loss: 7.5080 - accuracy: 0.211111/1112 [============================>.] - ETA: 0s - loss: 7.4971 - accuracy: 0.211112/1112 [==============================] - 12s 11ms/step - loss: 7.4967 - accuracy: 0.2176\n"
    }
   ],
   "source": [
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp_api.hparams(hparams_multiple)\n",
    "    history = model.fit(train_data, validation_data=val_data, epochs=1, callbacks=[\n",
    "        keras.callbacks.EarlyStopping('val_accuracy', patience=5),\n",
    "        keras.callbacks.TensorBoard(log_dir)\n",
    "        ])\n",
    "\n",
    "    #loss, acc = model.evaluate(x_test, y_test)\n",
    "    #tf.summary.scalar(\"test_loss\", loss, step=1)\n",
    "    #tf.summary.scalar(\"test_accuracy\", acc, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: version/20200918-103208/{saved_model.pbtxt|saved_model.pb}",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-cc59066136c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"version/20200918-103208\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m   \"\"\"\n\u001b[1;32m--> 578\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m--> 588\u001b[1;33m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \"\"\"\n\u001b[1;32m---> 56\u001b[1;33m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m                   (export_dir,\n\u001b[0;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: version/20200918-103208/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "new_model = tf.saved_model.load(\"version/20200918-103208\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-60181c3a7fd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: version/20200918-142458\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, version_dir, \n",
    "    signatures=model.call.get_concrete_function(tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32, name=\"call\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}