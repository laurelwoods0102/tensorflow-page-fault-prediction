{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600248737147",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TimeDistributed, \n",
    "    Dense, \n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Bidirectional, \n",
    "    LSTM, \n",
    "    Dropout,\n",
    "    Lambda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = {\n",
    "    'PAST_HISTORY': 16,\n",
    "    'FUTURE_TARGET': 8,\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'BUFFER_SIZE': 200000,\n",
    "    'EPOCHS': 500,\n",
    "    'VOCAB_SIZE': 16293\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/SEG_train_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_train, y_train = generate_timeseries(train_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().batch(static_params[\"BATCH_SIZE\"]).shuffle(static_params[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/SEG_val_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_val, y_val = generate_timeseries(val_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.cache().batch(static_params[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(161267, 16, 1)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBack(keras.Model):\n",
    "    def __init__(self, units, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = keras.layers.LSTMCell(units)\n",
    "\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = Dense(static_params[\"VOCAB_SIZE\"], activation=\"softmax\")\n",
    "        #self.decode = Lambda(lambda x: tf.reshape(tf.cast(tf.math.argmax(x, axis=1), tf.float32), (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_model = FeedBack(units=16, out_steps=static_params[\"FUTURE_TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(self, inputs):\n",
    "    onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "    # inputs.shape => (batch, time, features)\n",
    "    # x.shape => (batch, lstm_units)\n",
    "    x, *state = self.lstm_rnn(onehot_inputs)\n",
    "\n",
    "    # predictions.shape => (batch, features)\n",
    "    #prediction = self.dense(x)\n",
    "    prediction = self.dense(x)\n",
    "    #prediction = self.decode(prediction)\n",
    "\n",
    "    return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 16293), dtype=float32, numpy=\narray([[6.1300219e-05, 6.1480685e-05, 6.1337698e-05, ..., 6.1370658e-05,\n        6.1381965e-05, 6.1353712e-05],\n       [6.1300219e-05, 6.1480685e-05, 6.1337698e-05, ..., 6.1370658e-05,\n        6.1381965e-05, 6.1353712e-05]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "prediction, state = feedback_model.warmup(x_train[:2].reshape(2, -1, 1))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training=None):\n",
    "    # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "    predictions = []\n",
    "    # Initialize the lstm state\n",
    "    prediction, state = self.warmup(inputs)\n",
    "\n",
    "    # Insert the first prediction\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # Run the rest of the prediction steps\n",
    "    for _ in range(self.out_steps - 1):\n",
    "        # Use the last prediction as input.\n",
    "        x = prediction\n",
    "\n",
    "        # Execute one lstm step.\n",
    "        x, state = self.lstm_cell(x, states=state, training=training)\n",
    "\n",
    "        # Convert the lstm output to a prediction.\n",
    "        #prediction = self.dense(x)\n",
    "        prediction = self.dense(x)\n",
    "        #prediction = self.decode(prediction)\n",
    "\n",
    "\n",
    "        # Add the prediction to the output\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # predictions.shape => (time, batch, features)\n",
    "    predictions = tf.stack(predictions)\n",
    "\n",
    "    # predictions.shape => (batch, time, features)\n",
    "    predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "    return predictions\n",
    "\n",
    "FeedBack.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([2, 8, 16293])"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "result = feedback_model(x_train[:2].reshape(2, -1, 1))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 0.24114/158 [====================>.........] - ETA: 7s - loss: 6.3676 - accuracy: 0.24115/158 [====================>.........] - ETA: 7s - loss: 6.3287 - accuracy: 0.24116/158 [=====================>........] - ETA: 7s - loss: 6.3327 - accuracy: 0.24117/158 [=====================>........] - ETA: 7s - loss: 6.3463 - accuracy: 0.24118/158 [=====================>........] - ETA: 7s - loss: 6.3555 - accuracy: 0.24119/158 [=====================>........] - ETA: 6s - loss: 6.3447 - accuracy: 0.24120/158 [=====================>........] - ETA: 6s - loss: 6.3617 - accuracy: 0.24121/158 [=====================>........] - ETA: 6s - loss: 6.3251 - accuracy: 0.24122/158 [======================>.......] - ETA: 6s - loss: 6.2884 - accuracy: 0.24123/158 [======================>.......] - ETA: 6s - loss: 6.3001 - accuracy: 0.23124/158 [======================>.......] - ETA: 6s - loss: 6.3078 - accuracy: 0.23125/158 [======================>.......] - ETA: 5s - loss: 6.3381 - accuracy: 0.23126/158 [======================>.......] - ETA: 5s - loss: 6.3635 - accuracy: 0.23127/158 [=======================>......] - ETA: 5s - loss: 6.3820 - accuracy: 0.23128/158 [=======================>......] - ETA: 5s - loss: 6.3913 - accuracy: 0.23129/158 [=======================>......] - ETA: 5s - loss: 6.4093 - accuracy: 0.23130/158 [=======================>......] - ETA: 4s - loss: 6.4204 - accuracy: 0.23131/158 [=======================>......] - ETA: 4s - loss: 6.4080 - accuracy: 0.23132/158 [========================>.....] - ETA: 4s - loss: 6.4357 - accuracy: 0.23133/158 [========================>.....] - ETA: 4s - loss: 6.4325 - accuracy: 0.23134/158 [========================>.....] - ETA: 4s - loss: 6.4314 - accuracy: 0.23135/158 [========================>.....] - ETA: 4s - loss: 6.4390 - accuracy: 0.23136/158 [========================>.....] - ETA: 3s - loss: 6.4454 - accuracy: 0.23137/158 [=========================>....] - ETA: 3s - loss: 6.4422 - accuracy: 0.23138/158 [=========================>....] - ETA: 3s - loss: 6.4136 - accuracy: 0.23139/158 [=========================>....] - ETA: 3s - loss: 6.3809 - accuracy: 0.23140/158 [=========================>....] - ETA: 3s - loss: 6.3484 - accuracy: 0.23141/158 [=========================>....] - ETA: 3s - loss: 6.3450 - accuracy: 0.23142/158 [=========================>....] - ETA: 2s - loss: 6.3379 - accuracy: 0.23143/158 [==========================>...] - ETA: 2s - loss: 6.3439 - accuracy: 0.23144/158 [==========================>...] - ETA: 2s - loss: 6.3595 - accuracy: 0.23145/158 [==========================>...] - ETA: 2s - loss: 6.3586 - accuracy: 0.23146/158 [==========================>...] - ETA: 2s - loss: 6.3498 - accuracy: 0.23147/158 [==========================>...] - ETA: 1s - loss: 6.3579 - accuracy: 0.23148/158 [===========================>..] - ETA: 1s - loss: 6.3767 - accuracy: 0.23149/158 [===========================>..] - ETA: 1s - loss: 6.3835 - accuracy: 0.23150/158 [===========================>..] - ETA: 1s - loss: 6.3753 - accuracy: 0.23151/158 [===========================>..] - ETA: 1s - loss: 6.3451 - accuracy: 0.23152/158 [===========================>..] - ETA: 1s - loss: 6.3666 - accuracy: 0.23153/158 [============================>.] - ETA: 0s - loss: 6.3509 - accuracy: 0.23154/158 [============================>.] - ETA: 0s - loss: 6.3495 - accuracy: 0.23155/158 [============================>.] - ETA: 0s - loss: 6.3719 - accuracy: 0.23156/158 [============================>.] - ETA: 0s - loss: 6.3437 - accuracy: 0.23157/158 [============================>.] - ETA: 0s - loss: 6.3534 - accuracy: 0.23158/158 [==============================] - ETA: 0s - loss: 6.3544 - accuracy: 0.23158/158 [==============================] - 32s 202ms/step - loss: 6.3544 - accuracy: 0.2352 - val_loss: 7.9154 - val_accuracy: 0.1940\nEpoch 7/50\n  1/158 [..............................] - ETA: 0s - loss: 2.7506 - accuracy: 0.58  2/158 [..............................] - ETA: 13s - loss: 5.0502 - accuracy: 0.432  3/158 [..............................] - ETA: 18s - loss: 5.2185 - accuracy: 0.457  4/158 [..............................] - ETA: 20s - loss: 5.8756 - accuracy: 0.412  5/158 [..............................] - ETA: 21s - loss: 5.8794 - accuracy: 0.427  6/158 [>.............................] - ETA: 22s - loss: 6.4486 - accuracy: 0.360  7/158 [>.............................] - ETA: 22s - loss: 6.6244 - accuracy: 0.325  8/158 [>.............................] - ETA: 23s - loss: 6.5403 - accuracy: 0.331  9/158 [>.............................] - ETA: 23s - loss: 6.7187 - accuracy: 0.321 10/158 [>.............................] - ETA: 23s - loss: 6.7492 - accuracy: 0.310 11/158 [=>............................] - ETA: 23s - loss: 6.9663 - accuracy: 0.283 12/158 [=>............................] - ETA: 23s - loss: 7.0138 - accuracy: 0.282 13/158 [=>............................] - ETA: 23s - loss: 6.6327 - accuracy: 0.263 14/158 [=>............................] - ETA: 23s - loss: 6.7228 - accuracy: 0.257 15/158 [=>............................] - ETA: 23s - loss: 6.4310 - accuracy: 0.243 16/158 [==>...........................] - ETA: 23s - loss: 6.5224 - accuracy: 0.240 17/158 [==>...........................] - ETA: 23s - loss: 6.3465 - accuracy: 0.226 18/158 [==>...........................] - ETA: 23s - loss: 6.0952 - accuracy: 0.214 19/158 [==>...........................] - ETA: 23s - loss: 6.1024 - accuracy: 0.225 20/158 [==>...........................] - ETA: 23s - loss: 6.2799 - accuracy: 0.215 21/158 [==>...........................] - ETA: 23s - loss: 6.4358 - accuracy: 0.207 22/158 [===>..........................] - ETA: 23s - loss: 6.5863 - accuracy: 0.200 23/158 [===>..........................] - ETA: 23s - loss: 6.6395 - accuracy: 0.197 24/158 [===>..........................] - ETA: 23s - loss: 6.6301 - accuracy: 0.196 25/158 [===>..........................] - ETA: 22s - loss: 6.7623 - accuracy: 0.190 26/158 [===>..........................] - ETA: 22s - loss: 6.7682 - accuracy: 0.190 27/158 [====>.........................] - ETA: 22s - loss: 6.6878 - accuracy: 0.205 28/158 [====>.........................] - ETA: 22s - loss: 6.7590 - accuracy: 0.198 29/158 [====>.........................] - ETA: 22s - loss: 6.7888 - accuracy: 0.195 30/158 [====>.........................] - ETA: 22s - loss: 6.7537 - accuracy: 0.194 31/158 [====>.........................] - ETA: 22s - loss: 6.7568 - accuracy: 0.194 32/158 [=====>........................] - ETA: 21s - loss: 6.6449 - accuracy: 0.190 33/158 [=====>........................] - ETA: 21s - loss: 6.5433 - accuracy: 0.205 34/158 [=====>........................] - ETA: 21s - loss: 6.5723 - accuracy: 0.207 35/158 [=====>........................] - ETA: 21s - loss: 6.5216 - accuracy: 0.217 36/158 [=====>........................] - ETA: 21s - loss: 6.4814 - accuracy: 0.216 37/158 [======>.......................] - ETA: 21s - loss: 6.5229 - accuracy: 0.217 38/158 [======>.......................] - ETA: 21s - loss: 6.4008 - accuracy: 0.211 39/158 [======>.......................] - ETA: 20s - loss: 6.3039 - accuracy: 0.228 40/158 [======>.......................] - ETA: 20s - loss: 6.3170 - accuracy: 0.231 41/158 [======>.......................] - ETA: 20s - loss: 6.3684 - accuracy: 0.226 42/158 [======>.......................] - ETA: 20s - loss: 6.2600 - accuracy: 0.220 43/158 [=======>......................] - ETA: 20s - loss: 6.2691 - accuracy: 0.224 44/158 [=======>......................] - ETA: 20s - loss: 6.3543 - accuracy: 0.219 45/158 [=======>......................] - ETA: 19s - loss: 6.3469 - accuracy: 0.219 46/158 [=======>......................] - ETA: 19s - loss: 6.3890 - accuracy: 0.219 47/158 [=======>......................] - ETA: 19s - loss: 6.4418 - accuracy: 0.217 48/158 [========>.....................] - ETA: 19s - loss: 6.4780 - accuracy: 0.217 49/158 [========>.....................] - ETA: 19s - loss: 6.5161 - accuracy: 0.217 50/158 [========>.....................] - ETA: 19s - loss: 6.5396 - accuracy: 0.216 51/158 [========>.....................] - ETA: 18s - loss: 6.5348 - accuracy: 0.221 52/158 [========>.....................] - ETA: 18s - loss: 6.4757 - accuracy: 0.225 53/158 [=========>....................] - ETA: 18s - loss: 6.3957 - accuracy: 0.221 54/158 [=========>....................] - ETA: 18s - loss: 6.3108 - accuracy: 0.217 55/158 [=========>....................] - ETA: 18s - loss: 6.3310 - accuracy: 0.217 56/158 [=========>....................] - ETA: 18s - loss: 6.3651 - accuracy: 0.213 57/158 [=========>....................] - ETA: 17s - loss: 6.3782 - accuracy: 0.214 58/158 [==========>...................] - ETA: 17s - loss: 6.3514 - accuracy: 0.212 59/158 [==========>...................] - ETA: 17s - loss: 6.3670 - accuracy: 0.211 60/158 [==========>...................] - ETA: 17s - loss: 6.3824 - accuracy: 0.209 61/158 [==========>...................] - ETA: 17s - loss: 6.3868 - accuracy: 0.212 62/158 [==========>...................] - ETA: 17s - loss: 6.4039 - accuracy: 0.212 63/158 [==========>...................] - ETA: 16s - loss: 6.3943 - accuracy: 0.212 64/158 [===========>..................] - ETA: 16s - loss: 6.3915 - accuracy: 0.216 65/158 [===========>..................] - ETA: 16s - loss: 6.3402 - accuracy: 0.213 66/158 [===========>..................] - ETA: 16s - loss: 6.3629 - accuracy: 0.212 67/158 [===========>..................] - ETA: 16s - loss: 6.4047 - accuracy: 0.211 68/158 [===========>..................] - ETA: 16s - loss: 6.3993 - accuracy: 0.214 69/158 [============>.................] - ETA: 15s - loss: 6.4467 - accuracy: 0.212 70/158 [============>.................] - ETA: 15s - loss: 6.4591 - accuracy: 0.212 71/158 [============>.................] - ETA: 15s - loss: 6.4813 - accuracy: 0.211 72/158 [============>.................] - ETA: 15s - loss: 6.4426 - accuracy: 0.218 73/158 [============>.................] - ETA: 15s - loss: 6.3945 - accuracy: 0.226 74/158 [=============>................] - ETA: 15s - loss: 6.4147 - accuracy: 0.226 75/158 [=============>................] - ETA: 14s - loss: 6.4504 - accuracy: 0.224 76/158 [=============>................] - ETA: 14s - loss: 6.4599 - accuracy: 0.224 77/158 [=============>................] - ETA: 14s - loss: 6.4848 - accuracy: 0.223 78/158 [=============>................] - ETA: 14s - loss: 6.4482 - accuracy: 0.220 79/158 [==============>...............] - ETA: 14s - loss: 6.4680 - accuracy: 0.218 80/158 [==============>...............] - ETA: 14s - loss: 6.4956 - accuracy: 0.218 81/158 [==============>...............] - ETA: 13s - loss: 6.5274 - accuracy: 0.216 82/158 [==============>...............] - ETA: 13s - loss: 6.4977 - accuracy: 0.222 83/158 [==============>...............] - ETA: 13s - loss: 6.4439 - accuracy: 0.230 84/158 [==============>...............] - ETA: 13s - loss: 6.3894 - accuracy: 0.227 85/158 [===============>..............] - ETA: 13s - loss: 6.3572 - accuracy: 0.225 86/158 [===============>..............] - ETA: 13s - loss: 6.3045 - accuracy: 0.222 87/158 [===============>..............] - ETA: 12s - loss: 6.3287 - accuracy: 0.221 88/158 [===============>..............] - ETA: 12s - loss: 6.2953 - accuracy: 0.218 89/158 [===============>..............] - ETA: 12s - loss: 6.2958 - accuracy: 0.221 90/158 [================>.............] - ETA: 12s - loss: 6.2821 - accuracy: 0.225 91/158 [================>.............] - ETA: 12s - loss: 6.3065 - accuracy: 0.224 92/158 [================>.............] - ETA: 11s - loss: 6.3448 - accuracy: 0.222 93/158 [================>.............] - ETA: 11s - loss: 6.3178 - accuracy: 0.227 94/158 [================>.............] - ETA: 11s - loss: 6.3406 - accuracy: 0.227 95/158 [=================>............] - ETA: 11s - loss: 6.3604 - accuracy: 0.226 96/158 [=================>............] - ETA: 11s - loss: 6.3130 - accuracy: 0.224 97/158 [=================>............] - ETA: 11s - loss: 6.2887 - accuracy: 0.229 98/158 [=================>............] - ETA: 10s - loss: 6.2766 - accuracy: 0.232 99/158 [=================>............] - ETA: 10s - loss: 6.2751 - accuracy: 0.232100/158 [=================>............] - ETA: 10s - loss: 6.2871 - accuracy: 0.232101/158 [==================>...........] - ETA: 10s - loss: 6.2980 - accuracy: 0.232102/158 [==================>...........] - ETA: 10s - loss: 6.2963 - accuracy: 0.234103/158 [==================>...........] - ETA: 10s - loss: 6.2545 - accuracy: 0.232104/158 [==================>...........] - ETA: 9s - loss: 6.2337 - accuracy: 0.23105/158 [==================>...........] - ETA: 9s - loss: 6.2354 - accuracy: 0.23106/158 [===================>..........] - ETA: 9s - loss: 6.2525 - accuracy: 0.23107/158 [===================>..........] - ETA: 9s - loss: 6.2666 - accuracy: 0.23108/158 [===================>..........] - ETA: 9s - loss: 6.2307 - accuracy: 0.24109/158 [===================>..........] - ETA: 8s - loss: 6.1903 - accuracy: 0.24110/158 [===================>..........] - ETA: 8s - loss: 6.1667 - accuracy: 0.23111/158 [====================>.........] - ETA: 8s - loss: 6.1925 - accuracy: 0.23112/158 [====================>.........] - ETA: 8s - loss: 6.2145 - accuracy: 0.23113/158 [====================>.........] - ETA: 8s - loss: 6.2194 - accuracy: 0.23114/158 [====================>.........] - ETA: 8s - loss: 6.2231 - accuracy: 0.23115/158 [====================>.........] - ETA: 7s - loss: 6.2575 - accuracy: 0.23116/158 [=====================>........] - ETA: 7s - loss: 6.2670 - accuracy: 0.23117/158 [=====================>........] - ETA: 7s - loss: 6.2651 - accuracy: 0.23118/158 [=====================>........] - ETA: 7s - loss: 6.2272 - accuracy: 0.23119/158 [=====================>........] - ETA: 7s - loss: 6.2201 - accuracy: 0.23120/158 [=====================>........] - ETA: 6s - loss: 6.2372 - accuracy: 0.23121/158 [=====================>........] - ETA: 6s - loss: 6.2490 - accuracy: 0.23122/158 [======================>.......] - ETA: 6s - loss: 6.2123 - accuracy: 0.23123/158 [======================>.......] - ETA: 6s - loss: 6.2143 - accuracy: 0.23124/158 [======================>.......] - ETA: 6s - loss: 6.1945 - accuracy: 0.23125/158 [======================>.......] - ETA: 6s - loss: 6.2122 - accuracy: 0.23126/158 [======================>.......] - ETA: 5s - loss: 6.1803 - accuracy: 0.23127/158 [=======================>......] - ETA: 5s - loss: 6.1828 - accuracy: 0.23128/158 [=======================>......] - ETA: 5s - loss: 6.1636 - accuracy: 0.23129/158 [=======================>......] - ETA: 5s - loss: 6.1839 - accuracy: 0.23130/158 [=======================>......] - ETA: 5s - loss: 6.1559 - accuracy: 0.23131/158 [=======================>......] - ETA: 4s - loss: 6.1547 - accuracy: 0.23132/158 [========================>.....] - ETA: 4s - loss: 6.1474 - accuracy: 0.23133/158 [========================>.....] - ETA: 4s - loss: 6.1633 - accuracy: 0.23134/158 [========================>.....] - ETA: 4s - loss: 6.1761 - accuracy: 0.23135/158 [========================>.....] - ETA: 4s - loss: 6.1876 - accuracy: 0.23136/158 [========================>.....] - ETA: 4s - loss: 6.1927 - accuracy: 0.23137/158 [=========================>....] - ETA: 3s - loss: 6.2057 - accuracy: 0.23138/158 [=========================>....] - ETA: 3s - loss: 6.1941 - accuracy: 0.23139/158 [=========================>....] - ETA: 3s - loss: 6.1981 - accuracy: 0.24140/158 [=========================>....] - ETA: 3s - loss: 6.2228 - accuracy: 0.23141/158 [=========================>....] - ETA: 3s - loss: 6.2406 - accuracy: 0.23142/158 [=========================>....] - ETA: 2s - loss: 6.2583 - accuracy: 0.23143/158 [==========================>...] - ETA: 2s - loss: 6.2316 - accuracy: 0.24144/158 [==========================>...] - ETA: 2s - loss: 6.2540 - accuracy: 0.23145/158 [==========================>...] - ETA: 2s - loss: 6.2736 - accuracy: 0.23146/158 [==========================>...] - ETA: 2s - loss: 6.2889 - accuracy: 0.23147/158 [==========================>...] - ETA: 2s - loss: 6.3096 - accuracy: 0.23148/158 [===========================>..] - ETA: 1s - loss: 6.3155 - accuracy: 0.23149/158 [===========================>..] - ETA: 1s - loss: 6.3222 - accuracy: 0.23150/158 [===========================>..] - ETA: 1s - loss: 6.3451 - accuracy: 0.23151/158 [===========================>..] - ETA: 1s - loss: 6.3698 - accuracy: 0.23152/158 [===========================>..] - ETA: 1s - loss: 6.3502 - accuracy: 0.23153/158 [============================>.] - ETA: 0s - loss: 6.3575 - accuracy: 0.23154/158 [============================>.] - ETA: 0s - loss: 6.3626 - accuracy: 0.23155/158 [============================>.] - ETA: 0s - loss: 6.3685 - accuracy: 0.23156/158 [============================>.] - ETA: 0s - loss: 6.3509 - accuracy: 0.23157/158 [============================>.] - ETA: 0s - loss: 6.3598 - accuracy: 0.23158/158 [==============================] - ETA: 0s - loss: 6.3533 - accuracy: 0.23158/158 [==============================] - 33s 207ms/step - loss: 6.3533 - accuracy: 0.2352 - val_loss: 7.9167 - val_accuracy: 0.1940\nEpoch 8/50\n  1/158 [..............................] - ETA: 0s - loss: 8.4561 - accuracy: 0.14  2/158 [..............................] - ETA: 13s - loss: 7.6061 - accuracy: 0.252  3/158 [..............................] - ETA: 18s - loss: 7.5465 - accuracy: 0.263  4/158 [..............................] - ETA: 20s - loss: 7.9468 - accuracy: 0.200  5/158 [..............................] - ETA: 21s - loss: 8.1607 - accuracy: 0.183  6/158 [>.............................] - ETA: 22s - loss: 7.1419 - accuracy: 0.154  7/158 [>.............................] - ETA: 23s - loss: 7.3494 - accuracy: 0.155  8/158 [>.............................] - ETA: 23s - loss: 7.1911 - accuracy: 0.187  9/158 [>.............................] - ETA: 23s - loss: 6.7652 - accuracy: 0.166 10/158 [>.............................] - ETA: 23s - loss: 6.9501 - accuracy: 0.168 11/158 [=>............................] - ETA: 23s - loss: 6.9749 - accuracy: 0.165 12/158 [=>............................] - ETA: 23s - loss: 6.5652 - accuracy: 0.154 13/158 [=>............................] - ETA: 23s - loss: 6.3501 - accuracy: 0.196 14/158 [=>............................] - ETA: 23s - loss: 6.2755 - accuracy: 0.215 15/158 [=>............................] - ETA: 23s - loss: 6.1732 - accuracy: 0.238 16/158 [==>...........................] - ETA: 23s - loss: 6.1764 - accuracy: 0.237 17/158 [==>...........................] - ETA: 23s - loss: 6.2836 - accuracy: 0.231 18/158 [==>...........................] - ETA: 23s - loss: 6.4943 - accuracy: 0.220 19/158 [==>...........................] - ETA: 23s - loss: 6.4178 - accuracy: 0.237 20/158 [==>...........................] - ETA: 23s - loss: 6.5211 - accuracy: 0.225 21/158 [==>...........................] - ETA: 23s - loss: 6.3466 - accuracy: 0.253 22/158 [===>..........................] - ETA: 23s - loss: 6.2357 - accuracy: 0.274 23/158 [===>..........................] - ETA: 22s - loss: 6.3322 - accuracy: 0.269 24/158 [===>..........................] - ETA: 22s - loss: 6.2355 - accuracy: 0.285 25/158 [===>..........................] - ETA: 22s - loss: 6.3498 - accuracy: 0.278 26/158 [===>..........................] - ETA: 22s - loss: 6.2976 - accuracy: 0.288 27/158 [====>.........................] - ETA: 22s - loss: 6.2800 - accuracy: 0.284 28/158 [====>.........................] - ETA: 22s - loss: 6.2806 - accuracy: 0.290 29/158 [====>.........................] - ETA: 22s - loss: 6.3888 - accuracy: 0.282 30/158 [====>.........................] - ETA: 22s - loss: 6.2402 - accuracy: 0.273 31/158 [====>.........................] - ETA: 21s - loss: 6.1418 - accuracy: 0.286 32/158 [=====>........................] - ETA: 21s - loss: 6.1473 - accuracy: 0.289 33/158 [=====>........................] - ETA: 21s - loss: 6.2133 - accuracy: 0.286 34/158 [=====>........................] - ETA: 21s - loss: 6.2636 - accuracy: 0.2848"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-74278bcea1c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = feedback_model.fit(train_data, validation_data=val_data, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}