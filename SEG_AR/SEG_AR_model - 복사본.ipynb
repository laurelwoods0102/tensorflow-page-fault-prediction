{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600312040855",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import kerastuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TimeDistributed, \n",
    "    Dense, \n",
    "    Conv1D, \n",
    "    MaxPooling1D, \n",
    "    Bidirectional, \n",
    "    LSTM, \n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"ex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = {\n",
    "    'PAST_HISTORY': 16,\n",
    "    'FUTURE_TARGET': 8,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'BUFFER_SIZE': 200000,\n",
    "    'EPOCHS': 500,\n",
    "    'VOCAB_SIZE': 16293\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"HP_LSTM_1_UNITS\" : 128,\n",
    "    \"HP_LSTM_1_DROPOUT\" : 0.0,\n",
    "    \"HP_LEARNING_RATE\" : 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, n_feature)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
    "        #labels.append(dataset[i:i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/SEG_train_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_train, y_train = generate_timeseries(train_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().batch(static_params[\"BATCH_SIZE\"]).shuffle(static_params[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/SEG_val_set.csv\", delimiter=\"\\n\", dtype=np.int32)\n",
    "x_val, y_val = generate_timeseries(val_set, 0, None, static_params[\"PAST_HISTORY\"], static_params[\"FUTURE_TARGET\"])\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.cache().batch(static_params[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGAutoRegressive(keras.Model):\n",
    "    def __init__(self, units, dropout, output_steps, output_size):\n",
    "        super().__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = keras.layers.LSTMCell(units, dropout=dropout)\n",
    "\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = Dense(output_size, activation=\"softmax\")\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def warmup(self, inputs):\n",
    "        onehot_inputs = tf.squeeze(tf.one_hot(inputs, static_params[\"VOCAB_SIZE\"]), axis=2)\n",
    "\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(onehot_inputs)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "\n",
    "        return prediction, state\n",
    "\n",
    "    @tf.function#(input_signature=[tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32)])\n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        #predictions = []\n",
    "        predictions = tf.TensorArray(tf.float32, size=self.output_steps, clear_after_read=False)\n",
    "        # Initialize the lstm state\n",
    "        prediction, state = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction\n",
    "        #predictions.append(prediction)\n",
    "        predictions = predictions.write(0, prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps\n",
    "        for i in tf.range(self.output_steps - 1):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state, training=training)\n",
    "\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "\n",
    "            # Add the prediction to the output\n",
    "            #predictions.append(prediction)\n",
    "            predictions = predictions.write(i, prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        #predictions = tf.stack(predictions)\n",
    "        predictions = predictions.stack()\n",
    "\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEGAutoRegressive(\n",
    "    units=hparams[\"HP_LSTM_1_UNITS\"], dropout=hparams[\"HP_LSTM_1_DROPOUT\"], \n",
    "    output_steps=static_params[\"FUTURE_TARGET\"], output_size=static_params[\"VOCAB_SIZE\"])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Nadam(hparams[\"HP_LEARNING_RATE\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n         6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n        [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n         6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n        [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n         6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n        ...,\n        [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n         6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n        [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n         6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n         6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n        [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n         6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n        [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n         6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n        ...,\n        [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n         6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n        [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n         6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 281
    }
   ],
   "source": [
    "model.predict(x_train[:2].reshape(2, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[6.1394290e-05, 6.1312676e-05, 6.1343890e-05, ...,\n         6.1373983e-05, 6.1312516e-05, 6.1396429e-05],\n        [6.1401341e-05, 6.1324237e-05, 6.1342740e-05, ...,\n         6.1372775e-05, 6.1332474e-05, 6.1393475e-05],\n        [6.1404193e-05, 6.1334067e-05, 6.1344050e-05, ...,\n         6.1372601e-05, 6.1346393e-05, 6.1390361e-05],\n        ...,\n        [6.1400817e-05, 6.1354905e-05, 6.1353589e-05, ...,\n         6.1373510e-05, 6.1367158e-05, 6.1382583e-05],\n        [6.1398219e-05, 6.1359540e-05, 6.1357016e-05, ...,\n         6.1373816e-05, 6.1370185e-05, 6.1380808e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1394981e-05, 6.1348357e-05, 6.1331943e-05, ...,\n         6.1387371e-05, 6.1324776e-05, 6.1382940e-05],\n        [6.1401130e-05, 6.1351318e-05, 6.1334358e-05, ...,\n         6.1381717e-05, 6.1344195e-05, 6.1382299e-05],\n        [6.1403560e-05, 6.1354571e-05, 6.1338447e-05, ...,\n         6.1378858e-05, 6.1356877e-05, 6.1381259e-05],\n        ...,\n        [6.1400227e-05, 6.1363506e-05, 6.1352795e-05, ...,\n         6.1376493e-05, 6.1373350e-05, 6.1378327e-05],\n        [6.1397775e-05, 6.1365834e-05, 6.1356972e-05, ...,\n         6.1376355e-05, 6.1375191e-05, 6.1377687e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1408762e-05, 6.1347368e-05, 6.1339553e-05, ...,\n         6.1381506e-05, 6.1314597e-05, 6.1377395e-05],\n        [6.1412633e-05, 6.1348997e-05, 6.1339233e-05, ...,\n         6.1377228e-05, 6.1337923e-05, 6.1381288e-05],\n        [6.1413353e-05, 6.1351908e-05, 6.1341401e-05, ...,\n         6.1375285e-05, 6.1353421e-05, 6.1382401e-05],\n        ...,\n        [6.1406572e-05, 6.1361643e-05, 6.1353014e-05, ...,\n         6.1374340e-05, 6.1373990e-05, 6.1380080e-05],\n        [6.1403261e-05, 6.1364357e-05, 6.1356877e-05, ...,\n         6.1374478e-05, 6.1376333e-05, 6.1379047e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       ...,\n\n       [[6.1387917e-05, 6.1347600e-05, 6.1365055e-05, ...,\n         6.1383122e-05, 6.1360610e-05, 6.1396437e-05],\n        [6.1383820e-05, 6.1353749e-05, 6.1366758e-05, ...,\n         6.1380109e-05, 6.1361941e-05, 6.1392740e-05],\n        [6.1380808e-05, 6.1358922e-05, 6.1368170e-05, ...,\n         6.1378094e-05, 6.1362902e-05, 6.1389685e-05],\n        ...,\n        [6.1375984e-05, 6.1369181e-05, 6.1371116e-05, ...,\n         6.1375591e-05, 6.1365470e-05, 6.1383333e-05],\n        [6.1375300e-05, 6.1371196e-05, 6.1371800e-05, ...,\n         6.1375409e-05, 6.1366401e-05, 6.1381957e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1382074e-05, 6.1353174e-05, 6.1392479e-05, ...,\n         6.1396939e-05, 6.1387364e-05, 6.1380277e-05],\n        [6.1380189e-05, 6.1357838e-05, 6.1386556e-05, ...,\n         6.1388666e-05, 6.1382692e-05, 6.1385377e-05],\n        [6.1379062e-05, 6.1361767e-05, 6.1381943e-05, ...,\n         6.1383413e-05, 6.1379527e-05, 6.1387684e-05],\n        ...,\n        [6.1377803e-05, 6.1369865e-05, 6.1374441e-05, ...,\n         6.1376872e-05, 6.1375336e-05, 6.1386789e-05],\n        [6.1377687e-05, 6.1371567e-05, 6.1373408e-05, ...,\n         6.1376188e-05, 6.1374914e-05, 6.1385545e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n\n       [[6.1369676e-05, 6.1341409e-05, 6.1407809e-05, ...,\n         6.1366700e-05, 6.1415405e-05, 6.1407467e-05],\n        [6.1371495e-05, 6.1353014e-05, 6.1397353e-05, ...,\n         6.1368752e-05, 6.1402374e-05, 6.1406543e-05],\n        [6.1372768e-05, 6.1360995e-05, 6.1389379e-05, ...,\n         6.1370796e-05, 6.1393235e-05, 6.1404229e-05],\n        ...,\n        [6.1374783e-05, 6.1373088e-05, 6.1376413e-05, ...,\n         6.1375300e-05, 6.1379644e-05, 6.1394901e-05],\n        [6.1375147e-05, 6.1374936e-05, 6.1374514e-05, ...,\n         6.1376188e-05, 6.1377723e-05, 6.1391998e-05],\n        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n         0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "for x, y in train_data.take(1):\n",
    "    x_sample = x\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "........] - ETA: 1:18 - loss: 7.8367 - accuracy: 0.22 79/315 [======>.......................] - ETA: 1:17 - loss: 7.8457 - accuracy: 0.22 80/315 [======>.......................] - ETA: 1:18 - loss: 7.8519 - accuracy: 0.22 81/315 [======>.......................] - ETA: 1:18 - loss: 7.8593 - accuracy: 0.22 82/315 [======>.......................] - ETA: 1:19 - loss: 7.8014 - accuracy: 0.21 83/315 [======>.......................] - ETA: 1:18 - loss: 7.7999 - accuracy: 0.21 84/315 [=======>......................] - ETA: 1:18 - loss: 7.7981 - accuracy: 0.21 85/315 [=======>......................] - ETA: 1:18 - loss: 7.7973 - accuracy: 0.22 86/315 [=======>......................] - ETA: 1:17 - loss: 7.7412 - accuracy: 0.21 87/315 [=======>......................] - ETA: 1:17 - loss: 7.7176 - accuracy: 0.22 88/315 [=======>......................] - ETA: 1:16 - loss: 7.7200 - accuracy: 0.22 89/315 [=======>......................] - ETA: 1:16 - loss: 7.7281 - accuracy: 0.22 90/315 [=======>......................] - ETA: 1:16 - loss: 7.7281 - accuracy: 0.22 91/315 [=======>......................] - ETA: 1:15 - loss: 7.7397 - accuracy: 0.22 92/315 [=======>......................] - ETA: 1:15 - loss: 7.7263 - accuracy: 0.22 93/315 [=======>......................] - ETA: 1:14 - loss: 7.6910 - accuracy: 0.23 94/315 [=======>......................] - ETA: 1:14 - loss: 7.6896 - accuracy: 0.23 95/315 [========>.....................] - ETA: 1:13 - loss: 7.6856 - accuracy: 0.23 96/315 [========>.....................] - ETA: 1:13 - loss: 7.7134 - accuracy: 0.23 97/315 [========>.....................] - ETA: 1:12 - loss: 7.7290 - accuracy: 0.23 98/315 [========>.....................] - ETA: 1:13 - loss: 7.7087 - accuracy: 0.22 99/315 [========>.....................] - ETA: 1:12 - loss: 7.6832 - accuracy: 0.23100/315 [========>.....................] - ETA: 1:12 - loss: 7.6936 - accuracy: 0.23101/315 [========>.....................] - ETA: 1:12 - loss: 7.6746 - accuracy: 0.23102/315 [========>.....................] - ETA: 1:11 - loss: 7.6532 - accuracy: 0.23103/315 [========>.....................] - ETA: 1:12 - loss: 7.6750 - accuracy: 0.23104/315 [========>.....................] - ETA: 1:11 - loss: 7.6574 - accuracy: 0.23105/315 [=========>....................] - ETA: 1:12 - loss: 7.6519 - accuracy: 0.23106/315 [=========>....................] - ETA: 1:11 - loss: 7.6439 - accuracy: 0.23107/315 [=========>....................] - ETA: 1:12 - loss: 7.6516 - accuracy: 0.23108/315 [=========>....................] - ETA: 1:11 - loss: 7.6499 - accuracy: 0.23109/315 [=========>....................] - ETA: 1:11 - loss: 7.6672 - accuracy: 0.23110/315 [=========>....................] - ETA: 1:10 - loss: 7.6924 - accuracy: 0.22111/315 [=========>....................] - ETA: 1:10 - loss: 7.6981 - accuracy: 0.22112/315 [=========>....................] - ETA: 1:09 - loss: 7.6603 - accuracy: 0.22113/315 [=========>....................] - ETA: 1:10 - loss: 7.6831 - accuracy: 0.22114/315 [=========>....................] - ETA: 1:09 - loss: 7.6917 - accuracy: 0.22115/315 [=========>....................] - ETA: 1:09 - loss: 7.7040 - accuracy: 0.22116/315 [==========>...................] - ETA: 1:08 - loss: 7.6994 - accuracy: 0.22117/315 [==========>...................] - ETA: 1:08 - loss: 7.6769 - accuracy: 0.22118/315 [==========>...................] - ETA: 1:08 - loss: 7.6730 - accuracy: 0.22119/315 [==========>...................] - ETA: 1:08 - loss: 7.6697 - accuracy: 0.22120/315 [==========>...................] - ETA: 1:07 - loss: 7.6717 - accuracy: 0.22121/315 [==========>...................] - ETA: 1:08 - loss: 7.6739 - accuracy: 0.22122/315 [==========>...................] - ETA: 1:07 - loss: 7.6708 - accuracy: 0.23123/315 [==========>...................] - ETA: 1:07 - loss: 7.6779 - accuracy: 0.22124/315 [==========>...................] - ETA: 1:06 - loss: 7.6935 - accuracy: 0.22125/315 [==========>...................] - ETA: 1:06 - loss: 7.6712 - accuracy: 0.22126/315 [===========>..................] - ETA: 1:06 - loss: 7.6872 - accuracy: 0.22127/315 [===========>..................] - ETA: 1:06 - loss: 7.6640 - accuracy: 0.23128/315 [===========>..................] - ETA: 1:06 - loss: 7.6842 - accuracy: 0.22129/315 [===========>..................] - ETA: 1:05 - loss: 7.6510 - accuracy: 0.22130/315 [===========>..................] - ETA: 1:05 - loss: 7.6485 - accuracy: 0.22131/315 [===========>..................] - ETA: 1:04 - loss: 7.6490 - accuracy: 0.22132/315 [===========>..................] - ETA: 1:04 - loss: 7.6170 - accuracy: 0.23133/315 [===========>..................] - ETA: 1:04 - loss: 7.5912 - accuracy: 0.23134/315 [===========>..................] - ETA: 1:04 - loss: 7.5961 - accuracy: 0.23135/315 [===========>..................] - ETA: 1:04 - loss: 7.5748 - accuracy: 0.23136/315 [===========>..................] - ETA: 1:03 - loss: 7.5716 - accuracy: 0.23137/315 [============>.................] - ETA: 1:03 - loss: 7.5677 - accuracy: 0.23138/315 [============>.................] - ETA: 1:03 - loss: 7.5747 - accuracy: 0.23139/315 [============>.................] - ETA: 1:03 - loss: 7.5495 - accuracy: 0.24140/315 [============>.................] - ETA: 1:02 - loss: 7.5473 - accuracy: 0.24141/315 [============>.................] - ETA: 1:02 - loss: 7.5720 - accuracy: 0.23142/315 [============>.................] - ETA: 1:01 - loss: 7.5546 - accuracy: 0.23143/315 [============>.................] - ETA: 1:01 - loss: 7.5263 - accuracy: 0.24144/315 [============>.................] - ETA: 1:00 - loss: 7.5360 - accuracy: 0.24145/315 [============>.................] - ETA: 1:01 - loss: 7.5287 - accuracy: 0.24146/315 [============>.................] - ETA: 1:00 - loss: 7.5371 - accuracy: 0.24147/315 [=============>................] - ETA: 1:00 - loss: 7.5242 - accuracy: 0.24148/315 [=============>................] - ETA: 59s - loss: 7.4978 - accuracy: 0.238149/315 [=============>................] - ETA: 59s - loss: 7.5005 - accuracy: 0.238150/315 [=============>................] - ETA: 58s - loss: 7.5128 - accuracy: 0.237151/315 [=============>................] - ETA: 58s - loss: 7.4973 - accuracy: 0.235152/315 [=============>................] - ETA: 57s - loss: 7.5010 - accuracy: 0.235153/315 [=============>................] - ETA: 58s - loss: 7.4820 - accuracy: 0.233154/315 [=============>................] - ETA: 57s - loss: 7.4869 - accuracy: 0.232155/315 [=============>................] - ETA: 57s - loss: 7.4624 - accuracy: 0.235156/315 [=============>................] - ETA: 57s - loss: 7.4719 - accuracy: 0.235157/315 [=============>................] - ETA: 56s - loss: 7.4750 - accuracy: 0.235158/315 [==============>...............] - ETA: 56s - loss: 7.4823 - accuracy: 0.235159/315 [==============>...............] - ETA: 56s - loss: 7.4906 - accuracy: 0.234160/315 [==============>...............] - ETA: 56s - loss: 7.4653 - accuracy: 0.233161/315 [==============>...............] - ETA: 55s - loss: 7.4742 - accuracy: 0.233162/315 [==============>...............] - ETA: 55s - loss: 7.4717 - accuracy: 0.233163/315 [==============>...............] - ETA: 54s - loss: 7.4709 - accuracy: 0.233164/315 [==============>...............] - ETA: 54s - loss: 7.4731 - accuracy: 0.233165/315 [==============>...............] - ETA: 53s - loss: 7.4676 - accuracy: 0.235166/315 [==============>...............] - ETA: 53s - loss: 7.4540 - accuracy: 0.237167/315 [==============>...............] - ETA: 53s - loss: 7.4336 - accuracy: 0.240168/315 [===============>..............] - ETA: 52s - loss: 7.4499 - accuracy: 0.239169/315 [===============>..............] - ETA: 52s - loss: 7.4601 - accuracy: 0.239170/315 [===============>..............] - ETA: 52s - loss: 7.4351 - accuracy: 0.237171/315 [===============>..............] - ETA: 51s - loss: 7.4283 - accuracy: 0.239172/315 [===============>..............] - ETA: 51s - loss: 7.4389 - accuracy: 0.238173/315 [===============>..............] - ETA: 50s - loss: 7.4183 - accuracy: 0.237174/315 [===============>..............] - ETA: 50s - loss: 7.4218 - accuracy: 0.237175/315 [===============>..............] - ETA: 50s - loss: 7.4254 - accuracy: 0.236176/315 [===============>..............] - ETA: 49s - loss: 7.4402 - accuracy: 0.235177/315 [===============>..............] - ETA: 49s - loss: 7.4150 - accuracy: 0.234178/315 [===============>..............] - ETA: 49s - loss: 7.3969 - accuracy: 0.232179/315 [================>.............] - ETA: 48s - loss: 7.4056 - accuracy: 0.231180/315 [================>.............] - ETA: 48s - loss: 7.3841 - accuracy: 0.230181/315 [================>.............] - ETA: 48s - loss: 7.3908 - accuracy: 0.229182/315 [================>.............] - ETA: 47s - loss: 7.4013 - accuracy: 0.228183/315 [================>.............] - ETA: 47s - loss: 7.4046 - accuracy: 0.227184/315 [================>.............] - ETA: 47s - loss: 7.4158 - accuracy: 0.225185/315 [================>.............] - ETA: 46s - loss: 7.4288 - accuracy: 0.225186/315 [================>.............] - ETA: 46s - loss: 7.4437 - accuracy: 0.223187/315 [================>.............] - ETA: 46s - loss: 7.4380 - accuracy: 0.222188/315 [================>.............] - ETA: 45s - loss: 7.4507 - accuracy: 0.221189/315 [=================>............] - ETA: 45s - loss: 7.4554 - accuracy: 0.220190/315 [=================>............] - ETA: 45s - loss: 7.4704 - accuracy: 0.220191/315 [=================>............] - ETA: 45s - loss: 7.4484 - accuracy: 0.218192/315 [=================>............] - ETA: 45s - loss: 7.4312 - accuracy: 0.218193/315 [=================>............] - ETA: 44s - loss: 7.4141 - accuracy: 0.217194/315 [=================>............] - ETA: 44s - loss: 7.4110 - accuracy: 0.216195/315 [=================>............] - ETA: 43s - loss: 7.3870 - accuracy: 0.215196/315 [=================>............] - ETA: 43s - loss: 7.4007 - accuracy: 0.214197/315 [=================>............] - ETA: 42s - loss: 7.3973 - accuracy: 0.213198/315 [=================>............] - ETA: 42s - loss: 7.4099 - accuracy: 0.212199/315 [=================>............] - ETA: 42s - loss: 7.3925 - accuracy: 0.212200/315 [==================>...........] - ETA: 41s - loss: 7.3908 - accuracy: 0.212201/315 [==================>...........] - ETA: 41s - loss: 7.3917 - accuracy: 0.213202/315 [==================>...........] - ETA: 40s - loss: 7.3949 - accuracy: 0.212203/315 [==================>...........] - ETA: 40s - loss: 7.4044 - accuracy: 0.212204/315 [==================>...........] - ETA: 40s - loss: 7.4143 - accuracy: 0.211205/315 [==================>...........] - ETA: 39s - loss: 7.4216 - accuracy: 0.210206/315 [==================>...........] - ETA: 39s - loss: 7.4302 - accuracy: 0.209207/315 [==================>...........] - ETA: 39s - loss: 7.4262 - accuracy: 0.208208/315 [==================>...........] - ETA: 38s - loss: 7.4296 - accuracy: 0.208209/315 [==================>...........] - ETA: 38s - loss: 7.4437 - accuracy: 0.207210/315 [===================>..........] - ETA: 38s - loss: 7.4490 - accuracy: 0.207211/315 [===================>..........] - ETA: 37s - loss: 7.4320 - accuracy: 0.206212/315 [===================>..........] - ETA: 37s - loss: 7.4261 - accuracy: 0.208213/315 [===================>..........] - ETA: 37s - loss: 7.4264 - accuracy: 0.209214/315 [===================>..........] - ETA: 36s - loss: 7.4116 - accuracy: 0.208215/315 [===================>..........] - ETA: 36s - loss: 7.4053 - accuracy: 0.208216/315 [===================>..........] - ETA: 35s - loss: 7.4182 - accuracy: 0.207217/315 [===================>..........] - ETA: 35s - loss: 7.4230 - accuracy: 0.207218/315 [===================>..........] - ETA: 35s - loss: 7.4137 - accuracy: 0.209219/315 [===================>..........] - ETA: 34s - loss: 7.4098 - accuracy: 0.210220/315 [===================>..........] - ETA: 34s - loss: 7.4205 - accuracy: 0.209221/315 [====================>.........] - ETA: 34s - loss: 7.4317 - accuracy: 0.208222/315 [====================>.........] - ETA: 33s - loss: 7.4310 - accuracy: 0.210223/315 [====================>.........] - ETA: 33s - loss: 7.4217 - accuracy: 0.210224/315 [====================>.........] - ETA: 33s - loss: 7.4223 - accuracy: 0.211225/315 [====================>.........] - ETA: 32s - loss: 7.4108 - accuracy: 0.213226/315 [====================>.........] - ETA: 32s - loss: 7.3946 - accuracy: 0.215227/315 [====================>.........] - ETA: 32s - loss: 7.3986 - accuracy: 0.216228/315 [====================>.........] - ETA: 31s - loss: 7.3799 - accuracy: 0.219229/315 [====================>.........] - ETA: 31s - loss: 7.3592 - accuracy: 0.222230/315 [====================>.........] - ETA: 30s - loss: 7.3491 - accuracy: 0.223231/315 [=====================>........] - ETA: 30s - loss: 7.3550 - accuracy: 0.223232/315 [=====================>........] - ETA: 30s - loss: 7.3374 - accuracy: 0.222233/315 [=====================>........] - ETA: 29s - loss: 7.3387 - accuracy: 0.222234/315 [=====================>........] - ETA: 29s - loss: 7.3366 - accuracy: 0.223235/315 [=====================>........] - ETA: 29s - loss: 7.3159 - accuracy: 0.226236/315 [=====================>........] - ETA: 28s - loss: 7.3128 - accuracy: 0.227237/315 [=====================>........] - ETA: 28s - loss: 7.3200 - accuracy: 0.227238/315 [=====================>........] - ETA: 28s - loss: 7.3237 - accuracy: 0.227239/315 [=====================>........] - ETA: 27s - loss: 7.3064 - accuracy: 0.226240/315 [=====================>........] - ETA: 27s - loss: 7.2883 - accuracy: 0.225241/315 [=====================>........] - ETA: 26s - loss: 7.2748 - accuracy: 0.227242/315 [======================>.......] - ETA: 26s - loss: 7.2809 - accuracy: 0.227243/315 [======================>.......] - ETA: 26s - loss: 7.2889 - accuracy: 0.226244/315 [======================>.......] - ETA: 25s - loss: 7.2875 - accuracy: 0.226245/315 [======================>.......] - ETA: 25s - loss: 7.2705 - accuracy: 0.225246/315 [======================>.......] - ETA: 25s - loss: 7.2648 - accuracy: 0.226247/315 [======================>.......] - ETA: 24s - loss: 7.2460 - accuracy: 0.225248/315 [======================>.......] - ETA: 24s - loss: 7.2415 - accuracy: 0.226249/315 [======================>.......] - ETA: 24s - loss: 7.2267 - accuracy: 0.226250/315 [======================>.......] - ETA: 23s - loss: 7.2262 - accuracy: 0.226251/315 [======================>.......] - ETA: 23s - loss: 7.2080 - accuracy: 0.228252/315 [=======================>......] - ETA: 22s - loss: 7.2191 - accuracy: 0.227253/315 [=======================>......] - ETA: 22s - loss: 7.2303 - accuracy: 0.226254/315 [=======================>......] - ETA: 22s - loss: 7.2200 - accuracy: 0.227255/315 [=======================>......] - ETA: 21s - loss: 7.2135 - accuracy: 0.227256/315 [=======================>......] - ETA: 21s - loss: 7.2117 - accuracy: 0.226257/315 [=======================>......] - ETA: 21s - loss: 7.2187 - accuracy: 0.226258/315 [=======================>......] - ETA: 20s - loss: 7.2046 - accuracy: 0.225259/315 [=======================>......] - ETA: 20s - loss: 7.2161 - accuracy: 0.224260/315 [=======================>......] - ETA: 20s - loss: 7.2213 - accuracy: 0.224261/315 [=======================>......] - ETA: 19s - loss: 7.2274 - accuracy: 0.223262/315 [=======================>......] - ETA: 19s - loss: 7.2351 - accuracy: 0.222263/315 [========================>.....] - ETA: 19s - loss: 7.2362 - accuracy: 0.223264/315 [========================>.....] - ETA: 18s - loss: 7.2428 - accuracy: 0.222265/315 [========================>.....] - ETA: 18s - loss: 7.2470 - accuracy: 0.222266/315 [========================>.....] - ETA: 18s - loss: 7.2512 - accuracy: 0.222267/315 [========================>.....] - ETA: 17s - loss: 7.2367 - accuracy: 0.221268/315 [========================>.....] - ETA: 17s - loss: 7.2254 - accuracy: 0.220269/315 [========================>.....] - ETA: 16s - loss: 7.2228 - accuracy: 0.220270/315 [========================>.....] - ETA: 16s - loss: 7.2060 - accuracy: 0.222271/315 [========================>.....] - ETA: 16s - loss: 7.2051 - accuracy: 0.221272/315 [========================>.....] - ETA: 15s - loss: 7.1908 - accuracy: 0.221273/315 [=========================>....] - ETA: 15s - loss: 7.1734 - accuracy: 0.224274/315 [=========================>....] - ETA: 15s - loss: 7.1785 - accuracy: 0.223275/315 [=========================>....] - ETA: 14s - loss: 7.1834 - accuracy: 0.222276/315 [=========================>....] - ETA: 14s - loss: 7.1693 - accuracy: 0.224277/315 [=========================>....] - ETA: 13s - loss: 7.1791 - accuracy: 0.223278/315 [=========================>....] - ETA: 13s - loss: 7.1874 - accuracy: 0.223279/315 [=========================>....] - ETA: 13s - loss: 7.1893 - accuracy: 0.222280/315 [=========================>....] - ETA: 12s - loss: 7.1923 - accuracy: 0.221281/315 [=========================>....] - ETA: 12s - loss: 7.1755 - accuracy: 0.224282/315 [=========================>....] - ETA: 12s - loss: 7.1789 - accuracy: 0.223283/315 [=========================>....] - ETA: 11s - loss: 7.1780 - accuracy: 0.223284/315 [==========================>...] - ETA: 11s - loss: 7.1860 - accuracy: 0.222285/315 [==========================>...] - ETA: 11s - loss: 7.1884 - accuracy: 0.221286/315 [==========================>...] - ETA: 10s - loss: 7.1806 - accuracy: 0.221287/315 [==========================>...] - ETA: 10s - loss: 7.1644 - accuracy: 0.223288/315 [==========================>...] - ETA: 10s - loss: 7.1596 - accuracy: 0.223289/315 [==========================>...] - ETA: 9s - loss: 7.1455 - accuracy: 0.22290/315 [==========================>...] - ETA: 9s - loss: 7.1521 - accuracy: 0.22291/315 [==========================>...] - ETA: 8s - loss: 7.1361 - accuracy: 0.22292/315 [==========================>...] - ETA: 8s - loss: 7.1478 - accuracy: 0.22293/315 [==========================>...] - ETA: 8s - loss: 7.1384 - accuracy: 0.22294/315 [===========================>..] - ETA: 7s - loss: 7.1319 - accuracy: 0.22295/315 [===========================>..] - ETA: 7s - loss: 7.1366 - accuracy: 0.22296/315 [===========================>..] - ETA: 7s - loss: 7.1388 - accuracy: 0.22297/315 [===========================>..] - ETA: 6s - loss: 7.1406 - accuracy: 0.22298/315 [===========================>..] - ETA: 6s - loss: 7.1471 - accuracy: 0.22299/315 [===========================>..] - ETA: 5s - loss: 7.1466 - accuracy: 0.22300/315 [===========================>..] - ETA: 5s - loss: 7.1510 - accuracy: 0.22301/315 [===========================>..] - ETA: 5s - loss: 7.1433 - accuracy: 0.22302/315 [===========================>..] - ETA: 4s - loss: 7.1428 - accuracy: 0.22303/315 [===========================>..] - ETA: 4s - loss: 7.1416 - accuracy: 0.22304/315 [===========================>..] - ETA: 4s - loss: 7.1280 - accuracy: 0.22305/315 [============================>.] - ETA: 3s - loss: 7.1387 - accuracy: 0.22306/315 [============================>.] - ETA: 3s - loss: 7.1450 - accuracy: 0.22307/315 [============================>.] - ETA: 2s - loss: 7.1495 - accuracy: 0.22308/315 [============================>.] - ETA: 2s - loss: 7.1403 - accuracy: 0.22309/315 [============================>.] - ETA: 2s - loss: 7.1376 - accuracy: 0.22310/315 [============================>.] - ETA: 1s - loss: 7.1462 - accuracy: 0.22311/315 [============================>.] - ETA: 1s - loss: 7.1511 - accuracy: 0.22312/315 [============================>.] - ETA: 1s - loss: 7.1589 - accuracy: 0.22313/315 [============================>.] - ETA: 0s - loss: 7.1616 - accuracy: 0.22314/315 [============================>.] - ETA: 0s - loss: 7.1652 - accuracy: 0.22315/315 [==============================] - ETA: 0s - loss: 7.1532 - accuracy: 0.21315/315 [==============================] - 117s 372ms/step - loss: 7.1532 - accuracy: 0.2193\n"
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"seg_auto_regressive_35\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nlstm_cell_34 (LSTMCell)      multiple                  8408064\n_________________________________________________________________\nrnn_34 (RNN)                 multiple                  8408064\n_________________________________________________________________\ndense_34 (Dense)             multiple                  2101797\n=================================================================\nTotal params: 10,509,861\nTrainable params: 10,509,861\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: ex/model_4\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"ex/model_4\", \n",
    "    signatures=model.call.get_concrete_function(tf.TensorSpec(shape=[None, None, 1], dtype=tf.int32, name=\"input\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load(\"ex/model_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ItemsView(_SignatureMap({'serving_default': <tensorflow.python.saved_model.load._WrapperFunction object at 0x00000281F5C235C8>}))"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "new_model.signatures.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'output_0': <tf.Tensor: shape=(1, 8, 16293), dtype=float32, numpy=\n array([[[6.1407009e-05, 6.1159910e-05, 6.1471401e-05, ...,\n          6.1185157e-05, 6.1460407e-05, 6.1534498e-05],\n         [6.1415267e-05, 6.1199942e-05, 6.1451465e-05, ...,\n          6.1217317e-05, 6.1451734e-05, 6.1515988e-05],\n         [6.1419305e-05, 6.1232531e-05, 6.1435072e-05, ...,\n          6.1244253e-05, 6.1442028e-05, 6.1497434e-05],\n         ...,\n         [6.1416678e-05, 6.1299492e-05, 6.1402381e-05, ...,\n          6.1302126e-05, 6.1414576e-05, 6.1448438e-05],\n         [6.1413448e-05, 6.1314473e-05, 6.1395571e-05, ...,\n          6.1315688e-05, 6.1407140e-05, 6.1435487e-05],\n         [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,\n          0.0000000e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)>}"
     },
     "metadata": {},
     "execution_count": 294
    }
   ],
   "source": [
    "inference = new_model.signatures[\"serving_default\"]\n",
    "inference(tf.constant(x_train[0].reshape(1, -1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: version/20200916-205849\\assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Looks like there is an object (perhaps variable or layer) that is shared between different layers/models. This may cause issues when restoring the variable values.Object: <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61BDFC8>\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61B91C8> and <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x0000018AB61BDFC8>).\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.00730717, -0.0198552 ,  0.06291562, ..., -0.00453906,\n        -0.02963837,  0.05556399],\n       [-0.08210181,  0.01879025,  0.02236442, ...,  0.07328226,\n         0.03787373,  0.01388676],\n       [-0.00642601, -0.05339912, -0.00547218, ...,  0.05781174,\n        -0.05629285,  0.02388919],\n       ...,\n       [-0.03962746, -0.01832095,  0.04701049, ...,  0.07243678,\n         0.01312423,  0.01554349],\n       [-0.01769733, -0.03605518, -0.0195809 , ..., -0.0357281 ,\n        -0.01636354,  0.05007046],\n       [ 0.03688146,  0.06794242, -0.06881212, ...,  0.04910905,\n        -0.02574294,  0.01410268]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00083591,  0.0063998 ,  0.00388604, ..., -0.00505481,\n        -0.0074309 , -0.0105841 ],\n       [ 0.00651316, -0.01728297, -0.00512317, ...,  0.00266844,\n         0.00772635, -0.0064606 ],\n       [ 0.01302376, -0.01636562,  0.00464472, ...,  0.00846572,\n         0.00637446, -0.01419988],\n       ...,\n       [ 0.00821935, -0.00052315,  0.01726941, ..., -0.00361796,\n        -0.01594081, -0.00454655],\n       [-0.00550627, -0.0138326 , -0.00877023, ..., -0.0005246 ,\n        -0.00196624, -0.00357318],\n       [-0.0163309 ,  0.0125559 ,  0.00641901, ..., -0.01629131,\n         0.00078054, -0.00967654]], dtype=float32)>]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e7f950a97013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m   raise IOError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;31m# TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[0;32m    602\u001b[0m       loader = loader_cls(object_graph_proto,\n\u001b[0;32m    603\u001b[0m                           \u001b[0msaved_model_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m                           export_dir)\n\u001b[0m\u001b[0;32m    605\u001b[0m       \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m       \u001b[0mload_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_existing_objects_matched\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m           (\"Some Python objects were not bound to checkpointed values, likely \"\n\u001b[0;32m    782\u001b[0m            \"due to changes in the Python program: %s\") %\n\u001b[1;32m--> 783\u001b[1;33m           (list(unused_python_objects),))\n\u001b[0m\u001b[0;32m    784\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'lstm_cell_1/recurrent_kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.00730717, -0.0198552 ,  0.06291562, ..., -0.00453906,\n        -0.02963837,  0.05556399],\n       [-0.08210181,  0.01879025,  0.02236442, ...,  0.07328226,\n         0.03787373,  0.01388676],\n       [-0.00642601, -0.05339912, -0.00547218, ...,  0.05781174,\n        -0.05629285,  0.02388919],\n       ...,\n       [-0.03962746, -0.01832095,  0.04701049, ...,  0.07243678,\n         0.01312423,  0.01554349],\n       [-0.01769733, -0.03605518, -0.0195809 , ..., -0.0357281 ,\n        -0.01636354,  0.05007046],\n       [ 0.03688146,  0.06794242, -0.06881212, ...,  0.04910905,\n        -0.02574294,  0.01410268]], dtype=float32)>, <tf.Variable 'lstm_cell_1/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'lstm_cell_1/kernel:0' shape=(16293, 512) dtype=float32, numpy=\narray([[-0.00083591,  0.0063998 ,  0.00388604, ..., -0.00505481,\n        -0.0074309 , -0.0105841 ],\n       [ 0.00651316, -0.01728297, -0.00512317, ...,  0.00266844,\n         0.00772635, -0.0064606 ],\n       [ 0.01302376, -0.01636562,  0.00464472, ...,  0.00846572,\n         0.00637446, -0.01419988],\n       ...,\n       [ 0.00821935, -0.00052315,  0.01726941, ..., -0.00361796,\n        -0.01594081, -0.00454655],\n       [-0.00550627, -0.0138326 , -0.00877023, ..., -0.0005246 ,\n        -0.00196624, -0.00357318],\n       [-0.0163309 ,  0.0125559 ,  0.00641901, ..., -0.01629131,\n         0.00078054, -0.00967654]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.load_model(version_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(version_dir + \"/weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c3b0db8dd40e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "model_1.load_weights(version_dir + \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}