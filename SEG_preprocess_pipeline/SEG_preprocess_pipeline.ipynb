{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dill     # 0.3.2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global/Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SEG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset/Static Param List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 3196231680, 93292771632, 93293300344, ..., 92658792872,\n",
       "       92658792864, 92654987192], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "original_dataset = np.genfromtxt(\"../로그 데이터/SEG_SGEMM_result.txt\", delimiter=\"\\n\", dtype=np.int64)\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateDelta(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([X[i] - X[i+1] for i in range(int(len(X))-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTokenizer(TransformerMixin):\n",
    "    def __init__(self, minimum_category_occurence=2, oov_token=-1):\n",
    "        self.minimum_category_occurence = minimum_category_occurence\n",
    "        self.oov_token = oov_token\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        mask = (pd.Series(X).value_counts() <= self.minimum_category_occurence)\n",
    "        noise_index = np.where(np.isin(X, mask.index[mask == True]))[0]\n",
    "        X[noise_index] = self.oov_token\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, oov_token=-1):\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_counts = pd.Series(X).value_counts()\n",
    "        self.vocab_size = len(X_counts)\n",
    "        self.word_index = X_counts.index\n",
    "        self.vocabulary = {X_counts.index[i]:i for i in range(self.vocab_size)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] in self.word_index:\n",
    "                X_transformed.append(self.vocabulary[X[i]])\n",
    "            else:\n",
    "                X_transformed.append(self.vocabulary[self.oov_token])\n",
    "\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        return np.array([self.word_index[X[i]] for i in range(len(X))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Train/Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val / Test Ratio : 70% / 15% / 15%\n",
    "train_val_set, test_set = train_test_split(original_dataset, test_size=0.15, shuffle=False)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_train_pipeline = Pipeline([\n",
    "    ('calculate_delta', CalculateDelta()),\n",
    "    ('noise_tokenizer', NoiseTokenizer()),\n",
    "    ('sparse_category_encoder', SparseCategoryEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 897, 242, 961])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "processed_train_val_set = SEG_train_pipeline.fit_transform(train_val_set.copy())\n",
    "processed_train_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set, processed_val_set = train_test_split(processed_train_val_set, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Datasets/Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/{}_train_set_original.csv\".format(model_name), train_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "np.savetxt(\"data/{}_test_set_original.csv\".format(model_name), test_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "\n",
    "np.savetxt(\"data/{}_train_set.csv\".format(model_name), processed_train_set, fmt=\"%d\", delimiter=\"\\n\")\n",
    "np.savetxt(\"data/{}_val_set.csv\".format(model_name), processed_val_set, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = np.r_[train_set[-1], val_set]  # As one data point is lost during CalculateDelta process\n",
    "np.savetxt(\"data/{}_val_set_original.csv\".format(model_name), validation_set, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_test_pipeline = Pipeline([\n",
    "    ('calculate_delta', CalculateDelta()),\n",
    "    ('sparse_category_encoder', SEG_train_pipeline[\"sparse_category_encoder\"])\n",
    "])"
   ]
  },
  {
   "source": [
    "with open(\"static/test_pipeline.pkl\", 'wb') as f:\n",
    "    dill.dump(SEG_test_pipeline, f)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "np.savetxt(\"data/word_index.csv\", np.array(list(SEG_train_pipeline[\"sparse_category_encoder\"].vocabulary.keys())), fmt=\"%d\", delimiter=\"\\n\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_set = SEG_train_pipeline.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/{}_test_set.csv\".format(model_name), processed_test_set, fmt=\"%d\", delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{-1: 0,\n",
       " 0: 1,\n",
       " -4096: 2,\n",
       " -909517620: 3,\n",
       " 909517620: 4,\n",
       " -8192: 5,\n",
       " 8: 6,\n",
       " 4096: 7,\n",
       " -8: 8,\n",
       " -12288: 9,\n",
       " -2416: 10,\n",
       " -16384: 11,\n",
       " -24: 12,\n",
       " -3520: 13,\n",
       " 12: 14,\n",
       " 2744: 15,\n",
       " -6: 16,\n",
       " -64: 17,\n",
       " -32: 18,\n",
       " -20480: 19}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dict(list(SEG_train_pipeline[\"sparse_category_encoder\"].vocabulary.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16293"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "SEG_train_pipeline[\"sparse_category_encoder\"].vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}