{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from mixtures import discretized_mix_logistic_loss, sample_from_discretized_mix_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGWaveNet(tf.keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, residual_channels, dilation_channels, skip_channels, out_channels, quantization_channels=2**8, use_biases=False, initial_filter_width=32):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.residual_channels = residual_channels\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "\n",
    "        self.receptive_field = SEGWaveNet.calculate_receptive_field(self.filter_width, self.dilations, self.initial_filter_width)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_receptive_field(filter_width, dilations, initial_filter_width):\n",
    "        receptive_field = (filter_width - 1) * sum(dilations) + 1\n",
    "\n",
    "        # scalar_input\n",
    "        receptive_field += initial_filter_width - 1\n",
    "        return receptive_field\n",
    "\n",
    "    def _create_causal_layer(self, input_batch):\n",
    "        with tf.name_scope(\"causal_layer\"):\n",
    "            return keras.layers.Conv1D(\n",
    "                input_batch,\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=self.initial_filter_width,\n",
    "                padding='valid',\n",
    "                dilation_rate=1,\n",
    "                use_biase=False\n",
    "            )\n",
    "\n",
    "    def _create_queue(self):\n",
    "        pass\n",
    "\n",
    "    def _create_dilation_layer(self, input_batch, layer_index, dilation, output_width):\n",
    "        with tf.name_scope(\"dilation_layer_{}\".format(layer_index)):       #!\n",
    "            conv_filter = keras.layers.Conv1D(\n",
    "                input_batch,\n",
    "                filters=self.dilation_channels,\n",
    "                kernel_size=self.filter_width,\n",
    "                dilation_rate=dilation,\n",
    "                padding='valid',\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"conv_filter\"\n",
    "            )\n",
    "            conv_gate = keras.layers.Conv1D(\n",
    "                input_batch,\n",
    "                filters=self.dilation_channels,\n",
    "                kernel_size=self.filter_width,\n",
    "                dilation_rate=dilation,\n",
    "                padding='valid',\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"conv_gate\"\n",
    "            )\n",
    "\n",
    "            out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\n",
    "            \n",
    "            ## skip_contribution : Summed up to create output\n",
    "            skip_cut = tf.shape(out)[1] - output_width\n",
    "            out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "            skip_contribution = keras.layers.Conv1D(\n",
    "                out_skip,\n",
    "                filters=self.skip_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"skip\"\n",
    "            )\n",
    "\n",
    "            ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "            transformed = keras.layers.Conv1D(\n",
    "                out,\n",
    "                filters=self.residual_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                use_bias=self.use_biases,\n",
    "                name=\"dense\"\n",
    "            )\n",
    "\n",
    "            input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\n",
    "            input_batch_cut = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\n",
    "            dense_output = input_batch_cut + transformed\n",
    "\n",
    "            return skip_contribution, dense_output\n",
    "\n",
    "    def _create_network(self, input_batch):\n",
    "        if self.train_mode == False:\n",
    "            self._create_queue()\n",
    "\n",
    "        outputs = []\n",
    "        current_layer = input_batch     # Length is reduced by 1 due to causal cut\n",
    "\n",
    "        if self.train_mode == False:\n",
    "            self.causal_queue = tf.tensor_scatter_nd_update(\n",
    "                self.causal_queue,\n",
    "                tf.range(self.batch_size),\n",
    "                tf.concat([self.causal_queue[:, 1:, :], input_batch], axis=1)\n",
    "                )\n",
    "            current_layer = self.causal_queue\n",
    "\n",
    "        current_layer = self._create_causal_layer(current_layer)\n",
    "\n",
    "        if self.train_mode == True:\n",
    "            output_width = tf.shape(input_batch)[1] - self.receptive_field + 1\n",
    "        else:\n",
    "            output_width = 1\n",
    "\n",
    "        with tf.name_scope(\"dilated_stack\"):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope(\"layer_{}\".format(layer_index)):\n",
    "                    if self.train_mode == False:\n",
    "                        self.dilation_queue[layer_index] = tf.tensor_scatter_nd_update(\n",
    "                            self.dilation_queue[layer_index],\n",
    "                            tf.range(self.batch_size),\n",
    "                            tf.concat([self.dilation_queue[layer_index][:, 1:, :], current_layer], axis=1)\n",
    "                            )\n",
    "                        current_layer = self.dilation_queue[layer_index]\n",
    "\n",
    "                        output, current_layer = self._create_dilation_layer(current_layer, layer_index, dilation, output_width)\n",
    "                        outputs.append(output)\n",
    "\n",
    "                with tf.name_scope(\"postprocessing\"):\n",
    "                    total = sum(outputs)\n",
    "                    transformed_1 = tf.nn.relu(total)\n",
    "                    conv_1 = keras.layers.Conv1D(\n",
    "                        transformed_1,\n",
    "                        filters=self.skip_channels,\n",
    "                        kernel_size=1,\n",
    "                        padding=\"same\",\n",
    "                        use_bias=self.use_biases\n",
    "                    )\n",
    "\n",
    "                    transformed_2 = tf.nn.relu(conv_1)\n",
    "                    conv_2 = keras.layers.Conv1D(\n",
    "                        transformed_2,\n",
    "                        filters=self.out_channels,\n",
    "                        kernel_size=1,\n",
    "                        padding=\"same\",\n",
    "                        use_bias=self.use_biases\n",
    "                    )\n",
    "\n",
    "                return conv_2\n",
    "\n",
    "    def _one_hot(self, input_batch):\n",
    "        with tf.name_scope(\"one_hot_encode\"):\n",
    "            encoded = tf.one_hot(\n",
    "                input_batch,\n",
    "                depth=self.quantization_channels,\n",
    "                dtype=tf.float32\n",
    "                )\n",
    "            shape = [self.batch_size, -1, self.quantization_channels]\n",
    "            encoded = tf.reshape(encoded, shape)\n",
    "        return encoded\n",
    "\n",
    "    def predict_proba_incremental(self, x, name=\"wavenet\"):\n",
    "        with tf.name_scope(name):\n",
    "            encoded = tf.reshape(x, [self.batch_size, -1, 1])\n",
    "\n",
    "            raw_output = self._create_network(encoded)\n",
    "\n",
    "            out = tf.reshape(raw_output, [self.batch_size, -1, self.out_channels])\n",
    "            proba = sample_from_discretized_mix_logistic(out)\n",
    "\n",
    "            return proba\n",
    "\n",
    "    def add_loss(self, input_batch, l2_regularization_strength=None, name=\"wavenet\"):\n",
    "        with tf.name_scope(name):\n",
    "            encoded = self._one_hot(input_batch)\n",
    "            encoded = tf.cast(encoded, tf.float32)\n",
    "\n",
    "            network_input = tf.reshape(encoded, [self.batch_size, -1, 1])\n",
    "\n",
    "            network_input_width = tf.shape(network_input)[1] - 1\n",
    "\n",
    "            input = tf.slice(network_input, [0, 0, 0], [-1, network_input_width, 1])\n",
    "\n",
    "            raw_output = self._create_network(input)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                target_output = tf.slice(network_input, [0, self.receptive_field, 0], [-1, -1, -1])\n",
    "\n",
    "                loss = discretized_mix_logistic_loss(raw_output, target_output, num_class=2**16, reduce=False)      # num_class : 16 bits or 64bits ?\n",
    "                reduced_loss = tf.math.reduce_mean(loss)\n",
    "\n",
    "                tf.summary.scalar('loss', reduced_loss)\n",
    "\n",
    "                if l2_regularization_strength == None:\n",
    "                    self.loss = reduced_loss\n",
    "                else:\n",
    "                    l2_loss = tf.math.add_n([tf.nn.l2_loss(v) for v in SEGWaveNet.trainable_variables if not ('bias' in v.name)])   #\n",
    "\n",
    "                    total_loss = (reduced_loss + l2_regularization_strength * l2_loss)\n",
    "\n",
    "                    tf.summary.scalar('l2_loss', l2_loss)\n",
    "                    tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "                    self.loss = total_loss\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 3), dtype=int32, numpy=array([[[2, 2, 2]]])>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "tf.slice(t, [0, 1, 0], [1, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_layer = keras.layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=2,\n",
    "            padding='valid',\n",
    "            dilation_rate=1,\n",
    "            use_bias=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv1D at 0x1ec6e7bc7c8>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "causal_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = causal_layer(tf.ones([1, 2, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_queue = tf.Variable(initial_value=tf.zeros(shape=[1, 2, 32], dtype=tf.float32), name='causal_queue', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Inner dimensions of output shape must match inner dimensions of updates shape. Output: [1,2,32] updates: [1,2,32] [Op:TensorScatterUpdate]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-37b66b1f1c63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                 \u001b[0mcausal_queue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcausal_queue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                 )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mtensor_scatter_update\u001b[1;34m(tensor, indices, updates, name)\u001b[0m\n\u001b[0;32m  11153\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11154\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11155\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11156\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11157\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [1,2,32] updates: [1,2,32] [Op:TensorScatterUpdate]"
     ]
    }
   ],
   "source": [
    "causal_queue = tf.tensor_scatter_nd_update(\n",
    "                causal_queue,\n",
    "                tf.range(1),\n",
    "                tf.concat([causal_queue[:, 1:, :], x], axis=1)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        #print(self.kernel)\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones([1, 16, 2])\n",
    "x = tf.cast(x, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.Conv1D at 0x1ec6ff75348>"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "c = Conv1D(filters=3, kernel_size=4)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 13, 3), dtype=float32, numpy=\n",
       "array([[[-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ],\n",
       "        [-1.3553994 ,  0.28732443, -0.692432  ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "c(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'conv1d_34/kernel:0' shape=(4, 2, 3) dtype=float32, numpy=\narray([[[-0.19909331,  0.24411184,  0.07526147],\n        [-0.05335271, -0.06397757,  0.327456  ]],\n\n       [[-0.23370636,  0.33305246, -0.5221818 ],\n        [-0.45373005,  0.17659378, -0.45146018]],\n\n       [[-0.01570284, -0.36203146, -0.5384827 ],\n        [ 0.141541  ,  0.26893032,  0.06154954]],\n\n       [[-0.2169109 ,  0.19140851, -0.00373352],\n        [-0.32444423, -0.5007634 ,  0.3591593 ]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}