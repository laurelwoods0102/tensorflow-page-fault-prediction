{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('ProgramData': virtualenv)",
   "display_name": "Python 3.7.6 64-bit ('ProgramData': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixtures import discretized_mix_logistic_loss, sample_from_discretized_mix_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])\n",
    "\n",
    "    #def init_queue(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "            \n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, quantization_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        '''\n",
    "        # For Scalar output\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "        '''\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.quantization_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels=None, out_channels=None, use_biases=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "        # Scalar Input receptive field\n",
    "        self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "\n",
    "    def build(self, input_shape):     \n",
    "        self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model\n",
    "\n",
    "        self.preprocessing_layer = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=self.initial_filter_width,\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for _ in range(1):\n",
    "            for i, dilation in enumerate(self.dilations):\n",
    "                self.residual_blocks.append(\n",
    "                    ResidualBlock(\n",
    "                        layer_index=i,\n",
    "                        dilation=self.dilations[0], \n",
    "                        filter_width=self.filter_width, \n",
    "                        dilation_channels=self.dilation_channels, \n",
    "                        residual_channels=self.residual_channels, \n",
    "                        skip_channels=self.skip_channels, \n",
    "                        use_biases=self.use_biases, \n",
    "                        output_width=self.output_width)\n",
    "                    )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.quantization_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        '''\n",
    "        == predict_proba_incremental\n",
    "\n",
    "        Assume that x is integer (== scalar_input = True)\n",
    "        '''\n",
    "\n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "\n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "      \n",
    "        skip_sum = tf.math.add_n(skip_outputs)          \n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "\n",
    "        if not training:\n",
    "            out = tf.reshape(output, [self.batch_size, -1, self.quantization_channels])\n",
    "            #output = sample_from_discretized_mix_logistic(out)\n",
    "            output = tf.cast(tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            #loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y)\n",
    "            reduced_loss = tf.math.reduce_mean(loss)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedMixLogisticLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"discretized_mix_logistic_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss =  discretized_mix_logistic_loss(y_pred, y_true)\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "source": [
    "inputs = tf.random.uniform([1, 1025, 1], minval=0, maxval=255, dtype=tf.int32)\n",
    "inputs = tf.cast(inputs, tf.float32)\n",
    "inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1025, 1), dtype=float32, numpy=\n",
       "array([[[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        ...,\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "rg = tf.range(255)\n",
    "inputs = tf.concat([rg, rg, rg, rg, rg], axis=0)[:1025]\n",
    "inputs = tf.cast(tf.reshape(inputs, [1, 1025, 1]), tf.float32)\n",
    "#inputs = tf.reshape(inputs, [1, 1025, 1])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = inputs[:, :1024, :]\n",
    "x_train = inputs[:, :287, :]\n",
    "#y_train = inputs[:, 287:, :]\n",
    "y_train = inputs[:, 287:288, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([1, 287, 1]), TensorShape([1, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[32]]])>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 256), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "y_train = tf.cast(y_train, tf.int32)\n",
    "y_train_onehot = tf.one_hot(y_train, depth=2**8)\n",
    "y_train_onehot = tf.squeeze(y_train_onehot, axis=2)\n",
    "y_train_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParms follows the Diagram \n",
    "batch_size = 1\n",
    "dilations = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "filter_width = 2        # == kernel_size\n",
    "initial_filter_width = 32       # from (tacokr)\n",
    "dilation_channels = 32  # unknown\n",
    "residual_channels = 24\n",
    "skip_channels = 128\n",
    "quantization_channels = 2**8\n",
    "out_channels = 10*3\n",
    "use_biases = False\n",
    "\n",
    "#wavenet.receptive_field = 287\n",
    "#wavenet.output_width = 738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels, out_channels, use_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 992us/step - loss: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 991us/step - loss: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0000e+00\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0000e+00\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0000e+00\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b222420248>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "#wavenet.fit(x=x, y=x)\n",
    "wavenet.fit(x_train, y_train_onehot, batch_size=8, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 256), dtype=float32, numpy=\n",
       "array([[[ -5.786551 ,  -2.552747 ,  -3.067538 ,  -5.2054114,\n",
       "          -1.7160194,  -3.3329356,  -3.630709 ,  -8.918618 ,\n",
       "          -5.8658834,  -4.398642 ,  -3.149269 ,  -3.085446 ,\n",
       "          -4.090922 ,  -3.0003588,  -2.8042355,  -4.095027 ,\n",
       "          -5.67526  ,  -6.582371 ,  -2.8807018,  -2.9774275,\n",
       "          -2.5274134,  -2.8233483,  -2.9248116,  -1.6796691,\n",
       "          -5.1662602,  -3.086809 ,  -5.8180456,  -2.127972 ,\n",
       "          -4.9839396,  -2.3052182,  -3.6668222,  -2.8676462,\n",
       "          19.67266  ,  -2.882491 ,  -5.4081235,  -2.665713 ,\n",
       "          -4.8926096,  -3.022813 ,  -2.7793763,  -2.4388816,\n",
       "          -5.096632 ,  -8.119506 ,  -2.2289367,  -2.7809553,\n",
       "          -4.3246856,  -5.8219786,  -4.32346  , -12.484178 ,\n",
       "          -3.6884115,  -4.4123983,  -5.8725643,  -5.1686306,\n",
       "          -2.508771 ,  -5.0169435,  -3.159831 ,  -3.145667 ,\n",
       "          -3.8122768,  -5.2268686,  -3.1002738,  -3.508804 ,\n",
       "          -2.8082619,  -2.4882348,  -1.6771036,  -3.6980498,\n",
       "          -3.216933 ,  -2.8819537,  -1.2318171,  -5.182772 ,\n",
       "          -3.908192 ,  -6.270946 ,  -3.832935 ,  -3.3567934,\n",
       "          -3.6351185,  -2.5186126,  -5.600614 ,  -2.540246 ,\n",
       "          -2.6473083,  -4.894013 ,  -3.0581803,  -3.4820247,\n",
       "          -5.643589 ,  -3.9706657,  -3.027623 ,  -7.890685 ,\n",
       "          -2.9071531,  -3.9350038,  -4.492982 ,  -1.9387257,\n",
       "          -3.2824905,  -3.4984965,  -5.1376824,  -4.914792 ,\n",
       "          -4.281956 ,  -5.1860995,  -4.280681 ,  -2.8517573,\n",
       "          -2.0050702,  -3.7495334,  -2.4598274,  -3.656821 ,\n",
       "          -7.5637302,  -7.7082534,  -3.8383582,  -4.733212 ,\n",
       "          -4.4765344,  -5.224374 ,  -2.616003 ,  -2.8154602,\n",
       "          -7.872872 ,  -5.0838246,  -3.4166179,  -2.877619 ,\n",
       "          -2.7857256,  -6.437106 ,  -5.358173 ,  -5.9309683,\n",
       "          -3.1580446,  -3.4470594,  -3.5653167,  -3.2734659,\n",
       "          -4.6086216,  -4.3400707,  -3.5595388,  -3.1143808,\n",
       "          -3.944113 ,  -4.246043 ,  -3.465238 ,  -7.0357556,\n",
       "          -4.5524173,  -6.66158  ,  -2.5918803,  -6.2938085,\n",
       "          -3.1146746,  -2.759447 ,  -1.7517631,  -4.603934 ,\n",
       "          -6.653639 ,  -2.846127 ,  -3.2146792,  -2.9904287,\n",
       "          -3.218886 ,  -4.164601 ,  -7.090719 ,  -1.8561269,\n",
       "          -5.98183  ,  -7.2299147,  -7.418528 ,  -6.060756 ,\n",
       "          -3.1303773,  -5.2898164,  -2.7861798,  -2.7055922,\n",
       "          -2.6158338,  -3.3050516,  -3.2081044,  -3.786846 ,\n",
       "          -5.36085  ,  -3.595836 ,  -3.7347775,  -3.942403 ,\n",
       "          -2.8212366,  -2.2166877,  -2.9087913,  -2.8678296,\n",
       "          -2.684689 ,  -4.730176 ,  -5.7304597,  -2.7828038,\n",
       "          -7.0852065,  -4.0053306,  -3.426482 ,  -6.1984243,\n",
       "          -2.377021 ,  -1.9663415,  -2.5231028,  -3.4544713,\n",
       "          -2.7251034,  -2.8823206,  -4.080458 ,  -4.9619427,\n",
       "          -3.4878423,  -5.306552 ,  -3.1544118,  -4.525332 ,\n",
       "          -3.0729322,  -2.9486127,  -2.2733915,  -4.1106725,\n",
       "          -2.856817 ,  -4.0824594,  -3.3808324,  -6.8919406,\n",
       "          -2.833651 ,  -7.824736 ,  -3.2096894,  -4.805768 ,\n",
       "          -4.3105817,  -4.162802 ,  -3.3612409,  -5.0255833,\n",
       "          -3.5366912,  -3.037774 ,  -6.0564737,  -4.0090446,\n",
       "          -3.1593254,  -2.7392535,  -4.4183116,  -7.040127 ,\n",
       "          -2.6658096,  -3.5967677,  -2.5963898,  -3.3263514,\n",
       "          -5.748226 ,  -3.0137355,  -3.6854706,  -3.280268 ,\n",
       "          -2.056602 ,  -7.171436 ,  -3.1603277,  -5.9288664,\n",
       "          -2.8297138,  -3.120395 ,  -8.707944 ,  -2.3236737,\n",
       "          -1.9326789,  -2.6915295,  -2.335904 ,  -3.122296 ,\n",
       "          -4.255799 ,  -4.0134726,  -3.1261404,  -3.0480235,\n",
       "          -3.5634441,  -4.2266407,  -2.367019 ,  -5.5759344,\n",
       "          -4.3169346,  -5.491379 ,  -2.8803189,  -4.99235  ,\n",
       "          -5.6128774,  -3.1760423,  -2.8664138,  -3.492583 ,\n",
       "          -2.7793732,  -3.2172318,  -4.8880424,  -3.0360892,\n",
       "          -2.4343164,  -3.0648963,  -4.9675913,  -4.557722 ,\n",
       "          -2.4546266,  -4.128049 ,  -2.9091682,  -4.01008  ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "pred_training = wavenet(x_train, training=True)\n",
    "pred_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[32]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "tf.argmax(wavenet(x_train), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}