{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_Wavenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'20201205-021006'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(\"version\"):\n",
    "    os.makedirs(\"version\")\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "#param_list[\"DILATIONS\"] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "param_list[\"DILATIONS\"] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "param_list[\"FILTER_WIDTH\"] = 2                          # == kernel_size\n",
    "param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"FILTER_WIDTH\"]\n",
    "param_list[\"BATCH_SIZE\"] = 8    #########\n",
    "param_list[\"DILATION_CHANNELS\"] = 32\n",
    "param_list[\"RESIDUAL_CHANNELS\"] = 24\n",
    "param_list[\"SKIP_CHANNELS\"] = 128\n",
    "param_list[\"OUT_CHANNELS\"] = 16293                      # == vocab_size\n",
    "param_list[\"USE_BIASES\"] = False\n",
    "\n",
    "\n",
    "#quantization_channels = 2**8\n",
    "#out_channels = 10*3\n",
    "\n",
    "#initial_filter_width = 32       # Scalar Input\n",
    "#receptive_field = sum(dilation) + initial_filter_width         # Scalar Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "param_list[\"RECEPTIVE_FIELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/{}_train_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)[:5000]  ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_train = x_train.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"])) \n",
    "x_train = x_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.data.Dataset.from_tensor_slices(train_set[param_list[\"RECEPTIVE_FIELD\"]:]).window(1, 1, 1, True)\n",
    "y_train = y_train.flat_map(lambda y: y.batch(1))\n",
    "#y_train = y_train.map(lambda y: tf.expand_dims(y, axis=-1))\n",
    "y_train = y_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip((x_train, y_train))#.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/{}_val_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)[:5000]  ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tf.data.Dataset.from_tensor_slices(val_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_val = x_val.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"]))\n",
    "x_val = x_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = tf.data.Dataset.from_tensor_slices(val_set[param_list[\"RECEPTIVE_FIELD\"]:]).window(1, 1, 1, True)\n",
    "y_val = y_val.flat_map(lambda y: y.batch(1))\n",
    "y_val = y_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.data.Dataset.zip((x_val, y_val))#.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "            \n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels        # out_channels == quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, dilation_channels, residual_channels, skip_channels, out_channels=None, use_biases=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        #self.initial_filter_width = initial_filter_width       # Scalar Input\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels        # out_channels == quantization_channels     # Same as vocab_size in encoder-decoder\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "        # Scalar Input receptive field\n",
    "        #self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "\n",
    "        # Onehot Input Receptive Field\n",
    "        self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.filter_width\n",
    "\n",
    "    def build(self, input_shape):  # Unable to retrieve input_shape when using tf.data.Dataset  \n",
    "        #self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model\n",
    "        \n",
    "        self.output_width = 1\n",
    "\n",
    "        self.preprocessing_layer = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            #kernel_size=self.initial_filter_width,     # Scalar Input\n",
    "            kernel_size=self.filter_width,\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for i, dilation in enumerate(self.dilations):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    layer_index=i,\n",
    "                    dilation=self.dilations[0], \n",
    "                    filter_width=self.filter_width, \n",
    "                    dilation_channels=self.dilation_channels, \n",
    "                    residual_channels=self.residual_channels, \n",
    "                    skip_channels=self.skip_channels, \n",
    "                    use_biases=self.use_biases, \n",
    "                    output_width=self.output_width)\n",
    "                )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        #inputs = tf.sparse.to_dense(inputs)     # x from onehot dataset\n",
    "        inputs = tf.one_hot(inputs, self.out_channels, axis=-1)\n",
    "        \n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "        \n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "            \n",
    "        skip_sum = tf.math.add_n(skip_outputs)\n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "        \n",
    "        #out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "        #output = sample_from_discretized_mix_logistic(out)             # Generative\n",
    "\n",
    "        output = tf.cast(tf.nn.softmax(tf.cast(output, tf.float64)), tf.float32)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "        \n",
    "        #y = tf.expand_dims(tf.sparse.to_dense(y), axis=1)      # y from onehot dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            #reduced_loss = tf.math.reduce_mean(loss)\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(\n",
    "    batch_size=param_list[\"BATCH_SIZE\"], \n",
    "    dilations=param_list[\"DILATIONS\"], \n",
    "    filter_width=param_list[\"FILTER_WIDTH\"], \n",
    "    dilation_channels=param_list[\"DILATION_CHANNELS\"], \n",
    "    residual_channels=param_list[\"RESIDUAL_CHANNELS\"], \n",
    "    skip_channels=param_list[\"SKIP_CHANNELS\"], \n",
    "    out_channels=param_list[\"OUT_CHANNELS\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "source": [
    "wavenet.fit(train_data, epochs=10000, validation_data=val_data, \n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)])"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "369/369 [==============================] - 37s 99ms/step - loss: 8.8471 - val_loss: 9.6412\n",
      "Epoch 2/2\n",
      "369/369 [==============================] - 35s 94ms/step - loss: 8.7995 - val_loss: 9.6391\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2681372d188>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "source": [
    "wavenet.save(version_dir)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: version/20201205-021006\\assets\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}