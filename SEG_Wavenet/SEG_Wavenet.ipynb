{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SEG_Wavenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'20201208-060233'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(\"version\"):\n",
    "    os.makedirs(\"version\")\n",
    "version_dir = \"version/\" + timestamp \n",
    "\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = dict()\n",
    "\n",
    "param_list[\"BATCH_SIZE\"] = 8\n",
    "param_list[\"DILATIONS\"] = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "param_list[\"FILTER_WIDTH\"] = 2                          # == kernel_size\n",
    "param_list[\"RECEPTIVE_FIELD\"] = (param_list[\"FILTER_WIDTH\"] - 1) * sum(param_list[\"DILATIONS\"]) + param_list[\"FILTER_WIDTH\"]\n",
    "param_list[\"DILATION_CHANNELS\"] = 32\n",
    "param_list[\"RESIDUAL_CHANNELS\"] = 24\n",
    "param_list[\"SKIP_CHANNELS\"] = 128\n",
    "param_list[\"OUT_CHANNELS\"] = 16293                      # == vocab_size\n",
    "param_list[\"USE_BIASES\"] = False\n",
    "param_list[\"BUFFER_SIZE\"] = 200000\n",
    "param_list[\"SHUFFLE_SEED\"] = 102\n",
    "\n",
    "\n",
    "#quantization_channels = 2**8\n",
    "#out_channels = 10*3\n",
    "\n",
    "#initial_filter_width = 32       # Scalar Input\n",
    "#receptive_field = sum(dilation) + initial_filter_width         # Scalar Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "param_list[\"RECEPTIVE_FIELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.genfromtxt(\"data/{}_train_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_train = x_train.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"])) \n",
    "x_train = x_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.data.Dataset.from_tensor_slices(train_set[param_list[\"RECEPTIVE_FIELD\"]:]).window(1, 1, 1, True)\n",
    "y_train = y_train.flat_map(lambda y: y.batch(1))\n",
    "#y_train = y_train.map(lambda y: tf.expand_dims(y, axis=-1))\n",
    "y_train = y_train.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.zip((x_train, y_train)).shuffle(param_list[\"BUFFER_SIZE\"], param_list[\"SHUFFLE_SEED\"], reshuffle_each_iteration=True).prefetch(param_list[\"BUFFER_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = np.genfromtxt(\"data/{}_val_set.csv\".format(dataset_name), delimiter=\"\\n\", dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tf.data.Dataset.from_tensor_slices(val_set[:-1]).window(param_list[\"RECEPTIVE_FIELD\"], 1, 1, True)\n",
    "x_val = x_val.flat_map(lambda x: x.batch(param_list[\"RECEPTIVE_FIELD\"]))\n",
    "x_val = x_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = tf.data.Dataset.from_tensor_slices(val_set[param_list[\"RECEPTIVE_FIELD\"]:]).window(1, 1, 1, True)\n",
    "y_val = y_val.flat_map(lambda y: y.batch(1))\n",
    "y_val = y_val.batch(param_list[\"BATCH_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.data.Dataset.zip((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels        # out_channels == quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, dilation_channels, residual_channels, skip_channels, out_channels=None, use_biases=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        #self.initial_filter_width = initial_filter_width       # Scalar Input\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        # quantization_channels == out_channels\n",
    "        self.out_channels = out_channels             # Same as vocab_size in encoder-decoder\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "        # Scalar Input receptive field\n",
    "        #self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "\n",
    "        # Onehot Input Receptive Field\n",
    "        self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.filter_width\n",
    "\n",
    "    def build(self, input_shape):  # Unable to retrieve input_shape when using tf.data.Dataset  \n",
    "        #self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model\n",
    "        \n",
    "        self.output_width = 1\n",
    "\n",
    "        self.preprocessing_layer = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            #kernel_size=self.initial_filter_width,     # Scalar Input\n",
    "            kernel_size=self.filter_width,\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for i, dilation in enumerate(self.dilations):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    layer_index=i,\n",
    "                    dilation=dilation, \n",
    "                    filter_width=self.filter_width, \n",
    "                    dilation_channels=self.dilation_channels, \n",
    "                    residual_channels=self.residual_channels, \n",
    "                    skip_channels=self.skip_channels, \n",
    "                    use_biases=self.use_biases, \n",
    "                    output_width=self.output_width)\n",
    "                )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        #inputs = tf.sparse.to_dense(inputs)     # x from onehot dataset\n",
    "        inputs = tf.one_hot(inputs, self.out_channels, axis=-1)\n",
    "        \n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "        \n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "            \n",
    "        skip_sum = tf.math.add_n(skip_outputs)\n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "        \n",
    "        #out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "        #output = sample_from_discretized_mix_logistic(out)             # Generative\n",
    "\n",
    "        #if not training:\n",
    "        #    output = tf.nn.softmax(tf.cast(output, tf.float64))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)        \n",
    "        #y = tf.expand_dims(tf.sparse.to_dense(y), axis=1)      # y from onehot dataset\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "            #reduced_loss = tf.math.reduce_mean(loss)\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y = tf.one_hot(y, self.out_channels, axis=-1)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(\n",
    "    batch_size=param_list[\"BATCH_SIZE\"], \n",
    "    dilations=param_list[\"DILATIONS\"], \n",
    "    filter_width=param_list[\"FILTER_WIDTH\"], \n",
    "    dilation_channels=param_list[\"DILATION_CHANNELS\"], \n",
    "    residual_channels=param_list[\"RESIDUAL_CHANNELS\"], \n",
    "    skip_channels=param_list[\"SKIP_CHANNELS\"], \n",
    "    out_channels=param_list[\"OUT_CHANNELS\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "source": [
    "history = wavenet.fit(train_data, epochs=10000, validation_data=val_data, \n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)])"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10000\n",
      "19906/19906 [==============================] - 1353s 68ms/step - loss: 5.7549 - categorical_accuracy: 0.3943 - val_loss: 6.4943 - val_categorical_accuracy: 0.2350\n",
      "Epoch 2/10000\n",
      "19906/19906 [==============================] - 1349s 68ms/step - loss: 4.6159 - categorical_accuracy: 0.4151 - val_loss: 5.3252 - val_categorical_accuracy: 0.2498\n",
      "Epoch 3/10000\n",
      "19906/19906 [==============================] - 1383s 69ms/step - loss: 3.7985 - categorical_accuracy: 0.4378 - val_loss: 4.5198 - val_categorical_accuracy: 0.2959\n",
      "Epoch 4/10000\n",
      "19906/19906 [==============================] - 1371s 69ms/step - loss: 3.2122 - categorical_accuracy: 0.4812 - val_loss: 4.0125 - val_categorical_accuracy: 0.3593\n",
      "Epoch 5/10000\n",
      "19906/19906 [==============================] - 1309s 66ms/step - loss: 2.7822 - categorical_accuracy: 0.5344 - val_loss: 3.6092 - val_categorical_accuracy: 0.4362\n",
      "Epoch 6/10000\n",
      "19906/19906 [==============================] - 1370s 69ms/step - loss: 2.7364 - categorical_accuracy: 0.5620 - val_loss: 3.4259 - val_categorical_accuracy: 0.4815\n",
      "Epoch 7/10000\n",
      "19906/19906 [==============================] - 1383s 69ms/step - loss: 2.3032 - categorical_accuracy: 0.6076 - val_loss: 3.2608 - val_categorical_accuracy: 0.5311\n",
      "Epoch 8/10000\n",
      "19906/19906 [==============================] - 1367s 69ms/step - loss: 2.1252 - categorical_accuracy: 0.6362 - val_loss: 3.1776 - val_categorical_accuracy: 0.5492\n",
      "Epoch 9/10000\n",
      "19906/19906 [==============================] - 1380s 69ms/step - loss: 1.9883 - categorical_accuracy: 0.6570 - val_loss: 3.1195 - val_categorical_accuracy: 0.5793\n",
      "Epoch 10/10000\n",
      "19906/19906 [==============================] - 1257s 63ms/step - loss: 1.8660 - categorical_accuracy: 0.6762 - val_loss: 3.0403 - val_categorical_accuracy: 0.5948\n",
      "Epoch 11/10000\n",
      "19906/19906 [==============================] - 1374s 69ms/step - loss: 1.7592 - categorical_accuracy: 0.6915 - val_loss: 2.9593 - val_categorical_accuracy: 0.6137\n",
      "Epoch 12/10000\n",
      "19906/19906 [==============================] - 1391s 70ms/step - loss: 1.6659 - categorical_accuracy: 0.7065 - val_loss: 2.8551 - val_categorical_accuracy: 0.6309\n",
      "Epoch 13/10000\n",
      "19906/19906 [==============================] - 1383s 69ms/step - loss: 1.5937 - categorical_accuracy: 0.7184 - val_loss: 2.9401 - val_categorical_accuracy: 0.6482\n",
      "Epoch 14/10000\n",
      "19906/19906 [==============================] - 1283s 64ms/step - loss: 1.5293 - categorical_accuracy: 0.7281 - val_loss: 2.9060 - val_categorical_accuracy: 0.6480\n",
      "Epoch 15/10000\n",
      "19906/19906 [==============================] - 1258s 63ms/step - loss: 1.4819 - categorical_accuracy: 0.7359 - val_loss: 2.9178 - val_categorical_accuracy: 0.6599\n"
     ]
    }
   ]
  },
  {
   "source": [
    "wavenet.save(version_dir)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: version/20201208-060233\\assets\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
       "0   5.754943              0.394284  6.494298                  0.234984\n",
       "1   4.615860              0.415070  5.325154                  0.249771\n",
       "2   3.798453              0.437847  4.519756                  0.295859\n",
       "3   3.212161              0.481189  4.012517                  0.359268\n",
       "4   2.782216              0.534366  3.609171                  0.436159\n",
       "5   2.736429              0.561971  3.425898                  0.481515\n",
       "6   2.303215              0.607593  3.260771                  0.531052\n",
       "7   2.125237              0.636154  3.177567                  0.549236\n",
       "8   1.988255              0.657021  3.119489                  0.579282\n",
       "9   1.865956              0.676250  3.040307                  0.594775\n",
       "10  1.759207              0.691503  2.959271                  0.613717\n",
       "11  1.665873              0.706530  2.855106                  0.630856\n",
       "12  1.593722              0.718386  2.940069                  0.648204\n",
       "13  1.529326              0.728107  2.906049                  0.647969\n",
       "14  1.481900              0.735913  2.917766                  0.659882"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>categorical_accuracy</th>\n      <th>val_loss</th>\n      <th>val_categorical_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.754943</td>\n      <td>0.394284</td>\n      <td>6.494298</td>\n      <td>0.234984</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.615860</td>\n      <td>0.415070</td>\n      <td>5.325154</td>\n      <td>0.249771</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.798453</td>\n      <td>0.437847</td>\n      <td>4.519756</td>\n      <td>0.295859</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.212161</td>\n      <td>0.481189</td>\n      <td>4.012517</td>\n      <td>0.359268</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.782216</td>\n      <td>0.534366</td>\n      <td>3.609171</td>\n      <td>0.436159</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.736429</td>\n      <td>0.561971</td>\n      <td>3.425898</td>\n      <td>0.481515</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.303215</td>\n      <td>0.607593</td>\n      <td>3.260771</td>\n      <td>0.531052</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.125237</td>\n      <td>0.636154</td>\n      <td>3.177567</td>\n      <td>0.549236</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.988255</td>\n      <td>0.657021</td>\n      <td>3.119489</td>\n      <td>0.579282</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.865956</td>\n      <td>0.676250</td>\n      <td>3.040307</td>\n      <td>0.594775</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.759207</td>\n      <td>0.691503</td>\n      <td>2.959271</td>\n      <td>0.613717</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.665873</td>\n      <td>0.706530</td>\n      <td>2.855106</td>\n      <td>0.630856</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.593722</td>\n      <td>0.718386</td>\n      <td>2.940069</td>\n      <td>0.648204</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.529326</td>\n      <td>0.728107</td>\n      <td>2.906049</td>\n      <td>0.647969</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.481900</td>\n      <td>0.735913</td>\n      <td>2.917766</td>\n      <td>0.659882</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_history = pd.DataFrame.from_dict(history.history)\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.to_csv(\"version/{}/train_history.csv\".format(timestamp), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"version/{}/dilations.json\".format(timestamp), \"w\") as j:\n",
    "    json.dump({\"DILATIONS\":param_list[\"DILATIONS\"]}, j, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}