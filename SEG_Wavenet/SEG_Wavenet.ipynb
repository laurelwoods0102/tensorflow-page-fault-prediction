{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixtures import discretized_mix_logistic_loss, sample_from_discretized_mix_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])\n",
    "\n",
    "    #def init_queue(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "            \n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(x)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(x, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "        self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #self.receptive_field = input_shape[1] - sum(self.dilations)      \n",
    "        self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model\n",
    "\n",
    "        self.preprocessing_layer = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=self.initial_filter_width,\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for _ in range(1):\n",
    "            for i, dilation in enumerate(self.dilations):\n",
    "                self.residual_blocks.append(\n",
    "                    ResidualBlock(\n",
    "                        layer_index=i,\n",
    "                        dilation=self.dilations[0], \n",
    "                        filter_width=self.filter_width, \n",
    "                        dilation_channels=self.dilation_channels, \n",
    "                        residual_channels=self.residual_channels, \n",
    "                        skip_channels=self.skip_channels, \n",
    "                        use_biases=self.use_biases, \n",
    "                        output_width=self.output_width)\n",
    "                    )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        '''\n",
    "        == predict_proba_incremental\n",
    "\n",
    "        Assume that x is integer (== scalar_input = True)\n",
    "        '''\n",
    "\n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "\n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "\n",
    "        skip_sum = tf.math.add_n(skip_outputs)          \n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "\n",
    "        if not training:\n",
    "            out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "            #output = sample_from_discretized_mix_logistic(out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)            \n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedMixLogisticLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"discretized_mix_logistic_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss =  discretized_mix_logistic_loss(y_pred, y_true)\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1024, 1), dtype=float32, numpy=\n",
       "array([[[109.],\n",
       "        [184.],\n",
       "        [ 55.],\n",
       "        ...,\n",
       "        [186.],\n",
       "        [162.],\n",
       "        [243.]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 559
    }
   ],
   "source": [
    "x = tf.random.uniform([1, 1024, 1], minval=0, maxval=255, dtype=tf.int32)\n",
    "x = tf.cast(x, tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParms follows the Diagram \n",
    "batch_size = 1\n",
    "dilations = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "filter_width = 2        # == kernel_size\n",
    "initial_filter_width = 32       # from (tacokr)\n",
    "dilation_channels = 32  # unknown\n",
    "residual_channels = 24\n",
    "skip_channels = 128\n",
    "quantization_channels = 2**8\n",
    "out_channels = 10*3\n",
    "use_biases = False\n",
    "\n",
    "#wavenet.receptive_field = 287\n",
    "#wavenet.output_width = 738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels, out_channels, use_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 738, 30), dtype=float32, numpy=\n",
       "array([[[ 31.044838  , -18.3206    , -57.86234   , ...,  -1.4861172 ,\n",
       "          -0.27262142,  18.613617  ],\n",
       "        [ 37.923584  , -21.745275  , -71.82771   , ...,  -1.5249838 ,\n",
       "           0.10239481,  22.977215  ],\n",
       "        [ 37.41273   , -19.714155  , -66.07158   , ...,  -2.2198381 ,\n",
       "           1.0691111 ,  21.501781  ],\n",
       "        ...,\n",
       "        [ 38.902298  , -21.019793  , -70.890495  , ...,  -2.6501193 ,\n",
       "           0.61166775,  22.990387  ],\n",
       "        [ 37.43091   , -21.598278  , -71.11528   , ...,  -0.99064434,\n",
       "           0.27957615,  22.744024  ],\n",
       "        [ 38.454304  , -21.674555  , -71.42498   , ...,  -1.9789606 ,\n",
       "           0.2171533 ,  23.155088  ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 572
    }
   ],
   "source": [
    "pred_training = wavenet(x, training=True)\n",
    "pred_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[tv.name for tv in wavenet.trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=DiscretizedMixLogisticLoss(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.358543>"
      ]
     },
     "metadata": {},
     "execution_count": 573
    }
   ],
   "source": [
    "loss_fn = DiscretizedMixLogisticLoss()\n",
    "loss_fn(x[:, (1024-738):, :], pred_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "racy: 0.0054\n",
      "Epoch 4804/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8329 - accuracy: 0.0054\n",
      "Epoch 4805/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7431 - accuracy: 0.0054\n",
      "Epoch 4806/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6411 - accuracy: 0.0054\n",
      "Epoch 4807/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.5279 - accuracy: 0.0054\n",
      "Epoch 4808/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4047 - accuracy: 0.0054\n",
      "Epoch 4809/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2725 - accuracy: 0.0054\n",
      "Epoch 4810/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1320 - accuracy: 0.0054\n",
      "Epoch 4811/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9841 - accuracy: 0.0054\n",
      "Epoch 4812/5000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.8316 - accuracy: 0.0054\n",
      "Epoch 4813/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9696 - accuracy: 0.0054\n",
      "Epoch 4814/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 21.1360 - accuracy: 0.0054\n",
      "Epoch 4815/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.4257 - accuracy: 0.0054\n",
      "Epoch 4816/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.1809 - accuracy: 0.0054\n",
      "Epoch 4817/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4684 - accuracy: 0.0054\n",
      "Epoch 4818/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2876 - accuracy: 0.0054\n",
      "Epoch 4819/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2818 - accuracy: 0.0054\n",
      "Epoch 4820/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3336 - accuracy: 0.0054\n",
      "Epoch 4821/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4025 - accuracy: 0.0054\n",
      "Epoch 4822/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.4695 - accuracy: 0.0054\n",
      "Epoch 4823/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5253 - accuracy: 0.0054\n",
      "Epoch 4824/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5676 - accuracy: 0.0054\n",
      "Epoch 4825/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5954 - accuracy: 0.0054\n",
      "Epoch 4826/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.6091 - accuracy: 0.0054\n",
      "Epoch 4827/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.6102 - accuracy: 0.0054\n",
      "Epoch 4828/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5997 - accuracy: 0.0054\n",
      "Epoch 4829/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5792 - accuracy: 0.0054\n",
      "Epoch 4830/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5503 - accuracy: 0.0054\n",
      "Epoch 4831/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5141 - accuracy: 0.0054\n",
      "Epoch 4832/5000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.4722 - accuracy: 0.0054\n",
      "Epoch 4833/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.4253 - accuracy: 0.0054\n",
      "Epoch 4834/5000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.3745 - accuracy: 0.0054\n",
      "Epoch 4835/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3207 - accuracy: 0.0054\n",
      "Epoch 4836/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2644 - accuracy: 0.0054\n",
      "Epoch 4837/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2056 - accuracy: 0.0054\n",
      "Epoch 4838/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.1441 - accuracy: 0.0054\n",
      "Epoch 4839/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0791 - accuracy: 0.0054\n",
      "Epoch 4840/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.0092 - accuracy: 0.0054\n",
      "Epoch 4841/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9328 - accuracy: 0.0054\n",
      "Epoch 4842/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8476 - accuracy: 0.0054\n",
      "Epoch 4843/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7510 - accuracy: 0.0054\n",
      "Epoch 4844/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6401 - accuracy: 0.0054\n",
      "Epoch 4845/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.5121 - accuracy: 0.0054\n",
      "Epoch 4846/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3644 - accuracy: 0.0054\n",
      "Epoch 4847/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1955 - accuracy: 0.0054\n",
      "Epoch 4848/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0080 - accuracy: 0.0054\n",
      "Epoch 4849/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8122 - accuracy: 0.0054\n",
      "Epoch 4850/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6314 - accuracy: 0.0054\n",
      "Epoch 4851/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4923 - accuracy: 0.0054\n",
      "Epoch 4852/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3890 - accuracy: 0.0054\n",
      "Epoch 4853/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2681 - accuracy: 0.0054\n",
      "Epoch 4854/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1019 - accuracy: 0.0054\n",
      "Epoch 4855/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9250 - accuracy: 0.0054\n",
      "Epoch 4856/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7605 - accuracy: 0.0054\n",
      "Epoch 4857/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5965 - accuracy: 0.0054\n",
      "Epoch 4858/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4284 - accuracy: 0.0054\n",
      "Epoch 4859/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2568 - accuracy: 0.0054\n",
      "Epoch 4860/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0830 - accuracy: 0.0054\n",
      "Epoch 4861/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9704 - accuracy: 0.0054\n",
      "Epoch 4862/5000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 7.4370 - accuracy: 0.0054\n",
      "Epoch 4863/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 21.1282 - accuracy: 0.0054\n",
      "Epoch 4864/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4468 - accuracy: 0.0054\n",
      "Epoch 4865/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0681 - accuracy: 0.0054\n",
      "Epoch 4866/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9806 - accuracy: 0.0054\n",
      "Epoch 4867/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9707 - accuracy: 0.0054\n",
      "Epoch 4868/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9745 - accuracy: 0.0054\n",
      "Epoch 4869/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9708 - accuracy: 0.0054\n",
      "Epoch 4870/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.9534 - accuracy: 0.0054\n",
      "Epoch 4871/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9215 - accuracy: 0.0054\n",
      "Epoch 4872/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8763 - accuracy: 0.0054\n",
      "Epoch 4873/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8207 - accuracy: 0.0054\n",
      "Epoch 4874/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7603 - accuracy: 0.0054\n",
      "Epoch 4875/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6948 - accuracy: 0.0054\n",
      "Epoch 4876/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6227 - accuracy: 0.0054\n",
      "Epoch 4877/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5417 - accuracy: 0.0054\n",
      "Epoch 4878/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4489 - accuracy: 0.0054\n",
      "Epoch 4879/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3433 - accuracy: 0.0054\n",
      "Epoch 4880/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2269 - accuracy: 0.0054\n",
      "Epoch 4881/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1034 - accuracy: 0.0054\n",
      "Epoch 4882/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9749 - accuracy: 0.0054\n",
      "Epoch 4883/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8409 - accuracy: 0.0054\n",
      "Epoch 4884/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7009 - accuracy: 0.0054\n",
      "Epoch 4885/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5558 - accuracy: 0.0054\n",
      "Epoch 4886/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4063 - accuracy: 0.0054\n",
      "Epoch 4887/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2527 - accuracy: 0.0054\n",
      "Epoch 4888/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0956 - accuracy: 0.0054\n",
      "Epoch 4889/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9377 - accuracy: 0.0054\n",
      "Epoch 4890/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0215 - accuracy: 0.0054\n",
      "Epoch 4891/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.5463 - accuracy: 0.0054\n",
      "Epoch 4892/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 12.6162 - accuracy: 0.0054\n",
      "Epoch 4893/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2573 - accuracy: 0.0054\n",
      "Epoch 4894/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3079 - accuracy: 0.0054\n",
      "Epoch 4895/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9383 - accuracy: 0.0054\n",
      "Epoch 4896/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8546 - accuracy: 0.0054\n",
      "Epoch 4897/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8438 - accuracy: 0.0054\n",
      "Epoch 4898/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8559 - accuracy: 0.0054\n",
      "Epoch 4899/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8705 - accuracy: 0.0054\n",
      "Epoch 4900/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8784 - accuracy: 0.0054\n",
      "Epoch 4901/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8756 - accuracy: 0.0054\n",
      "Epoch 4902/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.8604 - accuracy: 0.0054\n",
      "Epoch 4903/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8321 - accuracy: 0.0054\n",
      "Epoch 4904/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7907 - accuracy: 0.0054\n",
      "Epoch 4905/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7362 - accuracy: 0.0054\n",
      "Epoch 4906/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6686 - accuracy: 0.0054\n",
      "Epoch 4907/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5880 - accuracy: 0.0054\n",
      "Epoch 4908/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4941 - accuracy: 0.0054\n",
      "Epoch 4909/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3872 - accuracy: 0.0054\n",
      "Epoch 4910/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2675 - accuracy: 0.0054\n",
      "Epoch 4911/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1368 - accuracy: 0.0054\n",
      "Epoch 4912/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9983 - accuracy: 0.0054\n",
      "Epoch 4913/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8576 - accuracy: 0.0054\n",
      "Epoch 4914/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7209 - accuracy: 0.0054\n",
      "Epoch 4915/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5917 - accuracy: 0.0054\n",
      "Epoch 4916/5000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.4639 - accuracy: 0.0054\n",
      "Epoch 4917/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3250 - accuracy: 0.0054\n",
      "Epoch 4918/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.1678 - accuracy: 0.0054\n",
      "Epoch 4919/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0022 - accuracy: 0.0054\n",
      "Epoch 4920/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8387 - accuracy: 0.0054\n",
      "Epoch 4921/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6734 - accuracy: 0.0054\n",
      "Epoch 4922/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.5085 - accuracy: 0.0054\n",
      "Epoch 4923/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3425 - accuracy: 0.0054\n",
      "Epoch 4924/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1782 - accuracy: 0.0054\n",
      "Epoch 4925/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1628 - accuracy: 0.0054\n",
      "Epoch 4926/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.7967 - accuracy: 0.0054\n",
      "Epoch 4927/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.6277 - accuracy: 0.0054\n",
      "Epoch 4928/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0825 - accuracy: 0.0054\n",
      "Epoch 4929/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5273 - accuracy: 0.0054\n",
      "Epoch 4930/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1688 - accuracy: 0.0054\n",
      "Epoch 4931/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.1804 - accuracy: 0.0054\n",
      "Epoch 4932/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2061 - accuracy: 0.0054\n",
      "Epoch 4933/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2005 - accuracy: 0.0054\n",
      "Epoch 4934/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1713 - accuracy: 0.0054\n",
      "Epoch 4935/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1252 - accuracy: 0.0054\n",
      "Epoch 4936/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0655 - accuracy: 0.0054\n",
      "Epoch 4937/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9936 - accuracy: 0.0054\n",
      "Epoch 4938/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9103 - accuracy: 0.0054\n",
      "Epoch 4939/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8164 - accuracy: 0.0054\n",
      "Epoch 4940/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7127 - accuracy: 0.0054\n",
      "Epoch 4941/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6002 - accuracy: 0.0054\n",
      "Epoch 4942/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4797 - accuracy: 0.0054\n",
      "Epoch 4943/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.3519 - accuracy: 0.0054\n",
      "Epoch 4944/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2176 - accuracy: 0.0054\n",
      "Epoch 4945/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0774 - accuracy: 0.0054\n",
      "Epoch 4946/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9318 - accuracy: 0.0054\n",
      "Epoch 4947/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7853 - accuracy: 0.0054\n",
      "Epoch 4948/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2104 - accuracy: 0.0054\n",
      "Epoch 4949/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.6344 - accuracy: 0.0054\n",
      "Epoch 4950/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6963 - accuracy: 0.0054\n",
      "Epoch 4951/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8387 - accuracy: 0.0054\n",
      "Epoch 4952/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.8064 - accuracy: 0.0054\n",
      "Epoch 4953/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8384 - accuracy: 0.0054\n",
      "Epoch 4954/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8379 - accuracy: 0.0054\n",
      "Epoch 4955/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8245 - accuracy: 0.0054\n",
      "Epoch 4956/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.7929 - accuracy: 0.0054\n",
      "Epoch 4957/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7458 - accuracy: 0.0054\n",
      "Epoch 4958/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6845 - accuracy: 0.0054\n",
      "Epoch 4959/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6106 - accuracy: 0.0054\n",
      "Epoch 4960/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5253 - accuracy: 0.0054\n",
      "Epoch 4961/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.4298 - accuracy: 0.0054\n",
      "Epoch 4962/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3250 - accuracy: 0.0054\n",
      "Epoch 4963/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2118 - accuracy: 0.0054\n",
      "Epoch 4964/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.0925 - accuracy: 0.0054\n",
      "Epoch 4965/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.0160 - accuracy: 0.0054\n",
      "Epoch 4966/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.1778 - accuracy: 0.0054\n",
      "Epoch 4967/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3855 - accuracy: 0.0054\n",
      "Epoch 4968/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7101 - accuracy: 0.0054\n",
      "Epoch 4969/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1139 - accuracy: 0.0054\n",
      "Epoch 4970/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0218 - accuracy: 0.0054\n",
      "Epoch 4971/5000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 5.0732 - accuracy: 0.0054\n",
      "Epoch 4972/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0860 - accuracy: 0.0054\n",
      "Epoch 4973/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0775 - accuracy: 0.0054\n",
      "Epoch 4974/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0522 - accuracy: 0.0054\n",
      "Epoch 4975/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.0119 - accuracy: 0.0054\n",
      "Epoch 4976/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9578 - accuracy: 0.0054\n",
      "Epoch 4977/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8908 - accuracy: 0.0054\n",
      "Epoch 4978/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8122 - accuracy: 0.0054\n",
      "Epoch 4979/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7229 - accuracy: 0.0054\n",
      "Epoch 4980/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.6239 - accuracy: 0.0054\n",
      "Epoch 4981/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5162 - accuracy: 0.0054\n",
      "Epoch 4982/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4006 - accuracy: 0.0054\n",
      "Epoch 4983/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2779 - accuracy: 0.0054\n",
      "Epoch 4984/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1487 - accuracy: 0.0054\n",
      "Epoch 4985/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0136 - accuracy: 0.0054\n",
      "Epoch 4986/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8736 - accuracy: 0.0054\n",
      "Epoch 4987/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7615 - accuracy: 0.0054\n",
      "Epoch 4988/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.3216 - accuracy: 0.0054\n",
      "Epoch 4989/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.7623 - accuracy: 0.0054\n",
      "Epoch 4990/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3397 - accuracy: 0.0054\n",
      "Epoch 4991/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9558 - accuracy: 0.0054\n",
      "Epoch 4992/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5238 - accuracy: 0.0054\n",
      "Epoch 4993/5000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 6.4104 - accuracy: 0.0054\n",
      "Epoch 4994/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4021 - accuracy: 0.0054\n",
      "Epoch 4995/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4229 - accuracy: 0.0054\n",
      "Epoch 4996/5000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.4445 - accuracy: 0.0054\n",
      "Epoch 4997/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4558 - accuracy: 0.0054\n",
      "Epoch 4998/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4530 - accuracy: 0.0054\n",
      "Epoch 4999/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4352 - accuracy: 0.0054\n",
      "Epoch 5000/5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4033 - accuracy: 0.0054\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25e748f6708>"
      ]
     },
     "metadata": {},
     "execution_count": 569
    }
   ],
   "source": [
    "#wavenet.fit(x=x, y=x)\n",
    "wavenet.fit(x, x[:, (1024-738):, :], batch_size=8, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 587
    }
   ],
   "source": [
    "#tf.argmin(wavenet(x), axis=1)\n",
    "#wavenet(x[:, :287, :])\n",
    "wavenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}