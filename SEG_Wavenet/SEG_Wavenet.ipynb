{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixtures import discretized_mix_logistic_loss, sample_from_discretized_mix_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.layers.Conv1D):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding=\"causal\", dilation_rate=1, use_bias=False, *args, **kwargs):\n",
    "        super().__init__(filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate)\n",
    "        \n",
    "        ## (issue) Set name other than k and d invoke error : TypeError: unsupported operand type(s) for +: 'int' and 'tuple'\n",
    "        self.k = kernel_size                \n",
    "        self.d = dilation_rate\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        if kernel_size > 1:\n",
    "            self.current_receptive_field = kernel_size + (kernel_size - 1) * (dilation_rate - 1)       # == queue_len (tf2)\n",
    "            self.residual_channels = residual_channels\n",
    "            self.queue = tf.zeros([1, self.current_receptive_field, filters])\n",
    "\n",
    "    def build(self, x_shape):\n",
    "        super().build(x_shape)\n",
    "\n",
    "        self.linearized_weights = tf.cast(tf.reshape(self.kernel, [-1, self.filters]), dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            return super().call(x)\n",
    "\n",
    "        if self.kernel_size > 1:\n",
    "            self.queue = self.queue[:, 1:, :]\n",
    "            self.queue = tf.concat([self.queue, tf.expand_dims(x[:, -1, :], axis=1)], axis=1)\n",
    "\n",
    "            if self.dilation_rate > 1:\n",
    "                x = self.queue[:, 0::self.d, :]\n",
    "            else:\n",
    "                x = self.queue\n",
    "\n",
    "            outputs = tf.matmul(tf.reshape(x, [1, -1]), self.linearized_weights)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "\n",
    "            return tf.reshape(outputs, [-1, 1, self.filters])\n",
    "\n",
    "    #def init_queue(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    def __init__(self, layer_index, dilation, filter_width, dilation_channels, residual_channels, skip_channels, use_biases, output_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_index = layer_index\n",
    "        self.dilation = dilation\n",
    "        self.filter_width = filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.output_width = output_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_filter = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_filter\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_gate = keras.layers.Conv1D(\n",
    "            filters=self.dilation_channels,\n",
    "            kernel_size=self.filter_width,\n",
    "            dilation_rate=self.dilation,\n",
    "            padding='valid',\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/conv_gate\".format(self.layer_index)\n",
    "        )\n",
    "        ## transformed : 1x1 conv to out (= gate * filter) to produce residuals (= dense output)\n",
    "        ## conv_residual (=skip_contribution in original)\n",
    "        self.conv_residual = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/dense\".format(self.layer_index)\n",
    "        )\n",
    "        self.conv_skip = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"residual_block_{}/skip\".format(self.layer_index)\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        out = tf.tanh(self.conv_filter(inputs)) * tf.sigmoid(self.conv_gate(inputs))\n",
    "        \n",
    "        if training:\n",
    "            skip_cut = tf.shape(out)[1] - self.output_width\n",
    "        else:\n",
    "            skip_cut = tf.shape(out)[1] - 1\n",
    "            \n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, self.dilation_channels])\n",
    "        skip_output = self.conv_skip(out_skip)\n",
    "\n",
    "        transformed = self.conv_residual(out)\n",
    "        input_cut = tf.shape(inputs)[1] - tf.shape(transformed)[1]\n",
    "        x_cut = tf.slice(inputs, [0, input_cut, 0], [-1, -1, -1])\n",
    "        dense_output = x_cut + transformed\n",
    "\n",
    "        return skip_output, dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(keras.Model):\n",
    "    def __init__(self, skip_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_channels = skip_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_1 = keras.layers.Conv1D(\n",
    "            filters=self.skip_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_1\"\n",
    "        )\n",
    "        self.conv_2 = keras.layers.Conv1D(\n",
    "            filters=self.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"postprocessing/conv_2\"\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.nn.relu(inputs)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels, out_channels, use_biases):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_biases = use_biases\n",
    "\n",
    "        self.receptive_field = (self.filter_width - 1) * sum(self.dilations) + self.initial_filter_width\n",
    "\n",
    "    def build(self, input_shape):     \n",
    "        self.output_width = input_shape[1] - self.receptive_field + 1       # total output width of model\n",
    "\n",
    "        self.preprocessing_layer = keras.layers.Conv1D(\n",
    "            filters=self.residual_channels,\n",
    "            kernel_size=self.initial_filter_width,\n",
    "            use_bias=self.use_biases,\n",
    "            name=\"preprocessing/conv\")\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for _ in range(1):\n",
    "            for i, dilation in enumerate(self.dilations):\n",
    "                self.residual_blocks.append(\n",
    "                    ResidualBlock(\n",
    "                        layer_index=i,\n",
    "                        dilation=self.dilations[0], \n",
    "                        filter_width=self.filter_width, \n",
    "                        dilation_channels=self.dilation_channels, \n",
    "                        residual_channels=self.residual_channels, \n",
    "                        skip_channels=self.skip_channels, \n",
    "                        use_biases=self.use_biases, \n",
    "                        output_width=self.output_width)\n",
    "                    )\n",
    "\n",
    "        self.postprocessing_layer = PostProcessing(self.skip_channels, self.out_channels, self.use_biases)\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, inputs, training=False):\n",
    "        '''\n",
    "        == predict_proba_incremental\n",
    "\n",
    "        Assume that x is integer (== scalar_input = True)\n",
    "        '''\n",
    "\n",
    "        x = self.preprocessing_layer(inputs)\n",
    "        skip_outputs = []\n",
    "\n",
    "        for layer_index in range(len(self.dilations)):\n",
    "            skip_output, x = self.residual_blocks[layer_index](x, training=training)\n",
    "            skip_outputs.append(skip_output)\n",
    "      \n",
    "        skip_sum = tf.math.add_n(skip_outputs)          \n",
    "        \n",
    "        output = self.postprocessing_layer(skip_sum)\n",
    "\n",
    "        if not training:\n",
    "            out = tf.reshape(output, [self.batch_size, -1, self.out_channels])\n",
    "            #output = sample_from_discretized_mix_logistic(out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train_step(self, data): \n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)            \n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedMixLogisticLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"discretized_mix_logistic_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss =  discretized_mix_logistic_loss(y_pred, y_true)\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "source": [
    "inputs = tf.random.uniform([1, 1025, 1], minval=0, maxval=255, dtype=tf.int32)\n",
    "inputs = tf.cast(inputs, tf.float32)\n",
    "inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1025, 1), dtype=float32, numpy=\n",
       "array([[[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        ...,\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "rg = tf.range(255)\n",
    "inputs = tf.concat([rg, rg, rg, rg, rg], axis=0)[:1025]\n",
    "inputs = tf.cast(tf.reshape(inputs, [1, 1025, 1]), tf.float32)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = inputs[:, :1024, :]\n",
    "x_train = inputs[:, :287, :]\n",
    "#y_train = inputs[:, 287:, :]\n",
    "y_train = inputs[:, 287:288, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([1, 287, 1]), TensorShape([1, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParms follows the Diagram \n",
    "batch_size = 1\n",
    "dilations = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "filter_width = 2        # == kernel_size\n",
    "initial_filter_width = 32       # from (tacokr)\n",
    "dilation_channels = 32  # unknown\n",
    "residual_channels = 24\n",
    "skip_channels = 128\n",
    "quantization_channels = 2**8\n",
    "out_channels = 10*3\n",
    "use_biases = False\n",
    "\n",
    "#wavenet.receptive_field = 287\n",
    "#wavenet.output_width = 738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(batch_size, dilations, filter_width, initial_filter_width, dilation_channels, residual_channels, skip_channels, quantization_channels, out_channels, use_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet.compile(keras.optimizers.Nadam(), loss=DiscretizedMixLogisticLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 980us/step - loss: 0.6931\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6931\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6931\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6931\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6931\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6931\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6931\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6931\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a89940348>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#wavenet.fit(x=x, y=x)\n",
    "wavenet.fit(x_train, y_train, batch_size=8, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 30), dtype=float32, numpy=\n",
       "array([[[ -4.7823987 ,  -2.524256  ,  -8.152562  ,  -8.037164  ,\n",
       "          -2.1420841 , -10.622566  ,  -3.7060866 ,  15.436218  ,\n",
       "          -6.561074  ,  -4.87299   ,   3.2193644 ,   5.4519005 ,\n",
       "           3.441081  ,  -4.6090403 ,  -3.4138145 ,   6.53657   ,\n",
       "          10.395798  ,  10.736645  ,  -1.3931888 ,   0.20424128,\n",
       "          -4.651189  ,   7.193293  ,  -1.5946159 ,  -0.74131685,\n",
       "          10.254593  ,  -1.7528414 ,   7.6960487 ,  21.846106  ,\n",
       "          -1.8778284 ,   6.5342507 ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pred_training = wavenet(x_train, training=True)\n",
    "pred_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "loss_fn = DiscretizedMixLogisticLoss()\n",
    "loss_fn(y_train, pred_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sample_from_discretized_mix_logistic(wavenet(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scale_min=float(tf.math.log(1e-14))\n",
    "y = wavenet(inputs[:, :287, :])\n",
    "y_true = inputs[:, 287:288, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 30), dtype=float32, numpy=\n",
       " array([[[ -4.7823987 ,  -2.524256  ,  -8.152562  ,  -8.037164  ,\n",
       "           -2.1420841 , -10.622566  ,  -3.7060866 ,  15.436218  ,\n",
       "           -6.561074  ,  -4.87299   ,   3.2193644 ,   5.4519005 ,\n",
       "            3.441081  ,  -4.6090403 ,  -3.4138145 ,   6.53657   ,\n",
       "           10.395798  ,  10.736645  ,  -1.3931888 ,   0.20424128,\n",
       "           -4.651189  ,   7.193293  ,  -1.5946159 ,  -0.74131685,\n",
       "           10.254593  ,  -1.7528414 ,   7.6960487 ,  21.846106  ,\n",
       "           -1.8778284 ,   6.5342507 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[32.]]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "y, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "nr_mix = y.shape[2] // 3\n",
    "nr_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 10), dtype=float32, numpy=\n",
       "array([[[ -4.7823987,  -2.524256 ,  -8.152562 ,  -8.037164 ,\n",
       "          -2.1420841, -10.622566 ,  -3.7060866,  15.436218 ,\n",
       "          -6.561074 ,  -4.87299  ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "logit_probs = y[:, :, :nr_mix]\n",
    "logit_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 10), dtype=float32, numpy=array([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "sel = tf.one_hot(tf.argmax(logit_probs - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit_probs), minval=1e-5, maxval=1. - 1e-5))), 2), depth=nr_mix, dtype=tf.float32)\n",
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[10.736645]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "means = tf.math.reduce_sum(y[:, :, nr_mix:nr_mix * 2] * sel, axis=2)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[21.846106]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "log_scales = tf.math.maximum(tf.math.reduce_sum(y[:, :, nr_mix * 2:nr_mix * 3] * sel, axis=2), log_scale_min)\n",
    "log_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.79140556]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "u = tf.random.uniform(tf.shape(means), minval=1e-5, maxval=1. - 1e-5)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.0983555e+09]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "x = means + tf.math.exp(log_scales) * (tf.math.log(u) - tf.math.log(1. - u))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x = tf.math.minimum(tf.math.maximum(x, -1.), 1.)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}